{"cells":[{"cell_type":"markdown","metadata":{"id":"_TCJbIWRMQ9h"},"source":["# 3 - Neural Machine Translation by Jointly Learning to Align and Translate\n","120220107 강희주 \n","\n","## Introduction\n"," 하나의 큰 neural network를 이용한 translation 방법인 neural machine translation은 대부분 encoder-decoder 형식으로 이루어져 있다. \n"," encoder의 경우, $$h_{t} = f(x_{t}, h_{t-1})$$ 위의 식과 같이 이전 time step t-1에서의 hidden state $h_{t-1}$와 해당 time step t에서의 입력 단어$x_{t}$를 통해 time step t에서의 hidden state $h_{t}$를 구한 후, 다음의 식과 같이 이렇게 구한 모든 hidden state들로 고정 길이 벡터c를 얻는다. $$c=q({h_{1}, ..., h_{T_{x}}})$$  \n"," 이때 f(),q()는 non-linear function을 의미하며, 이렇게 구한 고정 길이 벡터 c는 context vector라고도 하며, 입력 문장의 전체적인 정보들을 담고 있다.  \n"," \n"," decoder에서는 위와 같이 구한 고정 길이 벡터 c와, 이전까지 예측한 단어들 {$y_{1},..., y_{t-1}$}이 주어졌을 때, 다음 단어인 $y_{t}$를 예측하도록 학습된다. 일단 번역이라는 task는 target sequence를 y라 하고, 번역하고자 하는 sequence를 x라고 했을 때, x가 주어졌을 때의 y에 대한 조건부 확률을 최대화 하는 것을 목적으로 한다. 즉, 디코더에서는 $argmax_{y}P(y_{t} | {y_{1}, .., y_{t-1}}, c)$ 를 만족시키는 $y_{t}$를 찾는다. \n","\n"," 이런한 encoder-decoder 구조는 문장 길이가 짧을 때는 문제가 없지만, 고정 길이 벡터 c의 dimension 이 충분히 크지 않으면서 입력으로 들어온 문장의 길이가 길다면 전체 문장의 정보를 하나의 고정 길이 벡터 c에 함축할 수 없다는 문제가 발생한다.\n","\n"," 때문에 주어진 논문에서는 encoder-decoder 모델에서 새로운 구조를 추가해서 다음의 문제를 해결할 수 있는 방법을 제안한다. 기존의 encoder-decoder 모델에서는\n"," encoder에서 나온 모든 hidden state들을 활용하지 않고, 단순히 마지막에 나온 hidden state를 고정 길이 벡터 c로 정하였고, decoder에서는 이 벡터를 이용하여 번역 task를 수행하였다면, 주어진 논문에서는 encoder에서 나온 각각의 모든 RNN cell의 hidden state를 활용하여 decoder에서 dynamic하게 각 state 별로 context vector를 만들어 번역 task를 수행하면 앞서 말했던 문제를 해결할 수 있다고 제시하고 있다.    \n"," 이 방법의 장점은 두 가지가 있다. 첫째로, 더 이상 context vector는 고정된 길이의 벡터가 아니라는 점이다. 두 번째는 decoder에서 target word를 예측할 때, encoder에서 나온 모든 hidden state 중에서 집중해야 할 단어들에게만 집중할 수 있는 매커니즘을 따로 설계할 수 있다는 점이다. 여기에 해당하는 것이 바로 해당 논문의 주 아이디어인 *attention* 이다. "]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pXnRLpjmMYiN","outputId":"ce8217bc-9396-4d64-f9c4-9d542b90bb0a","executionInfo":{"status":"ok","timestamp":1665543225196,"user_tz":-540,"elapsed":143757,"user":{"displayName":"ᄀᄒᄌ","userId":"07458837797284130526"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torchtext==0.11.0\n","  Downloading torchtext-0.11.0-cp37-cp37m-manylinux1_x86_64.whl (8.0 MB)\n","\u001b[K     |████████████████████████████████| 8.0 MB 32.5 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.11.0) (4.64.1)\n","Collecting torch==1.10.0\n","  Downloading torch-1.10.0-cp37-cp37m-manylinux1_x86_64.whl (881.9 MB)\n","\u001b[K     |██████████████████████████████▎ | 834.1 MB 1.3 MB/s eta 0:00:39tcmalloc: large alloc 1147494400 bytes == 0x39e22000 @  0x7fb87f664615 0x58e046 0x4f2e5e 0x4d19df 0x51b31c 0x5b41c5 0x58f49e 0x51b221 0x5b41c5 0x58f49e 0x51837f 0x4cfabb 0x517aa0 0x4cfabb 0x517aa0 0x4cfabb 0x517aa0 0x4ba70a 0x538136 0x590055 0x51b180 0x5b41c5 0x58f49e 0x51837f 0x5b41c5 0x58f49e 0x51740e 0x58f2a7 0x517947 0x5b41c5 0x58f49e\n","\u001b[K     |████████████████████████████████| 881.9 MB 19 kB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.11.0) (1.21.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.11.0) (2.23.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10.0->torchtext==0.11.0) (4.1.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.11.0) (2022.9.24)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.11.0) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.11.0) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.11.0) (1.24.3)\n","Installing collected packages: torch, torchtext\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.12.1+cu113\n","    Uninstalling torch-1.12.1+cu113:\n","      Successfully uninstalled torch-1.12.1+cu113\n","  Attempting uninstall: torchtext\n","    Found existing installation: torchtext 0.13.1\n","    Uninstalling torchtext-0.13.1:\n","      Successfully uninstalled torchtext-0.13.1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchvision 0.13.1+cu113 requires torch==1.12.1, but you have torch 1.10.0 which is incompatible.\n","torchaudio 0.12.1+cu113 requires torch==1.12.1, but you have torch 1.10.0 which is incompatible.\u001b[0m\n","Successfully installed torch-1.10.0 torchtext-0.11.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting en-core-web-sm==3.4.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.0/en_core_web_sm-3.4.0-py3-none-any.whl (12.8 MB)\n","\u001b[K     |████████████████████████████████| 12.8 MB 24.4 MB/s \n","\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.4.0) (3.4.1)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (8.1.2)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.21.6)\n","Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.6.2)\n","Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.1.1)\n","Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.4.2)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.23.0)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.3.0)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.10)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.9.2)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.64.1)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.3)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.4.4)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.7)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (57.4.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (21.3)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.8)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.6)\n","Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.10.1)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.8)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.11.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.8.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.9)\n","Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (5.2.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2022.9.24)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.10)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.7.8)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.0.2)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (7.1.2)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.1)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_sm')\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting de-core-news-sm==3.4.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.4.0/de_core_news_sm-3.4.0-py3-none-any.whl (14.6 MB)\n","\u001b[K     |████████████████████████████████| 14.6 MB 25.5 MB/s \n","\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from de-core-news-sm==3.4.0) (3.4.1)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2.0.6)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (1.0.8)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (21.3)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (1.0.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2.11.3)\n","Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (0.10.1)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (1.9.2)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (3.0.7)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2.0.8)\n","Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (0.6.2)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (1.21.6)\n","Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (4.1.1)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (8.1.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (57.4.0)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (3.0.10)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (4.64.1)\n","Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (0.4.2)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (3.3.0)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2.23.0)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2.4.4)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (3.8.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (3.0.9)\n","Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (5.2.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2022.9.24)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (3.0.4)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (0.0.2)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (0.7.8)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (7.1.2)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2.0.1)\n","Installing collected packages: de-core-news-sm\n","Successfully installed de-core-news-sm-3.4.0\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('de_core_news_sm')\n"]}],"source":["#Colab 사용을 위한 준비\n","#torchtext.legacy 사용을 위해 torchtext를 0.11.0으로 downgrade\n","!pip install torchtext==0.11.0\n","#영어, 독일어 spacy 모델을 다운로드\n","!python -m spacy download en_core_web_sm\n","!python -m spacy download de_core_news_sm"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"FS_DVqj4MQ9j","executionInfo":{"status":"ok","timestamp":1665543232661,"user_tz":-540,"elapsed":7469,"user":{"displayName":"ᄀᄒᄌ","userId":"07458837797284130526"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","from torchtext.legacy.datasets import Multi30k\n","from torchtext.legacy.data import Field, BucketIterator\n","\n","import spacy\n","import numpy as np\n","\n","import random\n","import math\n","import time"]},{"cell_type":"markdown","metadata":{"id":"xH_2b7pBMQ9l"},"source":["난수 생성기의 seed를 고정하면, 매번 프로그램을 실행할 때 마다 생성되는 난수들의 수열이 같게 할 수 있다. 이를 통해 반복 실행시, 결과가 재현되도록 해주었다. "]},{"cell_type":"code","execution_count":3,"metadata":{"id":"uImtOjIwMQ9l","executionInfo":{"status":"ok","timestamp":1665543232662,"user_tz":-540,"elapsed":3,"user":{"displayName":"ᄀᄒᄌ","userId":"07458837797284130526"}}},"outputs":[],"source":["SEED = 1234\n","\n","random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","torch.cuda.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True"]},{"cell_type":"markdown","metadata":{"id":"JYBvxf6yMQ9l"},"source":["spacy는 파이썬의 자연어처리를 위한 오픈 소스 기반 라이브러리로 Tokeniztion(입력 텍스트를 단어, 문장 부호 등의 토큰으로 분류하는 과정), POS Tagging(문장 내 단어들의 품사를 식벼라여 태그를 붙여주는 과정), Dependency Parsing(각 token 들간의 의존관계를 고려하여 관련있는 단어들끼리 묶는 과정) 등의 기능을 제공한다. 독일어를 위한 spaCy 모델은 \"de_core_news_sm\"이고, 영어를 위한 모델은 \"en_core_web_sm\"이며 각 모델의 tokenizer에 접근하기 위해 해당 모델을 로드하였다. \n"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"ppYplm4CMQ9m","executionInfo":{"status":"ok","timestamp":1665543234861,"user_tz":-540,"elapsed":2202,"user":{"displayName":"ᄀᄒᄌ","userId":"07458837797284130526"}}},"outputs":[],"source":["spacy_de = spacy.load('de_core_news_sm')\n","spacy_en = spacy.load('en_core_web_sm')"]},{"cell_type":"markdown","metadata":{"id":"rgtHIqPYMQ9m"},"source":["각 언어의 tokenizer를 생성한다. 생성한 함수의 입력으로 들어온 text는 spaCy model의 tokenizer에 입력되어 해당 문자열을 구성하는 개별 토큰으로 분류되며, 정의된 함수의 출력으로는 개별 토큰들로 이루어진 list를 얻게 된다.   "]},{"cell_type":"code","execution_count":5,"metadata":{"id":"3bRTDQdBMQ9m","executionInfo":{"status":"ok","timestamp":1665543234861,"user_tz":-540,"elapsed":3,"user":{"displayName":"ᄀᄒᄌ","userId":"07458837797284130526"}}},"outputs":[],"source":["def tokenize_de(text):\n","    \"\"\"\n","    Tokenizes German text from a string into a list of strings\n","    \"\"\"\n","    return [tok.text for tok in spacy_de.tokenizer(text)]\n","\n","def tokenize_en(text):\n","    \"\"\"\n","    Tokenizes English text from a string into a list of strings\n","    \"\"\"\n","    return [tok.text for tok in spacy_en.tokenizer(text)]"]},{"cell_type":"markdown","metadata":{"id":"7H7pFMIiMQ9n"},"source":["torchtext 에는 필드(field)라는 도구를 제공한다. 필드를 통해 앞으로 어떤 전처리를 할 것인지를 정의한다. SRC field 안에서는 tokenizer로 tokenize_de 함수(독일어 tokenizer)를 사용하며,init_token 과 eos_token 인수를 통해 sos(start of sequence) 와 eos(end of sequece) 토큰을 추가하고, 데이터를 모두 소문자로 바꾸겠다고 정의한다. TPG field 안에서는 tokenizer로 tokenize_en 함수(영어 tokenizer)를 사용하며, SRC field와 마찬가지로 sos 와 eos 토큰을 추가하고, 데이터를 모두 소문자로 바꾸겠다고 정의한다.  "]},{"cell_type":"code","execution_count":6,"metadata":{"id":"YrsoKtxNMQ9n","executionInfo":{"status":"ok","timestamp":1665543234861,"user_tz":-540,"elapsed":2,"user":{"displayName":"ᄀᄒᄌ","userId":"07458837797284130526"}}},"outputs":[],"source":["SRC = Field(tokenize = tokenize_de, \n","            init_token = '<sos>', \n","            eos_token = '<eos>', \n","            lower = True)\n","\n","TRG = Field(tokenize = tokenize_en, \n","            init_token = '<sos>', \n","            eos_token = '<eos>', \n","            lower = True)"]},{"cell_type":"markdown","metadata":{"id":"BqWw-msyMQ9o"},"source":["Multi30k dataset은 약 30,000 개의 영어, 독일어, 프랑스어로 이루어진 문장들이 포함되어 있으며, 한 문장당 대략 12개의 단어로 구성되어 있다. exts는 source와 target으로 사용될 언어를 지정하며(source가 첫번째), fields는 source와 target에서 사용될 전처리 도구인 field를 지정해준다. \n","splits 함수를 통해 Multi30k dataset을 분할하여 train, validation, test data를 얻는다.   "]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M7lJxCZBMQ9o","outputId":"430b9039-ddff-42a0-91a7-6b8b65871c7b","executionInfo":{"status":"ok","timestamp":1665543247725,"user_tz":-540,"elapsed":12866,"user":{"displayName":"ᄀᄒᄌ","userId":"07458837797284130526"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["downloading training.tar.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1.21M/1.21M [00:01<00:00, 1.11MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["downloading validation.tar.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 46.3k/46.3k [00:00<00:00, 278kB/s]\n"]},{"output_type":"stream","name":"stdout","text":["downloading mmt_task1_test2016.tar.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 66.2k/66.2k [00:00<00:00, 275kB/s]\n"]}],"source":["train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'), \n","                                                    fields = (SRC, TRG))"]},{"cell_type":"markdown","metadata":{"id":"vKAx0xKCMQ9o"},"source":["source 언어와 target 언어의 *vocabulary*를 생성한다. vocabulary는 각각의 고유한 토큰을 인덱스와 연결하는데 사용된다. source 와 target 언어의 vocabulary는 서로 구분된다. \n","min_freq 인수를 사용하여 단어의 빈도 수가 2보다 작은 단어들은 처리하지 않는다. 다시 말해서 빈도 수가 최소 2보다 큰 단어들만 vocaburary에 추가하도록 최소 등장 빈도 조건을 추가하였다. 한번만 등장하는 단어들은 unknown token <unk>로 처리된다.\n","이때 우리 모델에서 정보 누출을 방지하기 위해 vocabulary는 오직 training set에서만 생성되어야 한다. 정보누출은 인위적으로 부풀려진 validation/test 점수를 제공하므로 방지되어야 한다.  "]},{"cell_type":"code","execution_count":8,"metadata":{"id":"2nw4WxgvMQ9o","executionInfo":{"status":"ok","timestamp":1665543248034,"user_tz":-540,"elapsed":310,"user":{"displayName":"ᄀᄒᄌ","userId":"07458837797284130526"}}},"outputs":[],"source":["SRC.build_vocab(train_data, min_freq = 2)\n","TRG.build_vocab(train_data, min_freq = 2)"]},{"cell_type":"markdown","metadata":{"id":"LqDBEHNfMQ9p"},"source":["선언한 device는 GPU로 설정이 되었으며, GPU를 사용할 수 없는 경우 CPU를 사용한다. "]},{"cell_type":"code","execution_count":9,"metadata":{"id":"yqGOHzgtMQ9p","executionInfo":{"status":"ok","timestamp":1665543248034,"user_tz":-540,"elapsed":3,"user":{"displayName":"ᄀᄒᄌ","userId":"07458837797284130526"}}},"outputs":[],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"markdown","metadata":{"id":"XiDp6IAIMQ9p"},"source":["BucketIterator를 생성한다. 이는 Pytorch의 dataloader와 쓰임새가 같다. 즉, 배치 크기 단위로 값을 차례대로 꺼내어 메모리로 가져오고 싶을 때 사용한다. 하지만 dataloader와는 다르게 비슷한 길이의 문장들끼리 batch를 만들기 때문에 padding의 개수를 최소화할 수 있다. \n","자연어 처리를 하다보면 각 문장의 길이가 서로 다를 수 있는데, 병렬 연산을 위해서는 여러 문장의 길이를 임의로 동일하게 맞춰주는 작업이 필요할 때가 있다. 이때 데이터에 특정값을 채워서 데이터의 크기(shape)를 조정하는 것을 padding 이라고 한다.  "]},{"cell_type":"code","execution_count":39,"metadata":{"id":"lxy5S5F3MQ9p","executionInfo":{"status":"ok","timestamp":1665545163701,"user_tz":-540,"elapsed":300,"user":{"displayName":"ᄀᄒᄌ","userId":"07458837797284130526"}}},"outputs":[],"source":["BATCH_SIZE = 128\n","#BATCH_SIZE=1\n","\n","train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n","    (train_data, valid_data, test_data), \n","    batch_size = BATCH_SIZE,\n","    device = device)"]},{"cell_type":"markdown","metadata":{"id":"OQqrynKXMQ9p"},"source":["## Building the Seq2Seq Model\n","\n","### Encoder\n","기존의 encoder에서와 동일하게 input sequence $x = (x_{1},..., x_{T_{x}})$가 encoder의 입력으로 제공된다. 그러나 한 방향만으로 input을 읽는 RNN과 달리 입력 문장에 대해서 이전에 나타나는 내용과 함께, 이후에 나타나는 내용을 읽는 bidirectional RNN(BiRNN)을 사용한다. \n","\n","BiRNN은 두 개의 RNN(forward RNN, backward RNN)으로 구성되며, Forward RNN은 처음부터 순차적으로 입력을 읽어서 hidden state를 생성한다. \n","\n","Backward RNN은 입력의 제일 뒤에서부터 역방향으로 입력을 읽어서 역방향의 hidden state를 생성한다. 아래의 식과 같이 time step j 에서의 입력 $x_j$에 대해서 forward hidden state $h_T^\\rightarrow$ 와 backward hidden state $h_T^\\leftarrow$ 를 연결해서(concat) j번째 hidden state를 생성한다. $$h_{j}=[h_j^\\rightarrow; h_j^\\leftarrow]^{T}$$ \n","인접한 state의 정보를 더 많이 가지고 있는 RNN의 특성상, $h_{j}$는 입력 단어 $x_{j}$의 양방향으로 더 가까운 위치에 있는 단어들의 정보를 더 많이 보유하게 된다. \n"]},{"cell_type":"code","execution_count":40,"metadata":{"id":"yj46s5nmMQ9q","executionInfo":{"status":"ok","timestamp":1665545168723,"user_tz":-540,"elapsed":282,"user":{"displayName":"ᄀᄒᄌ","userId":"07458837797284130526"}}},"outputs":[],"source":["class Encoder(nn.Module):\n","    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n","        super().__init__()\n","        '''\n","        nn.Embedding()은 룩업 테이블 과정을 수행한다. 즉, 특정 토큰과 맵핑되는 정수(이는 vocabulary에서 지정됨)를 인덱스로 가지는 \n","        테이블(임베딩 테이블)로부터 임베딩 벡터 값을 가져온다. nn.Embedding()은 크게 두 가지 인자를 받는데, 각각 input_dim과 emb_dim 이다. \n","        input_dim은 임베딩을 할 단어들의 개수로, 다시 말해 단어 집합의 크기이며, 임베딩 테이블은 인자로 받은 단어 집합의 크기만큼 행을 가진다. \n","        emb_dim은 특정 토큰과 맵핑되는 정수를 인덱스로 가지는 벡터(즉, 임베딩 벡터)의 차원에 해당하며 , 이는 사용자가 정해주는 하이퍼파라미터이다. \n","        '''\n","        self.embedding = nn.Embedding(input_dim, emb_dim)\n","        '''\n","        bidirectional = True로 설정하여 BiRNN을 구현한다. \n","        첫번째 인자는 input size를 설정한다. RNN cell에는 임베딩 벡터가 입력되므로 이 벡터의 차원에 해당되는 값을 전달해준다.\n","        두번째 인자에는 hidden state의 dimension을 전달해준다. BiRNN은 두 개의 RNN으로 구성되는데, 하나의 RNN에서 생성되는 hidden state의 차원에 해당한다. \n","        '''\n","        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n","        '''\n","        BiRNN이므로 fc(fully-connected) layer는 순방향 RNN과 역방향 RNN의 hidden state를 concat한 결과의 size를 input의 크기로 받는다. \n","        따라서 input의 size를 enc_hid_dim에 2를 곱해준 만큼의 크기로 설정하였다. 이는 후에 decoder의 초기 hidden state가 될 것이므로,\n","        nn.Linear 함수를 통해 decoder의 hidden state의 차원과 맞게끔 조절해준다. \n","        '''\n","        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n","        #dropout은 overfitting을 해결할 수 있는 방법 중 하나로, 일부 파라미터를 학습에 반영하지 않음으로써 모델을 일반화하는 방법이다. \n","        self.dropout = nn.Dropout(dropout)\n","        \n","    def forward(self, src):\n","        #src 텐서값 출력 \n","        #print(\"src\")\n","        #print(\"shape : \", src.shape)\n","        #print(src)\n","\n","        #src = [src len, batch size]\n","        '''\n","        src 텐서를 input으로 받으며 이를 임베딩하고, dropout을 적용한다. \n","        '''\n","        #임베딩된 텐서값 출력 \n","        embedded = self.embedding(src)\n","        #print(\"embedded before dropout\")\n","        #print(\"shape : \", embedded.shape)\n","        #print(embedded)\n","        embedded = self.dropout(embedded)\n","        \n","        #드롭 아웃 이후 임베딩 텐서값 출력 \n","        #print(\"embedded after dropout\")\n","        #print(\"shape : \", embedded.shape)\n","        #print(embedded)\n","\n","        #embedded = [src len, batch size, emb dim]\n","        '''\n","        outputs과 hidden은 rnn의 출력값이다\n","        output은 GPU의 hidden state를 모아놓은 것으로 세번째 차원 [hid dim*num directions]에서 첫번쨰는 forward RNN, 두번째는 backward RNN을 의미한다.\n","        hidden은 마지막(t=src len) hidden state 값으로, [n layers*num_directions , batch size , hid dim] 차원이다.  \n","\n","        '''\n","        outputs, hidden = self.rnn(embedded)\n","        #outputs 텐서와 hidden 텐서의 크기와 값 출력 \n","        #print(\"outputs\")\n","        #print(\"shape : \", outputs.shape)\n","        #print(outputs)\n","        #print(\"hidden\")\n","        #print(\"shape : \", hidden.shape)\n","        #print(hidden)        \n","        #outputs = [src len, batch size, hid dim * num directions]\n","        #hidden = [n layers * num directions, batch size, hid dim]\n","        \n","        #hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]\n","        #outputs are always from the last layer\n","        \n","        #hidden [-2, :, : ] is the last of the forwards RNN \n","        #hidden [-1, :, : ] is the last of the backwards RNN\n","        \n","        #initial decoder hidden is final hidden state of the forwards and backwards \n","        #  encoder RNNs fed through a linear layer\n","        '''\n","        순방향 hidden states와 역방향 hidden states를 concat을 하게 된다. \n","        hidden[-2,:,:]은 forwards RNN을, hidden[-1,:,:]은 backward RNN을 의미한다. Batch는 변하면 안되므로 Batch 차원은 유지(dim=1)한다. \n","        '''\n","        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n","        #FC layer를 통과한 후의 hidden 텐서의 크기와 값 출력\n","        #print(\"hidden after FC layer\")\n","        #print(\"shape : \", hidden.shape)\n","        #print(hidden)\n","\n","        #outputs = [src len, batch size, enc hid dim * 2]\n","        #hidden = [batch size, dec hid dim]\n","        \n","        return outputs, hidden"]},{"cell_type":"markdown","metadata":{"id":"bKN-M74lZ9Zi"},"source":["### 임베딩된 텐서값\n","\n","src\n","shape :  torch.Size([17, 1])\n","tensor([[   2],\n","        [   5],\n","        [  66],\n","        [  25],\n","        [ 443],\n","        [  23],\n","        [  22],\n","        [  14],\n","        [5620],\n","        [2165],\n","        [2408],\n","        [  10],\n","        [1910],\n","        [  23],\n","        [ 147],\n","        [   4],\n","        [   3]], device='cuda:0')\n","\n","src 텐서의 크기는 (src len, batch size)이며, src len은 sequence length 와 같다. 즉, 해당 문자열의 토큰의 개수이다. src는 벡터값이 아닌 스칼라 값의 list라는 것을 확인할 수 있는데, 이는 특정 토큰과 맵핑되는 정수를 가져온 것이기 때문이다.  "]},{"cell_type":"markdown","source":["### dropout 적용 전과 후의 임베딩된 텐서의 크기와 값\n","embedded before dropout\n","shape :  torch.Size([17, 1, 256])\n","tensor([[[ 0.0200,  0.0012,  0.0028,  ..., -0.0058, -0.0041, -0.0096]],\n","\n","        [[-0.0053,  0.0009, -0.0111,  ...,  0.0094, -0.0088,  0.0010]],\n","\n","        [[ 0.0132,  0.0101,  0.0139,  ...,  0.0018,  0.0006,  0.0002]],\n","\n","        ...,\n","\n","        [[-0.0118,  0.0077, -0.0036,  ...,  0.0030,  0.0109, -0.0012]],\n","\n","        [[-0.0139,  0.0171,  0.0039,  ...,  0.0028,  0.0143, -0.0139]],\n","\n","        [[ 0.0131,  0.0032, -0.0025,  ...,  0.0310,  0.0055, -0.0038]]],\n","       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n","embedded after dropout\n","shape :  torch.Size([17, 1, 256])\n","tensor([[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.0083,  0.0000]],\n","\n","        [[-0.0106,  0.0000, -0.0222,  ...,  0.0000,  0.0000,  0.0000]],\n","\n","        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0036,  0.0012,  0.0004]],\n","\n","        ...,\n","\n","        [[-0.0237,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.0023]],\n","\n","        [[-0.0278,  0.0000,  0.0000,  ...,  0.0000,  0.0286,  0.0000]],\n","\n","        [[ 0.0000,  0.0065,  0.0000,  ...,  0.0620,  0.0000,  0.0000]]],\n","       device='cuda:0', grad_fn=<FusedDropoutBackward0>)\n","\n","\n","임베딩된 텐서의 크기는 (src len, batch size, emb dim)임을 확인할 수 있다. \n","batch size와 emb dim은 각각 1과 256으로 설정해주었고, 입력된 문자열의 토큰의 개수가 12이기 때문에 src len이 12로 나왔다. \n","이를 통해, 특정 토큰과 맵핑되는 정수(이는 vocabulary에서 지정됨)를 인덱스로 가지는 테이블(임베딩 테이블)로부터 임베딩 벡터 값을 가져오는 룩업 테이블 과정이 수행되었음을 확인할 수 있다. \n","dropout 적용 전과 후의 차이점은 출력된 텐서값에서 확인할 수 있듯이 일부 값들이 0으로 대체되었다는 점이다. "],"metadata":{"id":"ilIhs5yMq3Fk"}},{"cell_type":"markdown","source":["### output 텐서와 hidden 텐서의 크기와 값\n","outputs과 hidden은 rnn의 출력값이다. output은 GPU의 hidden state를 모아놓은 것으로 세번째 차원 [hid dim*num directions]에서 첫번쨰는 forward RNN, 두번째는 backward RNN을 의미한다.\n","hidden은 마지막(t=src len) hidden state 값으로, [n layers*num_directions , batch size , hid dim] 차원이다.  \n","\n","outputs\n","shape :  torch.Size([17, 1, 1024])\n","tensor([[[ 1.5251e-03,  5.0332e-03, -1.8111e-03,  ..., -1.5339e-02,\n","          -1.0614e-02,  1.0925e-02]],\n","\n","        [[ 1.1445e-05,  5.9481e-03, -5.6105e-03,  ..., -1.7181e-02,\n","          -1.1947e-02,  1.0878e-02]],\n","\n","        [[-7.5090e-04,  7.2755e-03, -8.2482e-03,  ..., -1.6020e-02,\n","          -1.0675e-02,  1.2355e-02]],\n","\n","        ...,\n","\n","        [[ 2.3823e-03,  1.2973e-02, -9.9649e-03,  ..., -8.8709e-03,\n","          -3.8923e-03,  9.4221e-03]],\n","\n","        [[ 2.5274e-03,  1.4425e-02, -1.1705e-02,  ..., -6.3470e-03,\n","          -4.3193e-03,  5.7830e-03]],\n","\n","        [[ 2.0797e-04,  1.7569e-02, -1.3075e-02,  ..., -3.5139e-03,\n","          -2.6035e-03,  2.8823e-03]]], device='cuda:0',\n","       grad_fn=<CudnnRnnBackward0>)\n","hidden\n","shape :  torch.Size([2, 1, 512])\n","tensor([[[ 0.0002,  0.0176, -0.0131,  ...,  0.0091,  0.0095, -0.0116]],\n","\n","        [[ 0.0143,  0.0079, -0.0079,  ..., -0.0153, -0.0106,  0.0109]]],\n","       device='cuda:0', grad_fn=<CudnnRnnBackward0>)\n"],"metadata":{"id":"VusmTj1XsEyo"}},{"cell_type":"markdown","source":["### FC layer를 통과한 후의 hidden 텐서의 크기와 값 \n","순방향 hidden states와 역방향 hidden states를 concat을 하게 된다. \n","hidden[-2,:,:]은 forwards RNN을, hidden[-1,:,:]은 backward RNN을 의미한다. Batch는 변하면 안되므로 Batch 차원은 유지(dim=1)한다. \n","\n","hidden after FC layer\n","shape :  torch.Size([1, 512])\n","tensor([[-0.0279, -0.0243,  0.0289,  0.0258, -0.0362, -0.0199,  0.0279, -0.0300,\n","          0.0327, -0.0263, -0.0210, -0.0231, -0.0308,  0.0264,  0.0207, -0.0283,\n","         -0.0218, -0.0318,  0.0275,  0.0303,  0.0317, -0.0356, -0.0254, -0.0326,\n","          0.0164, -0.0264,  0.0086, -0.0279, -0.0191,  0.0213,  0.0230,  0.0251,\n","          0.0056,  0.0212,  0.0211,  0.0207, -0.0103,  0.0232, -0.0265,  0.0217,\n","          0.0086,  0.0206,  0.0171,  0.0209,  0.0247, -0.0303, -0.0214, -0.0238,\n","          0.0121,  0.0313, -0.0206,  0.0292,  0.0263,  0.0258,  0.0271, -0.0332,\n","         -0.0349,  0.0161, -0.0306,  0.0271, -0.0227, -0.0281, -0.0240, -0.0305,\n","          0.0237,  0.0349, -0.0270, -0.0297,  0.0195, -0.0269, -0.0032,  0.0119,\n","         -0.0316, -0.0178,  0.0294, -0.0210, -0.0325, -0.0156, -0.0282, -0.0283,\n","         -0.0273, -0.0294,  0.0147, -0.0246, -0.0285, -0.0331, -0.0225, -0.0281,\n","          0.0260, -0.0256, -0.0283,  0.0295,  0.0342, -0.0242, -0.0189, -0.0045,\n","          0.0306, -0.0275, -0.0056,  0.0152,  0.0008,  0.0300,  0.0325, -0.0247,\n","         -0.0276,  0.0266,  0.0270, -0.0242,  0.0202,  0.0241, -0.0376,  0.0265,\n","          0.0263,  0.0129, -0.0310, -0.0312, -0.0283,  0.0287,  0.0281,  0.0265,\n","         -0.0270,  0.0216, -0.0309,  0.0101, -0.0285, -0.0300, -0.0303,  0.0295,\n","          0.0332,  0.0239,  0.0306,  0.0208, -0.0084,  0.0263, -0.0314, -0.0155,\n","         -0.0190,  0.0319, -0.0291,  0.0205,  0.0170, -0.0215,  0.0312, -0.0206,\n","          0.0263,  0.0239,  0.0231,  0.0291,  0.0285,  0.0294, -0.0241,  0.0245,\n","         -0.0049, -0.0221, -0.0214,  0.0307,  0.0072,  0.0292,  0.0328, -0.0281,\n","         -0.0260,  0.0267, -0.0277,  0.0236,  0.0282,  0.0156, -0.0302,  0.0205,\n","         -0.0260,  0.0346, -0.0279,  0.0283, -0.0075, -0.0290, -0.0245,  0.0093,\n","          0.0067, -0.0248, -0.0239, -0.0130, -0.0324, -0.0274, -0.0259,  0.0334,\n","          0.0220,  0.0259,  0.0224,  0.0300, -0.0248,  0.0279, -0.0258, -0.0271,\n","         -0.0255,  0.0127, -0.0139,  0.0222, -0.0057,  0.0214, -0.0293,  0.0005,\n","          0.0280,  0.0241,  0.0254, -0.0275,  0.0243,  0.0126, -0.0267,  0.0153,\n","          0.0330,  0.0295, -0.0229,  0.0103, -0.0276, -0.0187,  0.0241, -0.0316,\n","          0.0136,  0.0229,  0.0265, -0.0249,  0.0275,  0.0334, -0.0295, -0.0275,\n","          0.0067,  0.0026, -0.0278, -0.0089, -0.0244,  0.0248,  0.0218, -0.0302,\n","          0.0320,  0.0003, -0.0243, -0.0003,  0.0334,  0.0278,  0.0294,  0.0270,\n","          0.0266, -0.0230,  0.0266, -0.0290,  0.0274,  0.0315, -0.0269, -0.0217,\n","          0.0277,  0.0284, -0.0206, -0.0262,  0.0270,  0.0239, -0.0104,  0.0283,\n","          0.0269,  0.0116, -0.0200, -0.0199, -0.0314,  0.0332,  0.0291, -0.0259,\n","         -0.0305, -0.0153,  0.0315,  0.0297, -0.0135, -0.0253, -0.0287,  0.0207,\n","         -0.0288, -0.0298, -0.0239, -0.0167, -0.0317,  0.0254, -0.0243,  0.0023,\n","         -0.0202,  0.0282,  0.0268,  0.0225,  0.0234,  0.0201,  0.0277, -0.0237,\n","         -0.0130,  0.0266,  0.0185,  0.0257,  0.0258, -0.0171, -0.0342, -0.0289,\n","          0.0271, -0.0248, -0.0043, -0.0043,  0.0099,  0.0211,  0.0105,  0.0022,\n","          0.0152,  0.0294,  0.0269, -0.0048,  0.0006,  0.0272,  0.0262, -0.0300,\n","          0.0252,  0.0095, -0.0016, -0.0271, -0.0225, -0.0251, -0.0342, -0.0149,\n","          0.0279, -0.0255, -0.0198, -0.0303, -0.0331,  0.0242, -0.0298,  0.0150,\n","          0.0357,  0.0163,  0.0275, -0.0313,  0.0192, -0.0329, -0.0295, -0.0184,\n","          0.0309,  0.0028,  0.0371, -0.0361,  0.0300, -0.0220, -0.0247,  0.0257,\n","          0.0259,  0.0243,  0.0323,  0.0248, -0.0248, -0.0340,  0.0166,  0.0319,\n","          0.0300, -0.0323,  0.0170, -0.0181,  0.0105, -0.0331, -0.0354, -0.0277,\n","          0.0166,  0.0233, -0.0254,  0.0293, -0.0171, -0.0269, -0.0121,  0.0304,\n","         -0.0286, -0.0328,  0.0248, -0.0264, -0.0288,  0.0294,  0.0176,  0.0339,\n","         -0.0205, -0.0105, -0.0361,  0.0268, -0.0222,  0.0182, -0.0324, -0.0217,\n","          0.0281,  0.0264,  0.0264, -0.0253, -0.0373,  0.0194, -0.0306,  0.0360,\n","         -0.0304,  0.0309, -0.0239,  0.0256,  0.0243,  0.0068, -0.0035,  0.0274,\n","          0.0269, -0.0272, -0.0262, -0.0150,  0.0258, -0.0050, -0.0191, -0.0225,\n","          0.0181, -0.0198,  0.0288, -0.0116, -0.0335,  0.0259, -0.0273,  0.0234,\n","          0.0240, -0.0239, -0.0260,  0.0259, -0.0311, -0.0327,  0.0270, -0.0220,\n","          0.0204, -0.0206, -0.0250, -0.0228, -0.0255, -0.0288,  0.0265, -0.0264,\n","          0.0030,  0.0269,  0.0106,  0.0255, -0.0270,  0.0279, -0.0011, -0.0328,\n","         -0.0280, -0.0072, -0.0214,  0.0141, -0.0160,  0.0226, -0.0271,  0.0246,\n","          0.0218,  0.0142, -0.0270,  0.0303,  0.0303, -0.0086, -0.0294, -0.0293,\n","          0.0298, -0.0300, -0.0265,  0.0232,  0.0233, -0.0251,  0.0233,  0.0291,\n","         -0.0145, -0.0293, -0.0298,  0.0187,  0.0198, -0.0266,  0.0069,  0.0273,\n","         -0.0243,  0.0082,  0.0263,  0.0286,  0.0294,  0.0330, -0.0046, -0.0192,\n","          0.0339,  0.0212, -0.0147,  0.0298, -0.0309, -0.0088, -0.0306,  0.0280,\n","          0.0282,  0.0125,  0.0266,  0.0304,  0.0218, -0.0312,  0.0342,  0.0292,\n","          0.0247, -0.0286, -0.0151, -0.0172,  0.0221, -0.0280,  0.0140,  0.0203,\n","          0.0265, -0.0290,  0.0231,  0.0286,  0.0112,  0.0188, -0.0270,  0.0308]],\n","       device='cuda:0', grad_fn=<TanhBackward0>)"],"metadata":{"id":"ylpO8gzOtLW9"}},{"cell_type":"markdown","metadata":{"id":"lugWSGa8MQ9q"},"source":["### Attention\n","\n","\n","Time step i에서 decoder의 hidden state를 $s_{i}$이라고 한다. Alignment model에서는 decoder의 time step i에서의 정보가 encoder의 time step j에서의 정보와 얼마나 연관성이 있는지 score를 계산한다. 해당 score를 구하기 위해서는 decoder의 바로 전 time step (i-1)에서의 hidden state $s_{t-1}$ 와 encoder의 time step j에서의 hidden state $h_{j}$를 이용해 아래의 식과 같은 연산을 수행한다. $$e_{ij} = a(s_{i-1}, h_{j})$$ 이 때 alignment model인 a()는 feedforward neural network라고 할 수 있다. \n","\n","이후, time step i 에서의 context vector인 $c_{i}$를 생성한다. $c_{i}$는 아래의 식과 같이 encoder의 모든 hidden state의 weighted sum으로 구성된다. $$c_{i} = \\sum_{j = 1}^{T_{x}}{\\alpha_{ij}h_{j}}$$이때, 여기서 등장하는 $\\alpha_{ij}$가 번역 단어를 생성해 낼때의 attention에 해당한다. encoder의 각 hidden state에 대한 weight $\\alpha_{ij}$는 아래의 식과 같이 Alignment model에서 구한 score $e_{ij}$를 이용해 구해진다. $$\\alpha_{ij} = \\frac{exp(e_{ij})}{\\sum_{k = 1}^{T_{x}}{exp(e_{ik})}}$$ 해당 weight $\\alpha_{ij}$는 target word $y_{i}$가 source word $x_{j}$에 어느정도 연관이 있는지 나타낸다. 그래서 decoder는 해당 weight 값을 기반으로 source sentence에서 어떤 위치의 단어에 더 attention을 줄지 판단할 수 있다. 이러한 방식으로 인해, 기조에 모든 문장을 하나의 고정 길이 벡터로 변환하는 작업을 수행하지 않고도 더 좋은 성능을 제공할 수 있다. "]},{"cell_type":"code","execution_count":41,"metadata":{"id":"U71CX1AKMQ9q","executionInfo":{"status":"ok","timestamp":1665545175317,"user_tz":-540,"elapsed":322,"user":{"displayName":"ᄀᄒᄌ","userId":"07458837797284130526"}}},"outputs":[],"source":["class Attention(nn.Module):\n","    def __init__(self, enc_hid_dim, dec_hid_dim):\n","        super().__init__()\n","        \n","        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n","        self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n","        \n","    def forward(self, hidden, encoder_outputs):\n","        \n","        #hidden = [batch size, dec hid dim]\n","        #encoder_outputs = [src len, batch size, enc hid dim * 2]\n","        \n","        batch_size = encoder_outputs.shape[1]\n","        src_len = encoder_outputs.shape[0]\n","        \n","        #repeat decoder hidden state src_len times\n","        '''\n","        인코더의 hidden state는 src_len개 tensor의 sequence이고, 디코더의 hidden state는 [batch size, dec hid dim]의 single vector이므로,\n","        길이를 맞춰주어야 한다. 이를 위해 unsqueeze(1)를 하여 [bacth size, 1, dec hid dim]로 바꾸고 src_len 번 repeat(1, T, 1) 한다. \n","        그러면 [batch size, src len, dec hid dim]이 될 것이다. \n","        encoder_outputs는 [src len, batch size, enc hid dim*2]의 차원을 갖고 있다. 이를 concat하고, FC에 feed하여 attn_dim으로 나타내기 \n","        위해 torch.Tensor.permute를 통해 텐서 차원끼리 교환한다. 이 결과 [batch size, src len, enc hid dim*2] 차원이 된다.  \n","\n","        '''\n","        #hidden의 크기를 확인\n","        #print(\"hidden : \", hidden.shape)\n","        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n","        #print(\"hidden_unsqueeze: \", hidden.shape)\n","        \n","\n","        #encoder_outputs의 크기를 확인 \n","        #print(\"encoder_outputs : \", encoder_outputs.shape)\n","        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n","        #print(\"encoder_outputs_permute : \", encoder_outputs.shape)\n","\n","        #hidden = [batch size, src len, dec hid dim]\n","        #encoder_outputs = [batch size, src len, enc hid dim * 2]\n","        '''\n","        이후 앞에서 구한 hidden과 encoder_outputs을 concat한다. [batch size, src len, dec hid dim ; batch size, src len, enc hid dim * 2]이므로,\n","        [batch size, src len, enc hid dim * 2 + dec hid dim]이 될 것이다. \n","        이제 energy를 계산한다. energy는 FC 인 self.attn 을 통과하여 얻는다. 차원은 [batch size, src len, dec hid dim]이다. 그 후 tanh를 통과한다.   \n","        '''\n","        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2))) \n","        #energy의 크기와 값을 확인 \n","        #print(\"energy\")\n","        #print(\"shape : \", energy.shape)\n","        #print(energy)\n","        #energy = [batch size, src len, dec hid dim]\n","\n","        attention = self.v(energy).squeeze(2)\n","        attention = F.softmax(attention, dim=1)\n","        #print(\"attention\")\n","        #print(\"shape: \", attention.shape)\n","        #print(attention)\n","        #attention= [batch size, src len]\n","        '''\n","        energy e_ij에 softmax를 취해 alpha_ij를 얻는다. \n","        어떤 i와 모든 j(1, ..., T_x)에 대해 alpha_ij 를 얻을 수 있다.  \n","        '''\n","        return attention"]},{"cell_type":"markdown","source":["###hidden의 크기 \n","인코더의 hidden state는 src_len개 tensor의 sequence이고, 디코더의 hidden state는 [batch size, dec hid dim]의 single vector이므로,길이를 맞춰주어야 한다. 이를 위해 unsqueeze(1)를 하여 [bacth size, 1, dec hid dim]로 바꾸고 src_len 번 repeat(1, T, 1) 한다. 그러면 [batch size, src len, dec hid dim]이 될 것이다. \n","\n","\n","hidden :  torch.Size([1, 512])\n","hidden_unsqueeze:  torch.Size([1, 17, 512])\n"],"metadata":{"id":"yibBhvnQt50p"}},{"cell_type":"markdown","source":["### encoder_outputs의 크기 \n","encoder_outputs는 [src len, batch size, enc hid dim*2]의 차원을 갖고 있다. 이를 concat하고, FC에 feed하여 attn_dim으로 나타내기 위해 torch.Tensor.permute를 통해 텐서 차원끼리 교환한다. 이 결과 [batch size, src len, enc hid dim*2] 차원이 된다.  \n","\n","\n","encoder_outputs :  torch.Size([17, 1, 1024])\n","encoder_outputs_permute :  torch.Size([1, 17, 1024])"],"metadata":{"id":"XfsyrQMgu8nf"}},{"cell_type":"markdown","source":["### energy와 attention의 크기와 값\n","\n","energy는 FC 인 self.attn 을 통과하여 얻는다. 차원은 [batch size, src len, dec hid dim]이다. energy는 decoder의 time step i에서의 정보가 encoder의 time step j에서의 정보와 얼마나 연관성이 있는지를 표현한 score에 해당한다. \n","\n","energy\n","shape :  torch.Size([1, 17, 512])\n","tensor([[[-0.0187,  0.0119, -0.0107,  ...,  0.0186,  0.0094,  0.0255],\n","         [-0.0198,  0.0156, -0.0126,  ...,  0.0214,  0.0108,  0.0276],\n","         [-0.0194,  0.0160, -0.0140,  ...,  0.0228,  0.0133,  0.0289],\n","         ...,\n","         [-0.0188,  0.0147, -0.0101,  ...,  0.0182,  0.0126,  0.0260],\n","         [-0.0184,  0.0149, -0.0110,  ...,  0.0179,  0.0133,  0.0256],\n","         [-0.0172,  0.0125, -0.0091,  ...,  0.0150,  0.0125,  0.0232]]],\n","       device='cuda:0', grad_fn=<TanhBackward0>)\n","\n","\n","energy e_ij에 softmax를 취해 alpha_ij를 얻는다. \n","\n","attention\n","shape:  torch.Size([1, 17])\n","tensor([[0.0579, 0.0585, 0.0589, 0.0592, 0.0592, 0.0594, 0.0594, 0.0595, 0.0591,\n","         0.0589, 0.0586, 0.0590, 0.0587, 0.0587, 0.0583, 0.0585, 0.0581]],\n","       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n"],"metadata":{"id":"eKAnkBehv0-z"}},{"cell_type":"markdown","source":["### attention matrix의 학습 방법\n"," Attention 을 계산할 때의 softmax 역시 각 row 별로 이뤄집니다. 그리고 여기서 만들어진 \n","(r,h) 크기의 sentence representation matrix 즉, attention matrix 를 \n","(1,r×h) 의 flatten vector 로 만들어 classifier 에 입력합니다.\n","그 후 attention matrix의 각 row들, 즉 r개의 관점들이 서로 가까워 지도록 유도하여 잘 학습되게 하기 위해서 regularization term을 추가한다. "],"metadata":{"id":"70CwYeaHZB93"}},{"cell_type":"markdown","metadata":{"id":"Dg-rwPv8MQ9r"},"source":["### Decoder\n","\n","decoder는 weight  $\\alpha_{ij}$ 값을 기반으로 source sentence에서 어떤 위치의 단어에 더 attention 을 줄지 판단할 수 있다. 이렇게 가중치를 곱해주며 각각의 output을 생성할 때마다 context vector를 만들면, 기존의 모든 문장을 하나의 고정 길이 벡터로 변환하는 작업을 수행하지 않고도 더 좋은 성능을 제공할 수 있다. \n"]},{"cell_type":"code","execution_count":42,"metadata":{"id":"b1LdwjqyMQ9r","executionInfo":{"status":"ok","timestamp":1665545178680,"user_tz":-540,"elapsed":296,"user":{"displayName":"ᄀᄒᄌ","userId":"07458837797284130526"}}},"outputs":[],"source":["class Decoder(nn.Module):\n","    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n","        super().__init__()\n","\n","        self.output_dim = output_dim\n","        self.attention = attention\n","        '''\n","        encoder의 결과인 hidden vector와 output vector, attention의 attention score를 받아 번역할 언어의 단어를 차례대로 반환한다. \n","        따라서 TRG 언어의 임베딩이 필요할 것이다. output_dim은 TRG 언어의 look-up words의 개수, emb_dim은 임베딩 벡터의 차원이다. \n","        '''\n","        self.embedding = nn.Embedding(output_dim, emb_dim)\n","        \n","        '''\n","        이후엔 encoder와 마찬가지로 GRU를 이용해 번역합니다. nn.Linear 함수의 첫번째 인자에서 (enc_hid_dim * 2) + dec_hid_dim 는 \n","        인코더의 context vector로부터 decoder의 attention score를 계산하는 layer의 input dimension에 해당한다. \n","        '''\n","        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n","        \n","        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n","        \n","        self.dropout = nn.Dropout(dropout)\n","        \n","    def forward(self, input, hidden, encoder_outputs):\n","             \n","        #input = [batch size]\n","        #hidden = [batch size, dec hid dim]\n","        #encoder_outputs = [src len, batch size, enc hid dim * 2]\n","        \n","        input = input.unsqueeze(0)\n","        #input의 크기와 값 확인 \n","        #print(\"input\")\n","        #print(\"shape : \", input.shape)\n","        #print(input) \n","\n","        #input = [1, batch size]\n","        \n","        embedded = self.dropout(self.embedding(input))\n","        #embedded의 크기와 값 확인\n","        #print(\"embedded\")\n","        #print(\"shape : \", embedded.shape)\n","        #print(embedded)\n","        #embedded = [1, batch size, emb dim]\n","        '''\n","        attention의 결과로 얻어지는 attention vector a는 차원이 [batch size, src len]이기 때문에 이를 [batchj size, 1, src len]로 바꾸어 준다. \n","        '''\n","        #a의 크기와 값 확인 \n","\n","        a = self.attention(hidden, encoder_outputs)\n","        \n","        #print(\"a\")\n","        #print(\"shape : \", a.shape)\n","        #print(a)\n","\n","        #a = [batch size, src len]\n","        \n","        a = a.unsqueeze(1)\n","\n","        #print(\"a_unsqueeze\")\n","        #print(\"shape : \", a.shape)\n","        #print(a)\n","        #a = [batch size, 1, src len]\n","        '''\n","        이후, Batch matrix multiplication을 하기 위해 permute(1,0,2)를 이용해 stacked hidden state인 encoder_outputs의 차원을 \n","        [src len, batch size, enc hid dim * 2]에서 [batch size, src len, enc hid dim * 2]로 바꾸어 준다. \n","        '''\n","        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n","        \n","        #encoder_outputs = [batch size, src len, enc hid dim * 2]\n","        '''\n","        weighted 는 앞에서 언급한 context vector c_i에 해당한다. \n","        [batch size, 1, src len]와 [batch size, src len, enc hid dim * 2] 의 배치곱이므로 , 결과는 [batch size, 1, enc hid dim * 2]가 된다.\n","        이를 다시 [1, batch size, enc hid dim *2]차원으로 바꾸어 준다. \n","        '''\n","        weighted = torch.bmm(a, encoder_outputs)\n","        #weighted의 크기와 값을 확인\n","        #print(\"weighted\")\n","        #print(\"shape : \", weighted.shape)\n","        #print(weighted)\n","        \n","        #weighted = [batch size, 1, enc hid dim * 2]\n","        \n","        weighted = weighted.permute(1, 0, 2)\n","        \n","        #weighted = [1, batch size, enc hid dim * 2]\n","        \n","        '''\n","        임베딩된 input word인 embedded 와 context vector c_i에 해당되는 weighted는 concat 된 후, 이전 시점의 디코더의 hidden state\n","        와 함께 디코더 RNN으로 전단된다. \n","        '''\n","        rnn_input = torch.cat((embedded, weighted), dim = 2)\n","        \n","        #rnn_input = [1, batch size, (enc hid dim * 2) + emb dim]\n","        '''\n","        decoder의 이전 시점의 hidden state에 해당하는 \"hidden\"은 encoder의 hidden으로 [batch size, dec hid dim]이다. 이를 decoder의 GRU에 넣기 위해\n","        unsqueeze(0)을 하여, [1, batch size, dec hid dim]을 얻는다. 마찬가지로 1은 src len 에 해당한다. \n","        output은 마찬가지로 hidden state 의 집합, hidden은 마지막 hidden state에 해당한다. \n","        '''    \n","        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n","        #output과 hidden의 크기와 값 확인 \n","        #print(\"output\")\n","        #print(\"shape : \", output.shape)\n","        #print(output)\n","        #print(\"hidden\")\n","        #print(\"shape : \", hidden.shape)\n","        #print(hidden)\n","        #output = [seq len, batch size, dec hid dim * n directions]\n","        #hidden = [n layers * n directions, batch size, dec hid dim]\n","        \n","        #seq len, n layers and n directions will always be 1 in this decoder, therefore:\n","        #output = [1, batch size, dec hid dim]\n","        #hidden = [1, batch size, dec hid dim]\n","        #this also means that output == hidden\n","        assert (output == hidden).all()\n","        '''\n","        그 후엔 FC layer에 임베딩된 input word와 weigthed(context vector c_i), 이전 시점의 디코더의 hidden state를 모두 concat한 결과를 전달하여 \n","        target sentence \"prediction\"을 예측한다.  \n","        src len은 전부 1이므로 이를 squeeze(0)하고 concat 한 후 FC에 넣는다. concat은 batch size를 중심으로 이루어진다. \n","        output은 [batch size, output dim]가 된다. Decoder의 결과는 이 output과 decoder의 hidden state인 hidden을 squeeze(0) 한 것이다. \n","        '''\n","        embedded = embedded.squeeze(0)\n","        output = output.squeeze(0)\n","        weighted = weighted.squeeze(0)\n","        \n","        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim = 1))\n","        #prediction 의 크기와 값 확인\n","        #print(\"prediction\")\n","        #print(\"shape : \", prediction.shape)\n","        #print(prediction)\n","\n","        #prediction = [batch size, output dim]\n","        \n","        return prediction, hidden.squeeze(0)"]},{"cell_type":"markdown","source":["### input의 크기와 값 \n","input은 vocabulary에서 가져온 토큰과 맵핑되는 정수에 해당한다. decoder에서는 이전 단어를 통해 다음 단어에 해당하는 출력으 예측하기 때문에 incoder에서와 같이 정수의 list가 아닌 하나의 정수이다.   \n","\n","input\n","shape :  torch.Size([1, 1])\n","tensor([[2]], device='cuda:0')\n","embedded\n","shape :  torch.Size([1, 1, 256])\n","tensor([[[ 0.0260,  0.0000,  0.0000, -0.0175,  0.0206,  0.0000,  0.0083,\n","          -0.0282,  0.0161, -0.0141,  0.0225,  0.0000,  0.0412,  0.0000,\n","           0.0281,  0.0000,  0.0000,  0.0000,  0.0000, -0.0158, -0.0174,\n","           0.0000, -0.0134,  0.0000,  0.0396, -0.0686,  0.0000,  0.0000,\n","          -0.0255, -0.0293,  0.0079,  0.0000,  0.0000,  0.0000,  0.0320,\n","           0.0000,  0.0000,  0.0000,  0.0236,  0.0000, -0.0123,  0.0000,\n","          -0.0098,  0.0281,  0.0000,  0.0000,  0.0215, -0.0467, -0.0377,\n","           0.0000,  0.0068, -0.0030,  0.0242,  0.0000, -0.0147,  0.0000,\n","           0.0000,  0.0000,  0.0024,  0.0254,  0.0141,  0.0000,  0.0000,\n","          -0.0018,  0.0000,  0.0000,  0.0000, -0.0242,  0.0089, -0.0416,\n","           0.0000, -0.0241, -0.0008, -0.0118, -0.0347,  0.0026,  0.0139,\n","           0.0003, -0.0177,  0.0000,  0.0000,  0.0113,  0.0101,  0.0147,\n","           0.0000,  0.0316, -0.0031,  0.0094,  0.0000,  0.0000,  0.0121,\n","           0.0011,  0.0000,  0.0000, -0.0221, -0.0193,  0.0000, -0.0218,\n","           0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0006,  0.0000,\n","           0.0000, -0.0214,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","           0.0000, -0.0163, -0.0193, -0.0035,  0.0314, -0.0147,  0.0000,\n","           0.0272,  0.0041,  0.0052,  0.0000, -0.0301,  0.0116,  0.0000,\n","           0.0000,  0.0046, -0.0075,  0.0000,  0.0000,  0.0194,  0.0000,\n","           0.0000,  0.0000,  0.0000, -0.0111,  0.0000,  0.0069,  0.0000,\n","           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0195, -0.0177,\n","           0.0000,  0.0198,  0.0000,  0.0000,  0.0027,  0.0000,  0.0231,\n","           0.0035,  0.0251,  0.0000,  0.0000,  0.0000, -0.0242,  0.0000,\n","           0.0000, -0.0223,  0.0000,  0.0000,  0.0060,  0.0000,  0.0000,\n","           0.0232,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","           0.0000,  0.0000,  0.0000,  0.0092,  0.0098, -0.0006,  0.0000,\n","          -0.0047,  0.0000,  0.0000,  0.0000,  0.0000,  0.0032,  0.0000,\n","           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","           0.0297,  0.0000,  0.0000,  0.0002,  0.0000,  0.0000,  0.0199,\n","          -0.0316,  0.0351,  0.0000, -0.0309,  0.0000,  0.0103,  0.0000,\n","          -0.0032,  0.0000,  0.0000, -0.0085,  0.0000, -0.0242, -0.0093,\n","          -0.0090,  0.0274,  0.0000, -0.0310, -0.0304,  0.0000,  0.0000,\n","           0.0000, -0.0088,  0.0000,  0.0000,  0.0000,  0.0143,  0.0000,\n","          -0.0006,  0.0126,  0.0000,  0.0000, -0.0089, -0.0320,  0.0003,\n","           0.0000,  0.0252, -0.0488,  0.0000,  0.0000,  0.0000, -0.0119,\n","           0.0079,  0.0000,  0.0000,  0.0000,  0.0000, -0.0519, -0.0005,\n","           0.0055,  0.0076, -0.0294, -0.0024]]], device='cuda:0',\n","       grad_fn=<FusedDropoutBackward0>)\n","hidden :  torch.Size([1, 512])\n","hidden_unsqueeze:  torch.Size([1, 17, 512])\n","encoder_outputs :  torch.Size([17, 1, 1024])\n","encoder_outputs_permute :  torch.Size([1, 17, 1024])\n","energy\n","shape :  torch.Size([1, 17, 512])\n","tensor([[[-0.0187,  0.0119, -0.0107,  ...,  0.0186,  0.0094,  0.0255],\n","         [-0.0198,  0.0156, -0.0126,  ...,  0.0214,  0.0108,  0.0276],\n","         [-0.0194,  0.0160, -0.0140,  ...,  0.0228,  0.0133,  0.0289],\n","         ...,\n","         [-0.0188,  0.0147, -0.0101,  ...,  0.0182,  0.0126,  0.0260],\n","         [-0.0184,  0.0149, -0.0110,  ...,  0.0179,  0.0133,  0.0256],\n","         [-0.0172,  0.0125, -0.0091,  ...,  0.0150,  0.0125,  0.0232]]],\n","       device='cuda:0', grad_fn=<TanhBackward0>)\n","attention\n","shape:  torch.Size([1, 17])\n","tensor([[0.0579, 0.0585, 0.0589, 0.0592, 0.0592, 0.0594, 0.0594, 0.0595, 0.0591,\n","         0.0589, 0.0586, 0.0590, 0.0587, 0.0587, 0.0583, 0.0585, 0.0581]],\n","       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n","\n","\n","\n","\n"],"metadata":{"id":"tXUGBClKwQM7"}},{"cell_type":"markdown","source":["### a의 크기와 값\n","attention의 결과로 얻어지는 attention vector a는 차원이 [batch size, src len]이기 때문에 이를 [batchj size, 1, src len]로 바꾸어 준다.\n","\n","a\n","shape :  torch.Size([1, 17])\n","tensor([[0.0579, 0.0585, 0.0589, 0.0592, 0.0592, 0.0594, 0.0594, 0.0595, 0.0591,\n","         0.0589, 0.0586, 0.0590, 0.0587, 0.0587, 0.0583, 0.0585, 0.0581]],\n","       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n","a_unsqueeze\n","shape :  torch.Size([1, 1, 17])\n","tensor([[[0.0579, 0.0585, 0.0589, 0.0592, 0.0592, 0.0594, 0.0594, 0.0595,\n","          0.0591, 0.0589, 0.0586, 0.0590, 0.0587, 0.0587, 0.0583, 0.0585,\n","          0.0581]]], device='cuda:0', grad_fn=<UnsqueezeBackward0>)"],"metadata":{"id":"HhXx6knwwQBj"}},{"cell_type":"markdown","source":["### weighted의 크기와 값\n","weighted 는 앞에서 언급한 context vector c_i에 해당한다. [batch size, 1, src len]와 [batch size, src len, enc hid dim * 2] 의 배치곱이므로 , 결과는 [batch size, 1, enc hid dim * 2]가 된다.\n","\n","weighted\n","shape :  torch.Size([1, 1, 1024])\n","tensor([[[ 0.0015,  0.0127, -0.0103,  ..., -0.0120, -0.0075,  0.0108]]],\n","       device='cuda:0', grad_fn=<BmmBackward0>)"],"metadata":{"id":"xHogfh0JxyDO"}},{"cell_type":"markdown","source":["### output과 hidden의 크기와 값\n","decoder의 이전 시점의 hidden state에 해당하는 \"hidden\"은 encoder의 hidden으로 [batch size, dec hid dim]이다. 이를 decoder의 GRU에 넣기 위해 unsqueeze(0)을 하여, [1, batch size, dec hid dim]을 얻는다. 마찬가지로 1은 src len 에 해당한다. output은 마찬가지로 hidden state 의 집합, hidden은 마지막 hidden state에 해당한다.  \n","\n","\n","output\n","shape :  torch.Size([1, 1, 512])\n","tensor([[[ 2.2570e-03, -3.8253e-02,  3.7735e-02,  2.5970e-02, -4.4589e-02,\n","          -3.0273e-02,  3.6394e-02, -3.1717e-02,  3.9863e-02, -1.5255e-02,\n","          -3.3338e-02, -1.7419e-02, -3.6518e-02,  3.4689e-02,  2.8800e-02,\n","          -4.0039e-02, -2.2648e-02, -3.6061e-02,  3.0145e-02,  1.6217e-02,\n","           4.0167e-02, -4.4061e-02, -3.1047e-02, -3.9956e-02, -5.9814e-03,\n","          -3.4440e-02,  1.6144e-02, -3.3578e-02, -2.6352e-02,  3.0678e-02,\n","           3.3989e-02,  3.0273e-02,  1.6516e-02,  1.0487e-04, -7.4943e-03,\n","          -6.6586e-04,  1.1805e-02,  3.0889e-02, -3.6117e-02,  3.9597e-02,\n","          -1.6291e-02,  2.7696e-02,  2.6594e-02,  2.5061e-02,  7.6721e-03,\n","          -3.5730e-02, -3.3831e-02,  2.4523e-03, -5.5764e-03,  9.9474e-03,\n","          -2.8743e-02,  3.5365e-02,  3.1851e-02,  3.0036e-02,  3.1861e-02,\n","          -3.9897e-02, -4.6914e-02,  2.7055e-02, -1.9682e-02,  9.5936e-03,\n","          -3.4327e-02, -3.9540e-02, -3.6293e-02, -4.0675e-02,  3.5316e-02,\n","           6.7988e-03, -3.0380e-02, -2.5303e-02,  3.0929e-02, -3.3368e-02,\n","           1.8965e-02, -6.6384e-03, -4.0935e-02,  1.0547e-02,  3.8551e-02,\n","          -2.1062e-02, -1.7702e-02, -2.3691e-02, -3.2288e-02, -3.2095e-02,\n","          -3.0837e-02, -2.9068e-02,  2.7641e-02, -2.1464e-02, -3.8175e-02,\n","          -3.9865e-02, -2.0241e-02, -3.7060e-02,  3.6387e-02, -2.9912e-02,\n","          -7.5894e-03,  3.3749e-02,  2.7372e-02, -2.7908e-02, -2.8593e-02,\n","          -2.0040e-02,  3.2119e-02, -3.9205e-02, -2.2092e-02,  2.6401e-03,\n","           1.8424e-02,  4.3818e-02,  4.4867e-03, -3.9112e-02,  9.3802e-05,\n","           3.4001e-02,  2.9565e-02, -3.3107e-02,  2.6531e-02,  2.9561e-02,\n","          -4.1857e-02,  3.5857e-02,  3.2084e-02, -1.1381e-02, -2.0387e-02,\n","          -2.8603e-02, -2.8873e-02,  2.3737e-02,  3.8286e-02,  3.5980e-02,\n","          -3.4494e-02,  3.3493e-02, -2.2277e-02,  2.1115e-02, -7.6063e-03,\n","          -4.0252e-02, -3.4443e-02,  3.7216e-02,  4.0882e-02,  3.4942e-02,\n","           6.9950e-03, -2.9300e-03, -2.4350e-02,  3.2026e-02, -4.1983e-02,\n","          -2.6793e-02, -3.3058e-02,  3.8543e-02, -3.3286e-02,  3.0363e-02,\n","          -8.0392e-04, -3.2993e-02,  3.3476e-02, -1.8195e-02,  3.3718e-02,\n","           3.2278e-02,  3.0087e-02,  8.2021e-03,  3.6994e-02,  3.8671e-02,\n","          -3.3288e-02,  3.0148e-03, -2.2125e-02, -3.1868e-02, -2.1835e-02,\n","           3.4906e-02, -1.4122e-02,  3.1666e-02,  3.5217e-02, -3.3022e-02,\n","          -3.2342e-02,  3.6243e-02, -2.2592e-02,  3.4220e-02,  3.8984e-02,\n","           2.9320e-02, -3.9222e-02,  2.9824e-02, -3.3082e-02,  3.8918e-02,\n","          -2.7739e-02,  3.6310e-02, -1.0910e-02, -1.1915e-02, -2.9959e-02,\n","          -4.2718e-03,  2.2343e-02, -3.6120e-02, -3.5076e-02, -2.4739e-02,\n","          -3.9858e-02, -2.4768e-02, -3.2745e-02,  2.5793e-02,  3.2107e-02,\n","           3.7052e-02,  3.3697e-02,  3.5334e-02, -3.5515e-02,  3.5781e-02,\n","           7.1162e-04, -2.6319e-02, -3.2555e-02,  2.5639e-02,  1.4822e-02,\n","           3.2281e-02, -2.4175e-02,  3.2613e-02, -3.5959e-02,  1.6648e-02,\n","           4.1497e-02,  3.1208e-02,  2.0676e-02, -3.4533e-02,  3.4016e-02,\n","           2.7009e-02, -3.6873e-02,  2.7568e-02,  2.7521e-02,  3.5123e-02,\n","          -3.5397e-02,  2.3818e-02, -3.9179e-02, -3.2125e-02,  1.5978e-02,\n","          -4.1205e-02,  2.4966e-02,  3.2400e-02,  3.7363e-02, -3.2628e-02,\n","           3.4790e-02,  2.0253e-02, -3.8254e-02, -3.9561e-02, -1.7729e-02,\n","          -1.7066e-02, -3.5628e-02, -2.4403e-02, -2.0647e-02,  3.5881e-02,\n","           3.0978e-02, -2.9159e-02,  3.6586e-02,  1.9943e-02, -3.5890e-02,\n","          -3.4620e-03,  4.2807e-02,  2.9274e-02,  3.7668e-02,  3.3245e-02,\n","           3.5911e-02, -3.2159e-02,  3.5292e-02, -3.3657e-02,  3.8158e-02,\n","           3.6568e-02, -3.6245e-02, -3.2701e-02,  3.1133e-02,  3.1679e-02,\n","           1.8031e-03, -3.3225e-02,  2.3579e-02,  3.6452e-02, -2.0142e-02,\n","           3.9044e-02,  3.0531e-02, -4.4215e-03, -3.2213e-02,  4.1738e-03,\n","          -3.5711e-02,  3.3179e-02,  3.5484e-02, -3.6056e-02, -3.6100e-02,\n","          -7.0733e-04,  4.2051e-02,  3.9146e-02,  3.3684e-03, -3.1837e-02,\n","          -3.8783e-02,  1.5177e-02, -2.0875e-02, -3.7740e-02, -3.2313e-02,\n","          -2.9770e-02, -3.9598e-02,  3.7501e-02, -2.6086e-02, -1.3215e-03,\n","          -2.8364e-02,  3.5805e-02,  3.3522e-02, -5.1516e-05,  3.4641e-02,\n","           2.6417e-02,  3.3285e-02, -3.0818e-02,  5.9357e-03,  3.5905e-02,\n","           2.9529e-02,  3.0337e-02,  3.6891e-02,  1.0010e-02, -4.6690e-03,\n","          -3.8046e-02,  3.4247e-02, -8.5350e-03,  4.0123e-03, -1.5401e-02,\n","           2.0253e-02,  2.5591e-02, -9.6380e-03, -1.8194e-02,  2.6951e-02,\n","           3.9290e-02,  3.1439e-02,  1.3406e-02, -1.1854e-02,  3.5664e-02,\n","           3.0977e-02, -4.0209e-02,  3.3306e-02, -1.1586e-02,  2.0590e-02,\n","          -3.7262e-02, -3.4649e-02, -2.6418e-02, -3.9492e-02, -2.8216e-02,\n","           3.9115e-02, -3.5162e-02, -2.8465e-02, -3.9597e-02, -4.0258e-02,\n","           2.6732e-02, -1.8421e-02,  3.1777e-02,  3.9844e-02,  2.7883e-02,\n","           3.6869e-02, -3.3543e-02,  2.8990e-02, -3.7596e-02, -3.2847e-02,\n","           7.3056e-03,  3.8603e-02,  1.6289e-02,  4.1616e-02, -3.7963e-02,\n","           3.5771e-02, -2.9240e-02, -2.9992e-02,  2.2219e-02,  1.6239e-02,\n","           3.8207e-02,  3.7262e-02,  3.2604e-02, -3.0482e-02, -4.3556e-02,\n","           3.0915e-02,  3.9548e-02,  3.7112e-02, -2.7515e-02,  3.3514e-02,\n","          -1.7190e-02,  2.4560e-02, -4.3151e-02, -3.9141e-02, -3.6613e-02,\n","           3.1045e-02,  3.4323e-02, -1.7538e-02,  2.8712e-02, -2.8562e-02,\n","          -3.5792e-02,  1.1919e-02,  3.5537e-02, -3.8999e-02, -4.1932e-02,\n","           3.3913e-02, -3.8506e-02, -4.3398e-02,  4.0710e-02,  2.9230e-02,\n","           4.0482e-02, -3.2057e-02,  4.0820e-03, -4.2434e-02,  3.6530e-02,\n","          -3.2461e-02,  1.6723e-02, -2.2221e-02, -2.4315e-02,  3.7426e-02,\n","           3.4833e-02,  3.7997e-02, -3.3759e-02, -4.2390e-02,  3.1277e-02,\n","          -3.1190e-02,  3.9525e-02, -3.7090e-02,  3.1981e-02, -2.9964e-02,\n","           3.5171e-02,  3.5420e-02,  2.2853e-02, -1.8023e-02,  3.8033e-02,\n","           3.7929e-02, -3.2902e-02, -3.8041e-02,  8.8540e-03,  3.0192e-02,\n","          -2.4843e-02,  8.0482e-03, -3.1601e-02,  2.7196e-02, -3.2798e-02,\n","           2.7064e-02, -6.1685e-03, -7.7153e-03,  3.3860e-02, -3.7294e-02,\n","           3.4264e-02, -2.2259e-04, -3.0768e-02, -3.7842e-02,  3.6583e-02,\n","          -2.5563e-02, -3.8405e-02,  1.5281e-02, -3.9154e-02,  3.2061e-02,\n","          -1.5665e-02, -3.0298e-02, -2.6575e-02, -3.5850e-02, -3.6353e-02,\n","           2.4760e-02, -3.8710e-02,  1.3621e-02,  3.6215e-02, -1.5095e-02,\n","           3.1064e-02, -3.2685e-02,  3.5884e-02, -1.9320e-02, -3.7717e-02,\n","          -3.5297e-02,  1.6108e-02, -3.4768e-02,  2.6273e-02,  9.7172e-03,\n","           3.5402e-02, -3.6623e-02,  3.4448e-02,  3.5790e-02, -7.6078e-03,\n","          -3.8128e-02,  3.8418e-02,  3.6123e-02, -1.8083e-03, -3.6030e-02,\n","          -3.6018e-02,  3.8513e-02, -4.0426e-02, -3.5133e-02,  3.1849e-02,\n","           3.1990e-02, -3.2372e-02,  3.5276e-02,  3.4268e-02, -2.8946e-02,\n","          -3.8864e-02, -6.6229e-03,  2.6614e-02,  3.3257e-02,  4.7537e-03,\n","           2.7661e-02,  3.8067e-02, -3.3849e-02,  2.5766e-02,  3.7927e-02,\n","           1.0734e-02,  3.8068e-02,  3.4535e-02, -1.2279e-02, -2.7565e-02,\n","           3.6420e-02,  2.2949e-02, -2.6622e-02,  1.4664e-02, -3.5957e-02,\n","          -1.7397e-02, -3.1491e-02,  3.9217e-02,  2.4810e-04, -1.2467e-02,\n","           9.0583e-04,  2.2183e-02,  3.5977e-02, -3.8864e-02,  4.1579e-02,\n","           3.3104e-02,  3.0406e-02, -2.3236e-02,  8.6063e-03,  4.6379e-04,\n","           2.0170e-02, -1.1366e-02, -1.0505e-02,  3.2480e-02,  3.4374e-02,\n","          -3.4472e-02,  3.6802e-02,  3.8613e-02, -1.2478e-02, -3.1966e-03,\n","          -3.1822e-02,  3.3685e-02]]], device='cuda:0',\n","       grad_fn=<CudnnRnnBackward0>)\n","\n","\n","hidden\n","shape :  torch.Size([1, 1, 512])\n","tensor([[[ 2.2570e-03, -3.8253e-02,  3.7735e-02,  2.5970e-02, -4.4589e-02,\n","          -3.0273e-02,  3.6394e-02, -3.1717e-02,  3.9863e-02, -1.5255e-02,\n","          -3.3338e-02, -1.7419e-02, -3.6518e-02,  3.4689e-02,  2.8800e-02,\n","          -4.0039e-02, -2.2648e-02, -3.6061e-02,  3.0145e-02,  1.6217e-02,\n","           4.0167e-02, -4.4061e-02, -3.1047e-02, -3.9956e-02, -5.9814e-03,\n","          -3.4440e-02,  1.6144e-02, -3.3578e-02, -2.6352e-02,  3.0678e-02,\n","           3.3989e-02,  3.0273e-02,  1.6516e-02,  1.0487e-04, -7.4943e-03,\n","          -6.6586e-04,  1.1805e-02,  3.0889e-02, -3.6117e-02,  3.9597e-02,\n","          -1.6291e-02,  2.7696e-02,  2.6594e-02,  2.5061e-02,  7.6721e-03,\n","          -3.5730e-02, -3.3831e-02,  2.4523e-03, -5.5764e-03,  9.9474e-03,\n","          -2.8743e-02,  3.5365e-02,  3.1851e-02,  3.0036e-02,  3.1861e-02,\n","          -3.9897e-02, -4.6914e-02,  2.7055e-02, -1.9682e-02,  9.5936e-03,\n","          -3.4327e-02, -3.9540e-02, -3.6293e-02, -4.0675e-02,  3.5316e-02,\n","           6.7988e-03, -3.0380e-02, -2.5303e-02,  3.0929e-02, -3.3368e-02,\n","           1.8965e-02, -6.6384e-03, -4.0935e-02,  1.0547e-02,  3.8551e-02,\n","          -2.1062e-02, -1.7702e-02, -2.3691e-02, -3.2288e-02, -3.2095e-02,\n","          -3.0837e-02, -2.9068e-02,  2.7641e-02, -2.1464e-02, -3.8175e-02,\n","          -3.9865e-02, -2.0241e-02, -3.7060e-02,  3.6387e-02, -2.9912e-02,\n","          -7.5894e-03,  3.3749e-02,  2.7372e-02, -2.7908e-02, -2.8593e-02,\n","          -2.0040e-02,  3.2119e-02, -3.9205e-02, -2.2092e-02,  2.6401e-03,\n","           1.8424e-02,  4.3818e-02,  4.4867e-03, -3.9112e-02,  9.3802e-05,\n","           3.4001e-02,  2.9565e-02, -3.3107e-02,  2.6531e-02,  2.9561e-02,\n","          -4.1857e-02,  3.5857e-02,  3.2084e-02, -1.1381e-02, -2.0387e-02,\n","          -2.8603e-02, -2.8873e-02,  2.3737e-02,  3.8286e-02,  3.5980e-02,\n","          -3.4494e-02,  3.3493e-02, -2.2277e-02,  2.1115e-02, -7.6063e-03,\n","          -4.0252e-02, -3.4443e-02,  3.7216e-02,  4.0882e-02,  3.4942e-02,\n","           6.9950e-03, -2.9300e-03, -2.4350e-02,  3.2026e-02, -4.1983e-02,\n","          -2.6793e-02, -3.3058e-02,  3.8543e-02, -3.3286e-02,  3.0363e-02,\n","          -8.0392e-04, -3.2993e-02,  3.3476e-02, -1.8195e-02,  3.3718e-02,\n","           3.2278e-02,  3.0087e-02,  8.2021e-03,  3.6994e-02,  3.8671e-02,\n","          -3.3288e-02,  3.0148e-03, -2.2125e-02, -3.1868e-02, -2.1835e-02,\n","           3.4906e-02, -1.4122e-02,  3.1666e-02,  3.5217e-02, -3.3022e-02,\n","          -3.2342e-02,  3.6243e-02, -2.2592e-02,  3.4220e-02,  3.8984e-02,\n","           2.9320e-02, -3.9222e-02,  2.9824e-02, -3.3082e-02,  3.8918e-02,\n","          -2.7739e-02,  3.6310e-02, -1.0910e-02, -1.1915e-02, -2.9959e-02,\n","          -4.2718e-03,  2.2343e-02, -3.6120e-02, -3.5076e-02, -2.4739e-02,\n","          -3.9858e-02, -2.4768e-02, -3.2745e-02,  2.5793e-02,  3.2107e-02,\n","           3.7052e-02,  3.3697e-02,  3.5334e-02, -3.5515e-02,  3.5781e-02,\n","           7.1162e-04, -2.6319e-02, -3.2555e-02,  2.5639e-02,  1.4822e-02,\n","           3.2281e-02, -2.4175e-02,  3.2613e-02, -3.5959e-02,  1.6648e-02,\n","           4.1497e-02,  3.1208e-02,  2.0676e-02, -3.4533e-02,  3.4016e-02,\n","           2.7009e-02, -3.6873e-02,  2.7568e-02,  2.7521e-02,  3.5123e-02,\n","          -3.5397e-02,  2.3818e-02, -3.9179e-02, -3.2125e-02,  1.5978e-02,\n","          -4.1205e-02,  2.4966e-02,  3.2400e-02,  3.7363e-02, -3.2628e-02,\n","           3.4790e-02,  2.0253e-02, -3.8254e-02, -3.9561e-02, -1.7729e-02,\n","          -1.7066e-02, -3.5628e-02, -2.4403e-02, -2.0647e-02,  3.5881e-02,\n","           3.0978e-02, -2.9159e-02,  3.6586e-02,  1.9943e-02, -3.5890e-02,\n","          -3.4620e-03,  4.2807e-02,  2.9274e-02,  3.7668e-02,  3.3245e-02,\n","           3.5911e-02, -3.2159e-02,  3.5292e-02, -3.3657e-02,  3.8158e-02,\n","           3.6568e-02, -3.6245e-02, -3.2701e-02,  3.1133e-02,  3.1679e-02,\n","           1.8031e-03, -3.3225e-02,  2.3579e-02,  3.6452e-02, -2.0142e-02,\n","           3.9044e-02,  3.0531e-02, -4.4215e-03, -3.2213e-02,  4.1738e-03,\n","          -3.5711e-02,  3.3179e-02,  3.5484e-02, -3.6056e-02, -3.6100e-02,\n","          -7.0733e-04,  4.2051e-02,  3.9146e-02,  3.3684e-03, -3.1837e-02,\n","          -3.8783e-02,  1.5177e-02, -2.0875e-02, -3.7740e-02, -3.2313e-02,\n","          -2.9770e-02, -3.9598e-02,  3.7501e-02, -2.6086e-02, -1.3215e-03,\n","          -2.8364e-02,  3.5805e-02,  3.3522e-02, -5.1516e-05,  3.4641e-02,\n","           2.6417e-02,  3.3285e-02, -3.0818e-02,  5.9357e-03,  3.5905e-02,\n","           2.9529e-02,  3.0337e-02,  3.6891e-02,  1.0010e-02, -4.6690e-03,\n","          -3.8046e-02,  3.4247e-02, -8.5350e-03,  4.0123e-03, -1.5401e-02,\n","           2.0253e-02,  2.5591e-02, -9.6380e-03, -1.8194e-02,  2.6951e-02,\n","           3.9290e-02,  3.1439e-02,  1.3406e-02, -1.1854e-02,  3.5664e-02,\n","           3.0977e-02, -4.0209e-02,  3.3306e-02, -1.1586e-02,  2.0590e-02,\n","          -3.7262e-02, -3.4649e-02, -2.6418e-02, -3.9492e-02, -2.8216e-02,\n","           3.9115e-02, -3.5162e-02, -2.8465e-02, -3.9597e-02, -4.0258e-02,\n","           2.6732e-02, -1.8421e-02,  3.1777e-02,  3.9844e-02,  2.7883e-02,\n","           3.6869e-02, -3.3543e-02,  2.8990e-02, -3.7596e-02, -3.2847e-02,\n","           7.3056e-03,  3.8603e-02,  1.6289e-02,  4.1616e-02, -3.7963e-02,\n","           3.5771e-02, -2.9240e-02, -2.9992e-02,  2.2219e-02,  1.6239e-02,\n","           3.8207e-02,  3.7262e-02,  3.2604e-02, -3.0482e-02, -4.3556e-02,\n","           3.0915e-02,  3.9548e-02,  3.7112e-02, -2.7515e-02,  3.3514e-02,\n","          -1.7190e-02,  2.4560e-02, -4.3151e-02, -3.9141e-02, -3.6613e-02,\n","           3.1045e-02,  3.4323e-02, -1.7538e-02,  2.8712e-02, -2.8562e-02,\n","          -3.5792e-02,  1.1919e-02,  3.5537e-02, -3.8999e-02, -4.1932e-02,\n","           3.3913e-02, -3.8506e-02, -4.3398e-02,  4.0710e-02,  2.9230e-02,\n","           4.0482e-02, -3.2057e-02,  4.0820e-03, -4.2434e-02,  3.6530e-02,\n","          -3.2461e-02,  1.6723e-02, -2.2221e-02, -2.4315e-02,  3.7426e-02,\n","           3.4833e-02,  3.7997e-02, -3.3759e-02, -4.2390e-02,  3.1277e-02,\n","          -3.1190e-02,  3.9525e-02, -3.7090e-02,  3.1981e-02, -2.9964e-02,\n","           3.5171e-02,  3.5420e-02,  2.2853e-02, -1.8023e-02,  3.8033e-02,\n","           3.7929e-02, -3.2902e-02, -3.8041e-02,  8.8540e-03,  3.0192e-02,\n","          -2.4843e-02,  8.0482e-03, -3.1601e-02,  2.7196e-02, -3.2798e-02,\n","           2.7064e-02, -6.1685e-03, -7.7153e-03,  3.3860e-02, -3.7294e-02,\n","           3.4264e-02, -2.2259e-04, -3.0768e-02, -3.7842e-02,  3.6583e-02,\n","          -2.5563e-02, -3.8405e-02,  1.5281e-02, -3.9154e-02,  3.2061e-02,\n","          -1.5665e-02, -3.0298e-02, -2.6575e-02, -3.5850e-02, -3.6353e-02,\n","           2.4760e-02, -3.8710e-02,  1.3621e-02,  3.6215e-02, -1.5095e-02,\n","           3.1064e-02, -3.2685e-02,  3.5884e-02, -1.9320e-02, -3.7717e-02,\n","          -3.5297e-02,  1.6108e-02, -3.4768e-02,  2.6273e-02,  9.7172e-03,\n","           3.5402e-02, -3.6623e-02,  3.4448e-02,  3.5790e-02, -7.6078e-03,\n","          -3.8128e-02,  3.8418e-02,  3.6123e-02, -1.8083e-03, -3.6030e-02,\n","          -3.6018e-02,  3.8513e-02, -4.0426e-02, -3.5133e-02,  3.1849e-02,\n","           3.1990e-02, -3.2372e-02,  3.5276e-02,  3.4268e-02, -2.8946e-02,\n","          -3.8864e-02, -6.6229e-03,  2.6614e-02,  3.3257e-02,  4.7537e-03,\n","           2.7661e-02,  3.8067e-02, -3.3849e-02,  2.5766e-02,  3.7927e-02,\n","           1.0734e-02,  3.8068e-02,  3.4535e-02, -1.2279e-02, -2.7565e-02,\n","           3.6420e-02,  2.2949e-02, -2.6622e-02,  1.4664e-02, -3.5957e-02,\n","          -1.7397e-02, -3.1491e-02,  3.9217e-02,  2.4810e-04, -1.2467e-02,\n","           9.0583e-04,  2.2183e-02,  3.5977e-02, -3.8864e-02,  4.1579e-02,\n","           3.3104e-02,  3.0406e-02, -2.3236e-02,  8.6063e-03,  4.6379e-04,\n","           2.0170e-02, -1.1366e-02, -1.0505e-02,  3.2480e-02,  3.4374e-02,\n","          -3.4472e-02,  3.6802e-02,  3.8613e-02, -1.2478e-02, -3.1966e-03,\n","          -3.1822e-02,  3.3685e-02]]], device='cuda:0',\n","       grad_fn=<CudnnRnnBackward0>)"],"metadata":{"id":"ENmyYinSyQJF"}},{"cell_type":"markdown","source":["### prediction 의 크기와 값\n","prediction 은 FC layer에 임베딩된 input word와 weigthed(context vector c_i), 이전 시점의 디코더의 hidden state를 모두 concat한 결과를 전달하여 얻은 예측 값이다.  \n","\n","prediction\n","shape :  torch.Size([1, 5893])\n","tensor([[ 0.0723, -0.0628, -0.0722,  ..., -0.0710, -0.0662, -0.0729]],\n","       device='cuda:0', grad_fn=<AddmmBackward0>)"],"metadata":{"id":"W8mEws3bymJp"}},{"cell_type":"code","execution_count":43,"metadata":{"id":"bhXnz0ZjMQ9r","executionInfo":{"status":"ok","timestamp":1665545185155,"user_tz":-540,"elapsed":268,"user":{"displayName":"ᄀᄒᄌ","userId":"07458837797284130526"}}},"outputs":[],"source":["class Seq2Seq(nn.Module):\n","    def __init__(self, encoder, decoder, device):\n","        super().__init__()\n","        \n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.device = device\n","        \n","    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n","        \n","        #src = [src len, batch size]\n","        #trg = [trg len, batch size]\n","        #teacher_forcing_ratio is probability to use teacher forcing\n","        #e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time\n","        \n","        batch_size = src.shape[1]\n","        trg_len = trg.shape[0]\n","        trg_vocab_size = self.decoder.output_dim\n","        \n","        #tensor to store decoder outputs\n","        '''\n","        outputs은 decoder를 수행한 결과를 담을 tensor에 해당한다. 처음에 이를 초기화한 후 번역의 결과를 담도록 한다. \n","        '''\n","        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n","        \n","        #encoder_outputs is all hidden states of the input sequence, back and forwards\n","        #hidden is the final forward and backward hidden states, passed through a linear layer\n","        \n","        '''\n","        이후, 얻은 src는 Encoder에 넣어준다. 인코더로 얻어지는 결과는 encoder_outputs과 hidden으로 encoder_outputs은 input\n","        sequence의 back/forward 모든 hidden state이고, hidden은 마지막 hidden state로 linear layer에 쓰인다.  \n","        '''\n","        encoder_outputs, hidden = self.encoder(src)\n","                \n","        #first input to the decoder is the <sos> tokens\n","        '''\n","        Encoder가 끝나면, 이 결과에 대해 decoding을 할 차례이다. 우선 문장의 시작을 알리는 토큰이 필요하다. trg의 0번째 idx는 \n","        <sos>토큰이므로 이를 이용할 것이다. 그러면 input은 [batch]의 vector가 된다.  \n","        '''\n","        input = trg[0,:]\n","        '''\n","        encoder의 output인 hidden과 encoder_outputs 그리고 토큰인 input을 디코더에 넣는다. \n","        '''\n","        for t in range(1, trg_len):\n","            \n","            #insert input token embedding, previous hidden state and all encoder hidden states\n","            #receive output tensor (predictions) and new hidden state\n","            output, hidden = self.decoder(input, hidden, encoder_outputs)\n","            \n","            #place predictions in a tensor holding predictions for each token\n","            outputs[t] = output\n","            \n","            #decide if we are going to use teacher forcing or not\n","            '''\n","            decoder가 예측한 다음 단어는 top1이 되고(softmax를 한 결과와 max를 한 결과가 같음), teacher forcing을 사용하겠다면\n","            trg[t]가 다음 output이 되어 decoder의 입력으로 들어가고, 그게 아니라면 top1을 넣어 teacher forcing을 사용하지 않을 것이다. \n","            '''\n","            teacher_force = random.random() < teacher_forcing_ratio\n","            \n","            #get the highest predicted token from our predictions\n","            top1 = output.argmax(1) \n","            \n","            #if teacher forcing, use actual next token as next input\n","            #if not, use predicted token\n","            input = trg[t] if teacher_force else top1\n","\n","        return outputs"]},{"cell_type":"markdown","metadata":{"id":"Y38uLHLRMQ9s"},"source":["## Training the Seq2Seq Model\n","\n","The rest of this tutorial is very similar to the previous one.\n","\n","We initialise our parameters, encoder, decoder and seq2seq model (placing it on the GPU if we have one). "]},{"cell_type":"code","execution_count":44,"metadata":{"id":"nW1I6x3qMQ9s","executionInfo":{"status":"ok","timestamp":1665545188459,"user_tz":-540,"elapsed":396,"user":{"displayName":"ᄀᄒᄌ","userId":"07458837797284130526"}}},"outputs":[],"source":["INPUT_DIM = len(SRC.vocab)\n","OUTPUT_DIM = len(TRG.vocab)\n","ENC_EMB_DIM = 256\n","DEC_EMB_DIM = 256\n","ENC_HID_DIM = 512\n","DEC_HID_DIM = 512\n","ENC_DROPOUT = 0.5\n","DEC_DROPOUT = 0.5\n","\n","attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n","enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n","dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n","\n","model = Seq2Seq(enc, dec, device).to(device)"]},{"cell_type":"markdown","metadata":{"id":"UI90jBCiMQ9s"},"source":["We use a simplified version of the weight initialization scheme used in the paper. Here, we will initialize all biases to zero and all weights from $\\mathcal{N}(0, 0.01)$."]},{"cell_type":"code","execution_count":45,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rbc85gMCMQ9s","outputId":"fa0e489a-75cc-445d-a209-b7e02efaac28","executionInfo":{"status":"ok","timestamp":1665545190199,"user_tz":-540,"elapsed":279,"user":{"displayName":"ᄀᄒᄌ","userId":"07458837797284130526"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Seq2Seq(\n","  (encoder): Encoder(\n","    (embedding): Embedding(7853, 256)\n","    (rnn): GRU(256, 512, bidirectional=True)\n","    (fc): Linear(in_features=1024, out_features=512, bias=True)\n","    (dropout): Dropout(p=0.5, inplace=False)\n","  )\n","  (decoder): Decoder(\n","    (attention): Attention(\n","      (attn): Linear(in_features=1536, out_features=512, bias=True)\n","      (v): Linear(in_features=512, out_features=1, bias=False)\n","    )\n","    (embedding): Embedding(5893, 256)\n","    (rnn): GRU(1280, 512)\n","    (fc_out): Linear(in_features=1792, out_features=5893, bias=True)\n","    (dropout): Dropout(p=0.5, inplace=False)\n","  )\n",")"]},"metadata":{},"execution_count":45}],"source":["def init_weights(m):\n","    for name, param in m.named_parameters():\n","        if 'weight' in name:\n","            nn.init.normal_(param.data, mean=0, std=0.01)\n","        else:\n","            nn.init.constant_(param.data, 0)\n","            \n","model.apply(init_weights)"]},{"cell_type":"markdown","metadata":{"id":"CrKgSCU6MQ9s"},"source":["Calculate the number of parameters. We get an increase of almost 50% in the amount of parameters from the last model. "]},{"cell_type":"code","execution_count":46,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tBg5hg5lMQ9s","outputId":"6696da17-3626-45e7-c802-54072e4d2ec2","executionInfo":{"status":"ok","timestamp":1665545192951,"user_tz":-540,"elapsed":450,"user":{"displayName":"ᄀᄒᄌ","userId":"07458837797284130526"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["The model has 20,518,405 trainable parameters\n"]}],"source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(f'The model has {count_parameters(model):,} trainable parameters')"]},{"cell_type":"markdown","metadata":{"id":"yxRvLoRwMQ9s"},"source":["We create an optimizer."]},{"cell_type":"code","execution_count":47,"metadata":{"id":"P5Ldn5YPMQ9s","executionInfo":{"status":"ok","timestamp":1665545194336,"user_tz":-540,"elapsed":1,"user":{"displayName":"ᄀᄒᄌ","userId":"07458837797284130526"}}},"outputs":[],"source":["optimizer = optim.Adam(model.parameters())"]},{"cell_type":"markdown","metadata":{"id":"_j1broyYMQ9s"},"source":["어떤 문자가 나올지 예측하는 문제이므로 loss function 으로 cross entropy를 사용한다. cross entrophy는 일밙거으로 분류 태스크에서 사용되는 손실함수이다. 값이 낮을수록 예측한 확률이 실제 데이터의 확률과 비슷하다는 것을 의미한다. \n","두 개의 확률 분포가 주어졌을 때, 둘이 얼마나 비슷한지를 나타내는 수치라고 할 수 있다. 낮은 확률로 예측해서 맞추거나, 높은 확률로 예측해서 틀리는 경우 loss가 더 크다. \n","cross entropy 의 손실함수는 아래의 식과 같다. $$CE = - \\sum_{i}{t_{i}log(y_{i})}$$ $t_{i}$는 i번째 클래스의 실제 확률을, $y_{i}$는 i 번째 클래스의 모델 예측 확률이다. 예측해야하는 토큰의 실제 값이(y)이 주어진 토큰 중 무엇인지를, 즉 각 토큰의 확률을 예측한다. "]},{"cell_type":"code","execution_count":48,"metadata":{"id":"9SDpJU6JMQ9s","executionInfo":{"status":"ok","timestamp":1665545195654,"user_tz":-540,"elapsed":1,"user":{"displayName":"ᄀᄒᄌ","userId":"07458837797284130526"}}},"outputs":[],"source":["TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n","\n","criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"]},{"cell_type":"markdown","metadata":{"id":"SI61d-plMQ9t"},"source":["We then create the training loop..."]},{"cell_type":"code","execution_count":49,"metadata":{"id":"DGqaepFfMQ9t","executionInfo":{"status":"ok","timestamp":1665545197249,"user_tz":-540,"elapsed":1,"user":{"displayName":"ᄀᄒᄌ","userId":"07458837797284130526"}}},"outputs":[],"source":["def train(model, iterator, optimizer, criterion, clip):\n","    \n","    model.train()\n","    \n","    epoch_loss = 0\n","    \n","    for i, batch in enumerate(iterator):\n","        #if i == 1:\n","          #break\n","        src = batch.src\n","        trg = batch.trg\n","        \n","        optimizer.zero_grad()\n","        \n","        output = model(src, trg)\n","        \n","        #trg = [trg len, batch size]\n","        #output = [trg len, batch size, output dim]\n","        \n","        output_dim = output.shape[-1]\n","        \n","        output = output[1:].view(-1, output_dim)\n","        trg = trg[1:].view(-1)\n","        \n","        #trg = [(trg len - 1) * batch size]\n","        #output = [(trg len - 1) * batch size, output dim]\n","        \n","        loss = criterion(output, trg)\n","        \n","        loss.backward()\n","        \n","        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n","        \n","        optimizer.step()\n","        \n","        epoch_loss += loss.item()\n","        \n","    return epoch_loss / len(iterator)"]},{"cell_type":"markdown","metadata":{"id":"Lr9tFnp6MQ9t"},"source":["...and the evaluation loop, remembering to set the model to `eval` mode and turn off teaching forcing."]},{"cell_type":"code","execution_count":50,"metadata":{"id":"uLQWAUn9MQ9t","executionInfo":{"status":"ok","timestamp":1665545198623,"user_tz":-540,"elapsed":1,"user":{"displayName":"ᄀᄒᄌ","userId":"07458837797284130526"}}},"outputs":[],"source":["def evaluate(model, iterator, criterion):\n","    \n","    model.eval()\n","    \n","    epoch_loss = 0\n","    \n","    with torch.no_grad():\n","    \n","        for i, batch in enumerate(iterator):\n","\n","            src = batch.src\n","            trg = batch.trg\n","\n","            output = model(src, trg, 0) #turn off teacher forcing\n","\n","            #trg = [trg len, batch size]\n","            #output = [trg len, batch size, output dim]\n","\n","            output_dim = output.shape[-1]\n","            \n","            output = output[1:].view(-1, output_dim)\n","            trg = trg[1:].view(-1)\n","\n","            #trg = [(trg len - 1) * batch size]\n","            #output = [(trg len - 1) * batch size, output dim]\n","\n","            loss = criterion(output, trg)\n","\n","            epoch_loss += loss.item()\n","        \n","    return epoch_loss / len(iterator)"]},{"cell_type":"markdown","metadata":{"id":"TR19UN_uMQ9t"},"source":["Finally, define a timing function."]},{"cell_type":"code","execution_count":51,"metadata":{"id":"3Ji2L_tpMQ9t","executionInfo":{"status":"ok","timestamp":1665545200501,"user_tz":-540,"elapsed":2,"user":{"displayName":"ᄀᄒᄌ","userId":"07458837797284130526"}}},"outputs":[],"source":["def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs"]},{"cell_type":"markdown","metadata":{"id":"ZsMChRPQMQ9t"},"source":["Then, we train our model, saving the parameters that give us the best validation loss."]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"aQigY_RQMQ9t","outputId":"7fd5194e-1e00-416a-cf18-68aa8f30142b","executionInfo":{"status":"error","timestamp":1665543344975,"user_tz":-540,"elapsed":1255,"user":{"displayName":"ᄀᄒᄌ","userId":"07458837797284130526"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["src\n","shape :  torch.Size([17, 1])\n","tensor([[   2],\n","        [   5],\n","        [  66],\n","        [  25],\n","        [ 443],\n","        [  23],\n","        [  22],\n","        [  14],\n","        [5620],\n","        [2165],\n","        [2408],\n","        [  10],\n","        [1910],\n","        [  23],\n","        [ 147],\n","        [   4],\n","        [   3]], device='cuda:0')\n","embedded before dropout\n","shape :  torch.Size([17, 1, 256])\n","tensor([[[ 0.0200,  0.0012,  0.0028,  ..., -0.0058, -0.0041, -0.0096]],\n","\n","        [[-0.0053,  0.0009, -0.0111,  ...,  0.0094, -0.0088,  0.0010]],\n","\n","        [[ 0.0132,  0.0101,  0.0139,  ...,  0.0018,  0.0006,  0.0002]],\n","\n","        ...,\n","\n","        [[-0.0118,  0.0077, -0.0036,  ...,  0.0030,  0.0109, -0.0012]],\n","\n","        [[-0.0139,  0.0171,  0.0039,  ...,  0.0028,  0.0143, -0.0139]],\n","\n","        [[ 0.0131,  0.0032, -0.0025,  ...,  0.0310,  0.0055, -0.0038]]],\n","       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n","embedded after dropout\n","shape :  torch.Size([17, 1, 256])\n","tensor([[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.0083,  0.0000]],\n","\n","        [[-0.0106,  0.0000, -0.0222,  ...,  0.0000,  0.0000,  0.0000]],\n","\n","        [[ 0.0000,  0.0000,  0.0000,  ...,  0.0036,  0.0012,  0.0004]],\n","\n","        ...,\n","\n","        [[-0.0237,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.0023]],\n","\n","        [[-0.0278,  0.0000,  0.0000,  ...,  0.0000,  0.0286,  0.0000]],\n","\n","        [[ 0.0000,  0.0065,  0.0000,  ...,  0.0620,  0.0000,  0.0000]]],\n","       device='cuda:0', grad_fn=<FusedDropoutBackward0>)\n","outputs\n","shape :  torch.Size([17, 1, 1024])\n","tensor([[[ 1.5251e-03,  5.0332e-03, -1.8111e-03,  ..., -1.5339e-02,\n","          -1.0614e-02,  1.0925e-02]],\n","\n","        [[ 1.1445e-05,  5.9481e-03, -5.6105e-03,  ..., -1.7181e-02,\n","          -1.1947e-02,  1.0878e-02]],\n","\n","        [[-7.5090e-04,  7.2755e-03, -8.2482e-03,  ..., -1.6020e-02,\n","          -1.0675e-02,  1.2355e-02]],\n","\n","        ...,\n","\n","        [[ 2.3823e-03,  1.2973e-02, -9.9649e-03,  ..., -8.8709e-03,\n","          -3.8923e-03,  9.4221e-03]],\n","\n","        [[ 2.5274e-03,  1.4425e-02, -1.1705e-02,  ..., -6.3470e-03,\n","          -4.3193e-03,  5.7830e-03]],\n","\n","        [[ 2.0797e-04,  1.7569e-02, -1.3075e-02,  ..., -3.5139e-03,\n","          -2.6035e-03,  2.8823e-03]]], device='cuda:0',\n","       grad_fn=<CudnnRnnBackward0>)\n","hidden\n","shape :  torch.Size([2, 1, 512])\n","tensor([[[ 0.0002,  0.0176, -0.0131,  ...,  0.0091,  0.0095, -0.0116]],\n","\n","        [[ 0.0143,  0.0079, -0.0079,  ..., -0.0153, -0.0106,  0.0109]]],\n","       device='cuda:0', grad_fn=<CudnnRnnBackward0>)\n","hidden after FC layer\n","shape :  torch.Size([1, 512])\n","tensor([[-0.0279, -0.0243,  0.0289,  0.0258, -0.0362, -0.0199,  0.0279, -0.0300,\n","          0.0327, -0.0263, -0.0210, -0.0231, -0.0308,  0.0264,  0.0207, -0.0283,\n","         -0.0218, -0.0318,  0.0275,  0.0303,  0.0317, -0.0356, -0.0254, -0.0326,\n","          0.0164, -0.0264,  0.0086, -0.0279, -0.0191,  0.0213,  0.0230,  0.0251,\n","          0.0056,  0.0212,  0.0211,  0.0207, -0.0103,  0.0232, -0.0265,  0.0217,\n","          0.0086,  0.0206,  0.0171,  0.0209,  0.0247, -0.0303, -0.0214, -0.0238,\n","          0.0121,  0.0313, -0.0206,  0.0292,  0.0263,  0.0258,  0.0271, -0.0332,\n","         -0.0349,  0.0161, -0.0306,  0.0271, -0.0227, -0.0281, -0.0240, -0.0305,\n","          0.0237,  0.0349, -0.0270, -0.0297,  0.0195, -0.0269, -0.0032,  0.0119,\n","         -0.0316, -0.0178,  0.0294, -0.0210, -0.0325, -0.0156, -0.0282, -0.0283,\n","         -0.0273, -0.0294,  0.0147, -0.0246, -0.0285, -0.0331, -0.0225, -0.0281,\n","          0.0260, -0.0256, -0.0283,  0.0295,  0.0342, -0.0242, -0.0189, -0.0045,\n","          0.0306, -0.0275, -0.0056,  0.0152,  0.0008,  0.0300,  0.0325, -0.0247,\n","         -0.0276,  0.0266,  0.0270, -0.0242,  0.0202,  0.0241, -0.0376,  0.0265,\n","          0.0263,  0.0129, -0.0310, -0.0312, -0.0283,  0.0287,  0.0281,  0.0265,\n","         -0.0270,  0.0216, -0.0309,  0.0101, -0.0285, -0.0300, -0.0303,  0.0295,\n","          0.0332,  0.0239,  0.0306,  0.0208, -0.0084,  0.0263, -0.0314, -0.0155,\n","         -0.0190,  0.0319, -0.0291,  0.0205,  0.0170, -0.0215,  0.0312, -0.0206,\n","          0.0263,  0.0239,  0.0231,  0.0291,  0.0285,  0.0294, -0.0241,  0.0245,\n","         -0.0049, -0.0221, -0.0214,  0.0307,  0.0072,  0.0292,  0.0328, -0.0281,\n","         -0.0260,  0.0267, -0.0277,  0.0236,  0.0282,  0.0156, -0.0302,  0.0205,\n","         -0.0260,  0.0346, -0.0279,  0.0283, -0.0075, -0.0290, -0.0245,  0.0093,\n","          0.0067, -0.0248, -0.0239, -0.0130, -0.0324, -0.0274, -0.0259,  0.0334,\n","          0.0220,  0.0259,  0.0224,  0.0300, -0.0248,  0.0279, -0.0258, -0.0271,\n","         -0.0255,  0.0127, -0.0139,  0.0222, -0.0057,  0.0214, -0.0293,  0.0005,\n","          0.0280,  0.0241,  0.0254, -0.0275,  0.0243,  0.0126, -0.0267,  0.0153,\n","          0.0330,  0.0295, -0.0229,  0.0103, -0.0276, -0.0187,  0.0241, -0.0316,\n","          0.0136,  0.0229,  0.0265, -0.0249,  0.0275,  0.0334, -0.0295, -0.0275,\n","          0.0067,  0.0026, -0.0278, -0.0089, -0.0244,  0.0248,  0.0218, -0.0302,\n","          0.0320,  0.0003, -0.0243, -0.0003,  0.0334,  0.0278,  0.0294,  0.0270,\n","          0.0266, -0.0230,  0.0266, -0.0290,  0.0274,  0.0315, -0.0269, -0.0217,\n","          0.0277,  0.0284, -0.0206, -0.0262,  0.0270,  0.0239, -0.0104,  0.0283,\n","          0.0269,  0.0116, -0.0200, -0.0199, -0.0314,  0.0332,  0.0291, -0.0259,\n","         -0.0305, -0.0153,  0.0315,  0.0297, -0.0135, -0.0253, -0.0287,  0.0207,\n","         -0.0288, -0.0298, -0.0239, -0.0167, -0.0317,  0.0254, -0.0243,  0.0023,\n","         -0.0202,  0.0282,  0.0268,  0.0225,  0.0234,  0.0201,  0.0277, -0.0237,\n","         -0.0130,  0.0266,  0.0185,  0.0257,  0.0258, -0.0171, -0.0342, -0.0289,\n","          0.0271, -0.0248, -0.0043, -0.0043,  0.0099,  0.0211,  0.0105,  0.0022,\n","          0.0152,  0.0294,  0.0269, -0.0048,  0.0006,  0.0272,  0.0262, -0.0300,\n","          0.0252,  0.0095, -0.0016, -0.0271, -0.0225, -0.0251, -0.0342, -0.0149,\n","          0.0279, -0.0255, -0.0198, -0.0303, -0.0331,  0.0242, -0.0298,  0.0150,\n","          0.0357,  0.0163,  0.0275, -0.0313,  0.0192, -0.0329, -0.0295, -0.0184,\n","          0.0309,  0.0028,  0.0371, -0.0361,  0.0300, -0.0220, -0.0247,  0.0257,\n","          0.0259,  0.0243,  0.0323,  0.0248, -0.0248, -0.0340,  0.0166,  0.0319,\n","          0.0300, -0.0323,  0.0170, -0.0181,  0.0105, -0.0331, -0.0354, -0.0277,\n","          0.0166,  0.0233, -0.0254,  0.0293, -0.0171, -0.0269, -0.0121,  0.0304,\n","         -0.0286, -0.0328,  0.0248, -0.0264, -0.0288,  0.0294,  0.0176,  0.0339,\n","         -0.0205, -0.0105, -0.0361,  0.0268, -0.0222,  0.0182, -0.0324, -0.0217,\n","          0.0281,  0.0264,  0.0264, -0.0253, -0.0373,  0.0194, -0.0306,  0.0360,\n","         -0.0304,  0.0309, -0.0239,  0.0256,  0.0243,  0.0068, -0.0035,  0.0274,\n","          0.0269, -0.0272, -0.0262, -0.0150,  0.0258, -0.0050, -0.0191, -0.0225,\n","          0.0181, -0.0198,  0.0288, -0.0116, -0.0335,  0.0259, -0.0273,  0.0234,\n","          0.0240, -0.0239, -0.0260,  0.0259, -0.0311, -0.0327,  0.0270, -0.0220,\n","          0.0204, -0.0206, -0.0250, -0.0228, -0.0255, -0.0288,  0.0265, -0.0264,\n","          0.0030,  0.0269,  0.0106,  0.0255, -0.0270,  0.0279, -0.0011, -0.0328,\n","         -0.0280, -0.0072, -0.0214,  0.0141, -0.0160,  0.0226, -0.0271,  0.0246,\n","          0.0218,  0.0142, -0.0270,  0.0303,  0.0303, -0.0086, -0.0294, -0.0293,\n","          0.0298, -0.0300, -0.0265,  0.0232,  0.0233, -0.0251,  0.0233,  0.0291,\n","         -0.0145, -0.0293, -0.0298,  0.0187,  0.0198, -0.0266,  0.0069,  0.0273,\n","         -0.0243,  0.0082,  0.0263,  0.0286,  0.0294,  0.0330, -0.0046, -0.0192,\n","          0.0339,  0.0212, -0.0147,  0.0298, -0.0309, -0.0088, -0.0306,  0.0280,\n","          0.0282,  0.0125,  0.0266,  0.0304,  0.0218, -0.0312,  0.0342,  0.0292,\n","          0.0247, -0.0286, -0.0151, -0.0172,  0.0221, -0.0280,  0.0140,  0.0203,\n","          0.0265, -0.0290,  0.0231,  0.0286,  0.0112,  0.0188, -0.0270,  0.0308]],\n","       device='cuda:0', grad_fn=<TanhBackward0>)\n","input\n","shape :  torch.Size([1, 1])\n","tensor([[2]], device='cuda:0')\n","embedded\n","shape :  torch.Size([1, 1, 256])\n","tensor([[[ 0.0260,  0.0000,  0.0000, -0.0175,  0.0206,  0.0000,  0.0083,\n","          -0.0282,  0.0161, -0.0141,  0.0225,  0.0000,  0.0412,  0.0000,\n","           0.0281,  0.0000,  0.0000,  0.0000,  0.0000, -0.0158, -0.0174,\n","           0.0000, -0.0134,  0.0000,  0.0396, -0.0686,  0.0000,  0.0000,\n","          -0.0255, -0.0293,  0.0079,  0.0000,  0.0000,  0.0000,  0.0320,\n","           0.0000,  0.0000,  0.0000,  0.0236,  0.0000, -0.0123,  0.0000,\n","          -0.0098,  0.0281,  0.0000,  0.0000,  0.0215, -0.0467, -0.0377,\n","           0.0000,  0.0068, -0.0030,  0.0242,  0.0000, -0.0147,  0.0000,\n","           0.0000,  0.0000,  0.0024,  0.0254,  0.0141,  0.0000,  0.0000,\n","          -0.0018,  0.0000,  0.0000,  0.0000, -0.0242,  0.0089, -0.0416,\n","           0.0000, -0.0241, -0.0008, -0.0118, -0.0347,  0.0026,  0.0139,\n","           0.0003, -0.0177,  0.0000,  0.0000,  0.0113,  0.0101,  0.0147,\n","           0.0000,  0.0316, -0.0031,  0.0094,  0.0000,  0.0000,  0.0121,\n","           0.0011,  0.0000,  0.0000, -0.0221, -0.0193,  0.0000, -0.0218,\n","           0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0006,  0.0000,\n","           0.0000, -0.0214,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","           0.0000, -0.0163, -0.0193, -0.0035,  0.0314, -0.0147,  0.0000,\n","           0.0272,  0.0041,  0.0052,  0.0000, -0.0301,  0.0116,  0.0000,\n","           0.0000,  0.0046, -0.0075,  0.0000,  0.0000,  0.0194,  0.0000,\n","           0.0000,  0.0000,  0.0000, -0.0111,  0.0000,  0.0069,  0.0000,\n","           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0195, -0.0177,\n","           0.0000,  0.0198,  0.0000,  0.0000,  0.0027,  0.0000,  0.0231,\n","           0.0035,  0.0251,  0.0000,  0.0000,  0.0000, -0.0242,  0.0000,\n","           0.0000, -0.0223,  0.0000,  0.0000,  0.0060,  0.0000,  0.0000,\n","           0.0232,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","           0.0000,  0.0000,  0.0000,  0.0092,  0.0098, -0.0006,  0.0000,\n","          -0.0047,  0.0000,  0.0000,  0.0000,  0.0000,  0.0032,  0.0000,\n","           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","           0.0297,  0.0000,  0.0000,  0.0002,  0.0000,  0.0000,  0.0199,\n","          -0.0316,  0.0351,  0.0000, -0.0309,  0.0000,  0.0103,  0.0000,\n","          -0.0032,  0.0000,  0.0000, -0.0085,  0.0000, -0.0242, -0.0093,\n","          -0.0090,  0.0274,  0.0000, -0.0310, -0.0304,  0.0000,  0.0000,\n","           0.0000, -0.0088,  0.0000,  0.0000,  0.0000,  0.0143,  0.0000,\n","          -0.0006,  0.0126,  0.0000,  0.0000, -0.0089, -0.0320,  0.0003,\n","           0.0000,  0.0252, -0.0488,  0.0000,  0.0000,  0.0000, -0.0119,\n","           0.0079,  0.0000,  0.0000,  0.0000,  0.0000, -0.0519, -0.0005,\n","           0.0055,  0.0076, -0.0294, -0.0024]]], device='cuda:0',\n","       grad_fn=<FusedDropoutBackward0>)\n","hidden :  torch.Size([1, 512])\n","hidden_unsqueeze:  torch.Size([1, 17, 512])\n","encoder_outputs :  torch.Size([17, 1, 1024])\n","encoder_outputs_permute :  torch.Size([1, 17, 1024])\n","energy\n","shape :  torch.Size([1, 17, 512])\n","tensor([[[-0.0187,  0.0119, -0.0107,  ...,  0.0186,  0.0094,  0.0255],\n","         [-0.0198,  0.0156, -0.0126,  ...,  0.0214,  0.0108,  0.0276],\n","         [-0.0194,  0.0160, -0.0140,  ...,  0.0228,  0.0133,  0.0289],\n","         ...,\n","         [-0.0188,  0.0147, -0.0101,  ...,  0.0182,  0.0126,  0.0260],\n","         [-0.0184,  0.0149, -0.0110,  ...,  0.0179,  0.0133,  0.0256],\n","         [-0.0172,  0.0125, -0.0091,  ...,  0.0150,  0.0125,  0.0232]]],\n","       device='cuda:0', grad_fn=<TanhBackward0>)\n","attention\n","shape:  torch.Size([1, 17])\n","tensor([[0.0579, 0.0585, 0.0589, 0.0592, 0.0592, 0.0594, 0.0594, 0.0595, 0.0591,\n","         0.0589, 0.0586, 0.0590, 0.0587, 0.0587, 0.0583, 0.0585, 0.0581]],\n","       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n","a\n","shape :  torch.Size([1, 17])\n","tensor([[0.0579, 0.0585, 0.0589, 0.0592, 0.0592, 0.0594, 0.0594, 0.0595, 0.0591,\n","         0.0589, 0.0586, 0.0590, 0.0587, 0.0587, 0.0583, 0.0585, 0.0581]],\n","       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n","a_unsqueeze\n","shape :  torch.Size([1, 1, 17])\n","tensor([[[0.0579, 0.0585, 0.0589, 0.0592, 0.0592, 0.0594, 0.0594, 0.0595,\n","          0.0591, 0.0589, 0.0586, 0.0590, 0.0587, 0.0587, 0.0583, 0.0585,\n","          0.0581]]], device='cuda:0', grad_fn=<UnsqueezeBackward0>)\n","weighted\n","shape :  torch.Size([1, 1, 1024])\n","tensor([[[ 0.0015,  0.0127, -0.0103,  ..., -0.0120, -0.0075,  0.0108]]],\n","       device='cuda:0', grad_fn=<BmmBackward0>)\n","output\n","shape :  torch.Size([1, 1, 512])\n","tensor([[[ 2.2570e-03, -3.8253e-02,  3.7735e-02,  2.5970e-02, -4.4589e-02,\n","          -3.0273e-02,  3.6394e-02, -3.1717e-02,  3.9863e-02, -1.5255e-02,\n","          -3.3338e-02, -1.7419e-02, -3.6518e-02,  3.4689e-02,  2.8800e-02,\n","          -4.0039e-02, -2.2648e-02, -3.6061e-02,  3.0145e-02,  1.6217e-02,\n","           4.0167e-02, -4.4061e-02, -3.1047e-02, -3.9956e-02, -5.9814e-03,\n","          -3.4440e-02,  1.6144e-02, -3.3578e-02, -2.6352e-02,  3.0678e-02,\n","           3.3989e-02,  3.0273e-02,  1.6516e-02,  1.0487e-04, -7.4943e-03,\n","          -6.6586e-04,  1.1805e-02,  3.0889e-02, -3.6117e-02,  3.9597e-02,\n","          -1.6291e-02,  2.7696e-02,  2.6594e-02,  2.5061e-02,  7.6721e-03,\n","          -3.5730e-02, -3.3831e-02,  2.4523e-03, -5.5764e-03,  9.9474e-03,\n","          -2.8743e-02,  3.5365e-02,  3.1851e-02,  3.0036e-02,  3.1861e-02,\n","          -3.9897e-02, -4.6914e-02,  2.7055e-02, -1.9682e-02,  9.5936e-03,\n","          -3.4327e-02, -3.9540e-02, -3.6293e-02, -4.0675e-02,  3.5316e-02,\n","           6.7988e-03, -3.0380e-02, -2.5303e-02,  3.0929e-02, -3.3368e-02,\n","           1.8965e-02, -6.6384e-03, -4.0935e-02,  1.0547e-02,  3.8551e-02,\n","          -2.1062e-02, -1.7702e-02, -2.3691e-02, -3.2288e-02, -3.2095e-02,\n","          -3.0837e-02, -2.9068e-02,  2.7641e-02, -2.1464e-02, -3.8175e-02,\n","          -3.9865e-02, -2.0241e-02, -3.7060e-02,  3.6387e-02, -2.9912e-02,\n","          -7.5894e-03,  3.3749e-02,  2.7372e-02, -2.7908e-02, -2.8593e-02,\n","          -2.0040e-02,  3.2119e-02, -3.9205e-02, -2.2092e-02,  2.6401e-03,\n","           1.8424e-02,  4.3818e-02,  4.4867e-03, -3.9112e-02,  9.3802e-05,\n","           3.4001e-02,  2.9565e-02, -3.3107e-02,  2.6531e-02,  2.9561e-02,\n","          -4.1857e-02,  3.5857e-02,  3.2084e-02, -1.1381e-02, -2.0387e-02,\n","          -2.8603e-02, -2.8873e-02,  2.3737e-02,  3.8286e-02,  3.5980e-02,\n","          -3.4494e-02,  3.3493e-02, -2.2277e-02,  2.1115e-02, -7.6063e-03,\n","          -4.0252e-02, -3.4443e-02,  3.7216e-02,  4.0882e-02,  3.4942e-02,\n","           6.9950e-03, -2.9300e-03, -2.4350e-02,  3.2026e-02, -4.1983e-02,\n","          -2.6793e-02, -3.3058e-02,  3.8543e-02, -3.3286e-02,  3.0363e-02,\n","          -8.0392e-04, -3.2993e-02,  3.3476e-02, -1.8195e-02,  3.3718e-02,\n","           3.2278e-02,  3.0087e-02,  8.2021e-03,  3.6994e-02,  3.8671e-02,\n","          -3.3288e-02,  3.0148e-03, -2.2125e-02, -3.1868e-02, -2.1835e-02,\n","           3.4906e-02, -1.4122e-02,  3.1666e-02,  3.5217e-02, -3.3022e-02,\n","          -3.2342e-02,  3.6243e-02, -2.2592e-02,  3.4220e-02,  3.8984e-02,\n","           2.9320e-02, -3.9222e-02,  2.9824e-02, -3.3082e-02,  3.8918e-02,\n","          -2.7739e-02,  3.6310e-02, -1.0910e-02, -1.1915e-02, -2.9959e-02,\n","          -4.2718e-03,  2.2343e-02, -3.6120e-02, -3.5076e-02, -2.4739e-02,\n","          -3.9858e-02, -2.4768e-02, -3.2745e-02,  2.5793e-02,  3.2107e-02,\n","           3.7052e-02,  3.3697e-02,  3.5334e-02, -3.5515e-02,  3.5781e-02,\n","           7.1162e-04, -2.6319e-02, -3.2555e-02,  2.5639e-02,  1.4822e-02,\n","           3.2281e-02, -2.4175e-02,  3.2613e-02, -3.5959e-02,  1.6648e-02,\n","           4.1497e-02,  3.1208e-02,  2.0676e-02, -3.4533e-02,  3.4016e-02,\n","           2.7009e-02, -3.6873e-02,  2.7568e-02,  2.7521e-02,  3.5123e-02,\n","          -3.5397e-02,  2.3818e-02, -3.9179e-02, -3.2125e-02,  1.5978e-02,\n","          -4.1205e-02,  2.4966e-02,  3.2400e-02,  3.7363e-02, -3.2628e-02,\n","           3.4790e-02,  2.0253e-02, -3.8254e-02, -3.9561e-02, -1.7729e-02,\n","          -1.7066e-02, -3.5628e-02, -2.4403e-02, -2.0647e-02,  3.5881e-02,\n","           3.0978e-02, -2.9159e-02,  3.6586e-02,  1.9943e-02, -3.5890e-02,\n","          -3.4620e-03,  4.2807e-02,  2.9274e-02,  3.7668e-02,  3.3245e-02,\n","           3.5911e-02, -3.2159e-02,  3.5292e-02, -3.3657e-02,  3.8158e-02,\n","           3.6568e-02, -3.6245e-02, -3.2701e-02,  3.1133e-02,  3.1679e-02,\n","           1.8031e-03, -3.3225e-02,  2.3579e-02,  3.6452e-02, -2.0142e-02,\n","           3.9044e-02,  3.0531e-02, -4.4215e-03, -3.2213e-02,  4.1738e-03,\n","          -3.5711e-02,  3.3179e-02,  3.5484e-02, -3.6056e-02, -3.6100e-02,\n","          -7.0733e-04,  4.2051e-02,  3.9146e-02,  3.3684e-03, -3.1837e-02,\n","          -3.8783e-02,  1.5177e-02, -2.0875e-02, -3.7740e-02, -3.2313e-02,\n","          -2.9770e-02, -3.9598e-02,  3.7501e-02, -2.6086e-02, -1.3215e-03,\n","          -2.8364e-02,  3.5805e-02,  3.3522e-02, -5.1516e-05,  3.4641e-02,\n","           2.6417e-02,  3.3285e-02, -3.0818e-02,  5.9357e-03,  3.5905e-02,\n","           2.9529e-02,  3.0337e-02,  3.6891e-02,  1.0010e-02, -4.6690e-03,\n","          -3.8046e-02,  3.4247e-02, -8.5350e-03,  4.0123e-03, -1.5401e-02,\n","           2.0253e-02,  2.5591e-02, -9.6380e-03, -1.8194e-02,  2.6951e-02,\n","           3.9290e-02,  3.1439e-02,  1.3406e-02, -1.1854e-02,  3.5664e-02,\n","           3.0977e-02, -4.0209e-02,  3.3306e-02, -1.1586e-02,  2.0590e-02,\n","          -3.7262e-02, -3.4649e-02, -2.6418e-02, -3.9492e-02, -2.8216e-02,\n","           3.9115e-02, -3.5162e-02, -2.8465e-02, -3.9597e-02, -4.0258e-02,\n","           2.6732e-02, -1.8421e-02,  3.1777e-02,  3.9844e-02,  2.7883e-02,\n","           3.6869e-02, -3.3543e-02,  2.8990e-02, -3.7596e-02, -3.2847e-02,\n","           7.3056e-03,  3.8603e-02,  1.6289e-02,  4.1616e-02, -3.7963e-02,\n","           3.5771e-02, -2.9240e-02, -2.9992e-02,  2.2219e-02,  1.6239e-02,\n","           3.8207e-02,  3.7262e-02,  3.2604e-02, -3.0482e-02, -4.3556e-02,\n","           3.0915e-02,  3.9548e-02,  3.7112e-02, -2.7515e-02,  3.3514e-02,\n","          -1.7190e-02,  2.4560e-02, -4.3151e-02, -3.9141e-02, -3.6613e-02,\n","           3.1045e-02,  3.4323e-02, -1.7538e-02,  2.8712e-02, -2.8562e-02,\n","          -3.5792e-02,  1.1919e-02,  3.5537e-02, -3.8999e-02, -4.1932e-02,\n","           3.3913e-02, -3.8506e-02, -4.3398e-02,  4.0710e-02,  2.9230e-02,\n","           4.0482e-02, -3.2057e-02,  4.0820e-03, -4.2434e-02,  3.6530e-02,\n","          -3.2461e-02,  1.6723e-02, -2.2221e-02, -2.4315e-02,  3.7426e-02,\n","           3.4833e-02,  3.7997e-02, -3.3759e-02, -4.2390e-02,  3.1277e-02,\n","          -3.1190e-02,  3.9525e-02, -3.7090e-02,  3.1981e-02, -2.9964e-02,\n","           3.5171e-02,  3.5420e-02,  2.2853e-02, -1.8023e-02,  3.8033e-02,\n","           3.7929e-02, -3.2902e-02, -3.8041e-02,  8.8540e-03,  3.0192e-02,\n","          -2.4843e-02,  8.0482e-03, -3.1601e-02,  2.7196e-02, -3.2798e-02,\n","           2.7064e-02, -6.1685e-03, -7.7153e-03,  3.3860e-02, -3.7294e-02,\n","           3.4264e-02, -2.2259e-04, -3.0768e-02, -3.7842e-02,  3.6583e-02,\n","          -2.5563e-02, -3.8405e-02,  1.5281e-02, -3.9154e-02,  3.2061e-02,\n","          -1.5665e-02, -3.0298e-02, -2.6575e-02, -3.5850e-02, -3.6353e-02,\n","           2.4760e-02, -3.8710e-02,  1.3621e-02,  3.6215e-02, -1.5095e-02,\n","           3.1064e-02, -3.2685e-02,  3.5884e-02, -1.9320e-02, -3.7717e-02,\n","          -3.5297e-02,  1.6108e-02, -3.4768e-02,  2.6273e-02,  9.7172e-03,\n","           3.5402e-02, -3.6623e-02,  3.4448e-02,  3.5790e-02, -7.6078e-03,\n","          -3.8128e-02,  3.8418e-02,  3.6123e-02, -1.8083e-03, -3.6030e-02,\n","          -3.6018e-02,  3.8513e-02, -4.0426e-02, -3.5133e-02,  3.1849e-02,\n","           3.1990e-02, -3.2372e-02,  3.5276e-02,  3.4268e-02, -2.8946e-02,\n","          -3.8864e-02, -6.6229e-03,  2.6614e-02,  3.3257e-02,  4.7537e-03,\n","           2.7661e-02,  3.8067e-02, -3.3849e-02,  2.5766e-02,  3.7927e-02,\n","           1.0734e-02,  3.8068e-02,  3.4535e-02, -1.2279e-02, -2.7565e-02,\n","           3.6420e-02,  2.2949e-02, -2.6622e-02,  1.4664e-02, -3.5957e-02,\n","          -1.7397e-02, -3.1491e-02,  3.9217e-02,  2.4810e-04, -1.2467e-02,\n","           9.0583e-04,  2.2183e-02,  3.5977e-02, -3.8864e-02,  4.1579e-02,\n","           3.3104e-02,  3.0406e-02, -2.3236e-02,  8.6063e-03,  4.6379e-04,\n","           2.0170e-02, -1.1366e-02, -1.0505e-02,  3.2480e-02,  3.4374e-02,\n","          -3.4472e-02,  3.6802e-02,  3.8613e-02, -1.2478e-02, -3.1966e-03,\n","          -3.1822e-02,  3.3685e-02]]], device='cuda:0',\n","       grad_fn=<CudnnRnnBackward0>)\n","hidden\n","shape :  torch.Size([1, 1, 512])\n","tensor([[[ 2.2570e-03, -3.8253e-02,  3.7735e-02,  2.5970e-02, -4.4589e-02,\n","          -3.0273e-02,  3.6394e-02, -3.1717e-02,  3.9863e-02, -1.5255e-02,\n","          -3.3338e-02, -1.7419e-02, -3.6518e-02,  3.4689e-02,  2.8800e-02,\n","          -4.0039e-02, -2.2648e-02, -3.6061e-02,  3.0145e-02,  1.6217e-02,\n","           4.0167e-02, -4.4061e-02, -3.1047e-02, -3.9956e-02, -5.9814e-03,\n","          -3.4440e-02,  1.6144e-02, -3.3578e-02, -2.6352e-02,  3.0678e-02,\n","           3.3989e-02,  3.0273e-02,  1.6516e-02,  1.0487e-04, -7.4943e-03,\n","          -6.6586e-04,  1.1805e-02,  3.0889e-02, -3.6117e-02,  3.9597e-02,\n","          -1.6291e-02,  2.7696e-02,  2.6594e-02,  2.5061e-02,  7.6721e-03,\n","          -3.5730e-02, -3.3831e-02,  2.4523e-03, -5.5764e-03,  9.9474e-03,\n","          -2.8743e-02,  3.5365e-02,  3.1851e-02,  3.0036e-02,  3.1861e-02,\n","          -3.9897e-02, -4.6914e-02,  2.7055e-02, -1.9682e-02,  9.5936e-03,\n","          -3.4327e-02, -3.9540e-02, -3.6293e-02, -4.0675e-02,  3.5316e-02,\n","           6.7988e-03, -3.0380e-02, -2.5303e-02,  3.0929e-02, -3.3368e-02,\n","           1.8965e-02, -6.6384e-03, -4.0935e-02,  1.0547e-02,  3.8551e-02,\n","          -2.1062e-02, -1.7702e-02, -2.3691e-02, -3.2288e-02, -3.2095e-02,\n","          -3.0837e-02, -2.9068e-02,  2.7641e-02, -2.1464e-02, -3.8175e-02,\n","          -3.9865e-02, -2.0241e-02, -3.7060e-02,  3.6387e-02, -2.9912e-02,\n","          -7.5894e-03,  3.3749e-02,  2.7372e-02, -2.7908e-02, -2.8593e-02,\n","          -2.0040e-02,  3.2119e-02, -3.9205e-02, -2.2092e-02,  2.6401e-03,\n","           1.8424e-02,  4.3818e-02,  4.4867e-03, -3.9112e-02,  9.3802e-05,\n","           3.4001e-02,  2.9565e-02, -3.3107e-02,  2.6531e-02,  2.9561e-02,\n","          -4.1857e-02,  3.5857e-02,  3.2084e-02, -1.1381e-02, -2.0387e-02,\n","          -2.8603e-02, -2.8873e-02,  2.3737e-02,  3.8286e-02,  3.5980e-02,\n","          -3.4494e-02,  3.3493e-02, -2.2277e-02,  2.1115e-02, -7.6063e-03,\n","          -4.0252e-02, -3.4443e-02,  3.7216e-02,  4.0882e-02,  3.4942e-02,\n","           6.9950e-03, -2.9300e-03, -2.4350e-02,  3.2026e-02, -4.1983e-02,\n","          -2.6793e-02, -3.3058e-02,  3.8543e-02, -3.3286e-02,  3.0363e-02,\n","          -8.0392e-04, -3.2993e-02,  3.3476e-02, -1.8195e-02,  3.3718e-02,\n","           3.2278e-02,  3.0087e-02,  8.2021e-03,  3.6994e-02,  3.8671e-02,\n","          -3.3288e-02,  3.0148e-03, -2.2125e-02, -3.1868e-02, -2.1835e-02,\n","           3.4906e-02, -1.4122e-02,  3.1666e-02,  3.5217e-02, -3.3022e-02,\n","          -3.2342e-02,  3.6243e-02, -2.2592e-02,  3.4220e-02,  3.8984e-02,\n","           2.9320e-02, -3.9222e-02,  2.9824e-02, -3.3082e-02,  3.8918e-02,\n","          -2.7739e-02,  3.6310e-02, -1.0910e-02, -1.1915e-02, -2.9959e-02,\n","          -4.2718e-03,  2.2343e-02, -3.6120e-02, -3.5076e-02, -2.4739e-02,\n","          -3.9858e-02, -2.4768e-02, -3.2745e-02,  2.5793e-02,  3.2107e-02,\n","           3.7052e-02,  3.3697e-02,  3.5334e-02, -3.5515e-02,  3.5781e-02,\n","           7.1162e-04, -2.6319e-02, -3.2555e-02,  2.5639e-02,  1.4822e-02,\n","           3.2281e-02, -2.4175e-02,  3.2613e-02, -3.5959e-02,  1.6648e-02,\n","           4.1497e-02,  3.1208e-02,  2.0676e-02, -3.4533e-02,  3.4016e-02,\n","           2.7009e-02, -3.6873e-02,  2.7568e-02,  2.7521e-02,  3.5123e-02,\n","          -3.5397e-02,  2.3818e-02, -3.9179e-02, -3.2125e-02,  1.5978e-02,\n","          -4.1205e-02,  2.4966e-02,  3.2400e-02,  3.7363e-02, -3.2628e-02,\n","           3.4790e-02,  2.0253e-02, -3.8254e-02, -3.9561e-02, -1.7729e-02,\n","          -1.7066e-02, -3.5628e-02, -2.4403e-02, -2.0647e-02,  3.5881e-02,\n","           3.0978e-02, -2.9159e-02,  3.6586e-02,  1.9943e-02, -3.5890e-02,\n","          -3.4620e-03,  4.2807e-02,  2.9274e-02,  3.7668e-02,  3.3245e-02,\n","           3.5911e-02, -3.2159e-02,  3.5292e-02, -3.3657e-02,  3.8158e-02,\n","           3.6568e-02, -3.6245e-02, -3.2701e-02,  3.1133e-02,  3.1679e-02,\n","           1.8031e-03, -3.3225e-02,  2.3579e-02,  3.6452e-02, -2.0142e-02,\n","           3.9044e-02,  3.0531e-02, -4.4215e-03, -3.2213e-02,  4.1738e-03,\n","          -3.5711e-02,  3.3179e-02,  3.5484e-02, -3.6056e-02, -3.6100e-02,\n","          -7.0733e-04,  4.2051e-02,  3.9146e-02,  3.3684e-03, -3.1837e-02,\n","          -3.8783e-02,  1.5177e-02, -2.0875e-02, -3.7740e-02, -3.2313e-02,\n","          -2.9770e-02, -3.9598e-02,  3.7501e-02, -2.6086e-02, -1.3215e-03,\n","          -2.8364e-02,  3.5805e-02,  3.3522e-02, -5.1516e-05,  3.4641e-02,\n","           2.6417e-02,  3.3285e-02, -3.0818e-02,  5.9357e-03,  3.5905e-02,\n","           2.9529e-02,  3.0337e-02,  3.6891e-02,  1.0010e-02, -4.6690e-03,\n","          -3.8046e-02,  3.4247e-02, -8.5350e-03,  4.0123e-03, -1.5401e-02,\n","           2.0253e-02,  2.5591e-02, -9.6380e-03, -1.8194e-02,  2.6951e-02,\n","           3.9290e-02,  3.1439e-02,  1.3406e-02, -1.1854e-02,  3.5664e-02,\n","           3.0977e-02, -4.0209e-02,  3.3306e-02, -1.1586e-02,  2.0590e-02,\n","          -3.7262e-02, -3.4649e-02, -2.6418e-02, -3.9492e-02, -2.8216e-02,\n","           3.9115e-02, -3.5162e-02, -2.8465e-02, -3.9597e-02, -4.0258e-02,\n","           2.6732e-02, -1.8421e-02,  3.1777e-02,  3.9844e-02,  2.7883e-02,\n","           3.6869e-02, -3.3543e-02,  2.8990e-02, -3.7596e-02, -3.2847e-02,\n","           7.3056e-03,  3.8603e-02,  1.6289e-02,  4.1616e-02, -3.7963e-02,\n","           3.5771e-02, -2.9240e-02, -2.9992e-02,  2.2219e-02,  1.6239e-02,\n","           3.8207e-02,  3.7262e-02,  3.2604e-02, -3.0482e-02, -4.3556e-02,\n","           3.0915e-02,  3.9548e-02,  3.7112e-02, -2.7515e-02,  3.3514e-02,\n","          -1.7190e-02,  2.4560e-02, -4.3151e-02, -3.9141e-02, -3.6613e-02,\n","           3.1045e-02,  3.4323e-02, -1.7538e-02,  2.8712e-02, -2.8562e-02,\n","          -3.5792e-02,  1.1919e-02,  3.5537e-02, -3.8999e-02, -4.1932e-02,\n","           3.3913e-02, -3.8506e-02, -4.3398e-02,  4.0710e-02,  2.9230e-02,\n","           4.0482e-02, -3.2057e-02,  4.0820e-03, -4.2434e-02,  3.6530e-02,\n","          -3.2461e-02,  1.6723e-02, -2.2221e-02, -2.4315e-02,  3.7426e-02,\n","           3.4833e-02,  3.7997e-02, -3.3759e-02, -4.2390e-02,  3.1277e-02,\n","          -3.1190e-02,  3.9525e-02, -3.7090e-02,  3.1981e-02, -2.9964e-02,\n","           3.5171e-02,  3.5420e-02,  2.2853e-02, -1.8023e-02,  3.8033e-02,\n","           3.7929e-02, -3.2902e-02, -3.8041e-02,  8.8540e-03,  3.0192e-02,\n","          -2.4843e-02,  8.0482e-03, -3.1601e-02,  2.7196e-02, -3.2798e-02,\n","           2.7064e-02, -6.1685e-03, -7.7153e-03,  3.3860e-02, -3.7294e-02,\n","           3.4264e-02, -2.2259e-04, -3.0768e-02, -3.7842e-02,  3.6583e-02,\n","          -2.5563e-02, -3.8405e-02,  1.5281e-02, -3.9154e-02,  3.2061e-02,\n","          -1.5665e-02, -3.0298e-02, -2.6575e-02, -3.5850e-02, -3.6353e-02,\n","           2.4760e-02, -3.8710e-02,  1.3621e-02,  3.6215e-02, -1.5095e-02,\n","           3.1064e-02, -3.2685e-02,  3.5884e-02, -1.9320e-02, -3.7717e-02,\n","          -3.5297e-02,  1.6108e-02, -3.4768e-02,  2.6273e-02,  9.7172e-03,\n","           3.5402e-02, -3.6623e-02,  3.4448e-02,  3.5790e-02, -7.6078e-03,\n","          -3.8128e-02,  3.8418e-02,  3.6123e-02, -1.8083e-03, -3.6030e-02,\n","          -3.6018e-02,  3.8513e-02, -4.0426e-02, -3.5133e-02,  3.1849e-02,\n","           3.1990e-02, -3.2372e-02,  3.5276e-02,  3.4268e-02, -2.8946e-02,\n","          -3.8864e-02, -6.6229e-03,  2.6614e-02,  3.3257e-02,  4.7537e-03,\n","           2.7661e-02,  3.8067e-02, -3.3849e-02,  2.5766e-02,  3.7927e-02,\n","           1.0734e-02,  3.8068e-02,  3.4535e-02, -1.2279e-02, -2.7565e-02,\n","           3.6420e-02,  2.2949e-02, -2.6622e-02,  1.4664e-02, -3.5957e-02,\n","          -1.7397e-02, -3.1491e-02,  3.9217e-02,  2.4810e-04, -1.2467e-02,\n","           9.0583e-04,  2.2183e-02,  3.5977e-02, -3.8864e-02,  4.1579e-02,\n","           3.3104e-02,  3.0406e-02, -2.3236e-02,  8.6063e-03,  4.6379e-04,\n","           2.0170e-02, -1.1366e-02, -1.0505e-02,  3.2480e-02,  3.4374e-02,\n","          -3.4472e-02,  3.6802e-02,  3.8613e-02, -1.2478e-02, -3.1966e-03,\n","          -3.1822e-02,  3.3685e-02]]], device='cuda:0',\n","       grad_fn=<CudnnRnnBackward0>)\n","prediction\n","shape :  torch.Size([1, 5893])\n","tensor([[ 0.0723, -0.0628, -0.0722,  ..., -0.0710, -0.0662, -0.0729]],\n","       device='cuda:0', grad_fn=<AddmmBackward0>)\n","input\n","shape :  torch.Size([1, 1])\n","tensor([[4]], device='cuda:0')\n","embedded\n","shape :  torch.Size([1, 1, 256])\n","tensor([[[ 1.6198e-02,  3.4645e-02,  0.0000e+00,  3.0081e-02,  0.0000e+00,\n","           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.1846e-03,\n","           9.1595e-03,  0.0000e+00,  0.0000e+00,  2.6623e-02,  3.5461e-02,\n","           0.0000e+00,  1.2650e-02,  2.2573e-02,  2.3558e-02,  0.0000e+00,\n","           0.0000e+00,  9.1617e-04, -7.1932e-03,  0.0000e+00, -1.1886e-03,\n","          -3.9367e-02,  0.0000e+00, -1.7484e-02,  2.1988e-02, -4.5220e-03,\n","           3.2308e-03, -7.6678e-03,  0.0000e+00,  3.5479e-02,  0.0000e+00,\n","           0.0000e+00,  0.0000e+00,  0.0000e+00,  1.8450e-02,  0.0000e+00,\n","           0.0000e+00,  0.0000e+00,  0.0000e+00,  4.3452e-02,  0.0000e+00,\n","           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n","           4.9885e-02,  0.0000e+00, -1.7271e-02,  1.5164e-02,  2.6292e-02,\n","           0.0000e+00,  2.4577e-02, -9.0013e-04, -4.0109e-03, -1.7841e-02,\n","           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -2.6164e-02,\n","          -2.7826e-02,  3.8585e-02, -1.7205e-02,  0.0000e+00,  0.0000e+00,\n","          -1.6763e-02,  0.0000e+00,  0.0000e+00,  5.0927e-03, -1.6040e-02,\n","          -1.9211e-02,  2.1168e-02,  0.0000e+00,  0.0000e+00, -1.1366e-02,\n","          -8.9516e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n","           0.0000e+00,  0.0000e+00,  0.0000e+00,  1.7465e-02,  0.0000e+00,\n","           0.0000e+00,  1.3733e-02, -2.3955e-02,  0.0000e+00, -8.3767e-03,\n","           0.0000e+00, -1.7144e-02, -2.0959e-02, -5.3746e-03,  0.0000e+00,\n","           1.5936e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.6670e-02,\n","           0.0000e+00,  2.9442e-02,  4.2129e-02,  4.0837e-02,  5.6076e-03,\n","           7.6755e-03,  0.0000e+00, -3.7044e-02,  0.0000e+00,  0.0000e+00,\n","           0.0000e+00, -7.6056e-03,  2.3016e-04,  0.0000e+00,  2.5596e-02,\n","           0.0000e+00, -5.5914e-03,  0.0000e+00, -1.8731e-02, -2.8633e-02,\n","           0.0000e+00,  2.2462e-02, -2.8256e-02,  1.1942e-02,  0.0000e+00,\n","           3.1326e-02,  0.0000e+00,  7.2169e-03,  0.0000e+00,  0.0000e+00,\n","          -4.4891e-02,  0.0000e+00, -2.6115e-02,  0.0000e+00,  2.8670e-02,\n","           0.0000e+00,  0.0000e+00,  3.2835e-02,  0.0000e+00, -1.7256e-02,\n","           0.0000e+00, -1.9388e-02, -9.3979e-03,  8.4739e-03,  0.0000e+00,\n","           0.0000e+00,  5.7542e-03,  0.0000e+00,  4.7561e-02,  0.0000e+00,\n","           0.0000e+00,  0.0000e+00, -8.5997e-03, -1.6569e-02,  0.0000e+00,\n","           0.0000e+00,  0.0000e+00, -1.7497e-02, -3.4400e-03, -3.3899e-02,\n","          -2.4264e-02,  0.0000e+00, -7.3450e-03,  0.0000e+00,  0.0000e+00,\n","           2.5278e-02, -1.3256e-02,  0.0000e+00, -4.3250e-03,  0.0000e+00,\n","           1.5267e-02, -1.6279e-02,  4.2635e-02, -3.3425e-05,  0.0000e+00,\n","           0.0000e+00,  2.7508e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n","           0.0000e+00,  0.0000e+00,  0.0000e+00, -2.4396e-02, -8.2209e-03,\n","           7.7654e-03,  1.0814e-02,  1.8444e-02,  0.0000e+00,  5.9375e-03,\n","           0.0000e+00,  2.5312e-02,  0.0000e+00, -1.5369e-03, -1.5033e-02,\n","           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.2535e-02,\n","           1.8396e-02,  0.0000e+00,  8.0419e-05,  1.2856e-02,  7.0679e-03,\n","           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n","          -1.1707e-02,  5.0956e-04, -1.3479e-02,  2.3792e-02,  0.0000e+00,\n","           0.0000e+00, -2.4792e-03, -1.1039e-02, -1.3922e-02,  0.0000e+00,\n","          -1.8195e-02,  0.0000e+00,  9.2606e-03,  1.8188e-02,  0.0000e+00,\n","           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -5.1313e-02,\n","           0.0000e+00, -1.7875e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n","           0.0000e+00,  0.0000e+00, -1.9487e-03, -1.1276e-02,  0.0000e+00,\n","           3.5272e-02, -1.9455e-02, -1.7182e-04,  0.0000e+00,  0.0000e+00,\n","          -9.4923e-03,  6.0281e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n","           1.2824e-02]]], device='cuda:0', grad_fn=<FusedDropoutBackward0>)\n","hidden :  torch.Size([1, 512])\n","hidden_unsqueeze:  torch.Size([1, 17, 512])\n","encoder_outputs :  torch.Size([17, 1, 1024])\n","encoder_outputs_permute :  torch.Size([1, 17, 1024])\n","energy\n","shape :  torch.Size([1, 17, 512])\n","tensor([[[-0.0198,  0.0181, -0.0088,  ...,  0.0210,  0.0046,  0.0236],\n","         [-0.0210,  0.0218, -0.0107,  ...,  0.0238,  0.0060,  0.0256],\n","         [-0.0206,  0.0222, -0.0121,  ...,  0.0252,  0.0085,  0.0270],\n","         ...,\n","         [-0.0199,  0.0209, -0.0082,  ...,  0.0205,  0.0078,  0.0241],\n","         [-0.0195,  0.0211, -0.0091,  ...,  0.0203,  0.0085,  0.0237],\n","         [-0.0184,  0.0187, -0.0072,  ...,  0.0174,  0.0077,  0.0213]]],\n","       device='cuda:0', grad_fn=<TanhBackward0>)\n","attention\n","shape:  torch.Size([1, 17])\n","tensor([[0.0579, 0.0585, 0.0589, 0.0592, 0.0592, 0.0594, 0.0594, 0.0595, 0.0591,\n","         0.0589, 0.0586, 0.0590, 0.0587, 0.0587, 0.0583, 0.0585, 0.0581]],\n","       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n","a\n","shape :  torch.Size([1, 17])\n","tensor([[0.0579, 0.0585, 0.0589, 0.0592, 0.0592, 0.0594, 0.0594, 0.0595, 0.0591,\n","         0.0589, 0.0586, 0.0590, 0.0587, 0.0587, 0.0583, 0.0585, 0.0581]],\n","       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n","a_unsqueeze\n","shape :  torch.Size([1, 1, 17])\n","tensor([[[0.0579, 0.0585, 0.0589, 0.0592, 0.0592, 0.0594, 0.0594, 0.0595,\n","          0.0591, 0.0589, 0.0586, 0.0590, 0.0587, 0.0587, 0.0583, 0.0585,\n","          0.0581]]], device='cuda:0', grad_fn=<UnsqueezeBackward0>)\n","weighted\n","shape :  torch.Size([1, 1, 1024])\n","tensor([[[ 0.0015,  0.0127, -0.0103,  ..., -0.0120, -0.0075,  0.0108]]],\n","       device='cuda:0', grad_fn=<BmmBackward0>)\n","output\n","shape :  torch.Size([1, 1, 512])\n","tensor([[[ 0.0248, -0.0500,  0.0443,  0.0229, -0.0463, -0.0398,  0.0434,\n","          -0.0301,  0.0463, -0.0105, -0.0432, -0.0144, -0.0434,  0.0398,\n","           0.0366, -0.0488, -0.0240, -0.0425,  0.0341,  0.0105,  0.0488,\n","          -0.0512, -0.0374, -0.0460, -0.0215, -0.0369,  0.0231, -0.0366,\n","          -0.0358,  0.0395,  0.0428,  0.0387,  0.0264, -0.0163, -0.0321,\n","          -0.0189,  0.0282,  0.0407, -0.0419,  0.0510, -0.0345,  0.0362,\n","           0.0351,  0.0312, -0.0045, -0.0373, -0.0460,  0.0222, -0.0215,\n","          -0.0088, -0.0368,  0.0402,  0.0388,  0.0356,  0.0412, -0.0457,\n","          -0.0557,  0.0383, -0.0112, -0.0050, -0.0404, -0.0457, -0.0462,\n","          -0.0477,  0.0407, -0.0124, -0.0367, -0.0222,  0.0404, -0.0388,\n","           0.0332, -0.0242, -0.0419,  0.0304,  0.0455, -0.0208, -0.0045,\n","          -0.0333, -0.0393, -0.0347, -0.0347, -0.0255,  0.0392, -0.0203,\n","          -0.0443, -0.0461, -0.0201, -0.0447,  0.0444, -0.0326,  0.0064,\n","           0.0393,  0.0222, -0.0377, -0.0411, -0.0321,  0.0346, -0.0465,\n","          -0.0360, -0.0025,  0.0327,  0.0525, -0.0174, -0.0474,  0.0203,\n","           0.0391,  0.0323, -0.0440,  0.0349,  0.0402, -0.0437,  0.0434,\n","           0.0383, -0.0302, -0.0151, -0.0246, -0.0269,  0.0205,  0.0433,\n","           0.0424, -0.0420,  0.0439, -0.0113,  0.0308,  0.0060, -0.0490,\n","          -0.0405,  0.0429,  0.0490,  0.0414, -0.0100, -0.0224, -0.0403,\n","           0.0398, -0.0480, -0.0389, -0.0451,  0.0451, -0.0373,  0.0383,\n","          -0.0166, -0.0426,  0.0372, -0.0179,  0.0391,  0.0380,  0.0353,\n","          -0.0059,  0.0452,  0.0455, -0.0423, -0.0110, -0.0361, -0.0411,\n","          -0.0221,  0.0411, -0.0351,  0.0371,  0.0393, -0.0383, -0.0377,\n","           0.0422, -0.0195,  0.0416,  0.0469,  0.0401, -0.0469,  0.0383,\n","          -0.0426,  0.0466, -0.0301,  0.0448, -0.0154,  0.0024, -0.0364,\n","          -0.0174,  0.0332, -0.0474, -0.0450, -0.0354, -0.0493, -0.0232,\n","          -0.0404,  0.0164,  0.0416,  0.0440,  0.0414,  0.0382, -0.0456,\n","           0.0424,  0.0206, -0.0258, -0.0379,  0.0399,  0.0366,  0.0429,\n","          -0.0375,  0.0374, -0.0400,  0.0288,  0.0499,  0.0388,  0.0177,\n","          -0.0441,  0.0425,  0.0374, -0.0418,  0.0374,  0.0227,  0.0409,\n","          -0.0428,  0.0369, -0.0496, -0.0438,  0.0064, -0.0451,  0.0359,\n","           0.0415,  0.0476, -0.0384,  0.0460,  0.0116, -0.0441, -0.0482,\n","          -0.0354, -0.0324, -0.0425, -0.0389, -0.0203,  0.0460,  0.0362,\n","          -0.0262,  0.0416,  0.0347, -0.0468, -0.0063,  0.0520,  0.0259,\n","           0.0452,  0.0403,  0.0432, -0.0422,  0.0417, -0.0367,  0.0456,\n","           0.0415, -0.0439, -0.0427,  0.0365,  0.0398,  0.0165, -0.0410,\n","           0.0229,  0.0452, -0.0278,  0.0463,  0.0418, -0.0197, -0.0463,\n","           0.0214, -0.0412,  0.0272,  0.0422, -0.0443, -0.0434,  0.0115,\n","           0.0484,  0.0446,  0.0142, -0.0397, -0.0487,  0.0125, -0.0160,\n","          -0.0458, -0.0443, -0.0420, -0.0468,  0.0473, -0.0284, -0.0042,\n","          -0.0382,  0.0398,  0.0388, -0.0165,  0.0440,  0.0347,  0.0360,\n","          -0.0400,  0.0223,  0.0440,  0.0374,  0.0335,  0.0454,  0.0287,\n","           0.0187, -0.0483,  0.0443,  0.0051,  0.0137, -0.0247,  0.0285,\n","           0.0261, -0.0252, -0.0337,  0.0361,  0.0490,  0.0363,  0.0288,\n","          -0.0240,  0.0418,  0.0381, -0.0499,  0.0429, -0.0288,  0.0361,\n","          -0.0452, -0.0452, -0.0279, -0.0457, -0.0420,  0.0494, -0.0418,\n","          -0.0375, -0.0494, -0.0474,  0.0318, -0.0093,  0.0448,  0.0441,\n","           0.0406,  0.0458, -0.0380,  0.0380, -0.0431, -0.0343,  0.0284,\n","           0.0453,  0.0291,  0.0519, -0.0471,  0.0460, -0.0374, -0.0377,\n","           0.0211,  0.0058,  0.0472,  0.0435,  0.0399, -0.0359, -0.0486,\n","           0.0441,  0.0454,  0.0452, -0.0224,  0.0460, -0.0165,  0.0384,\n","          -0.0505, -0.0438, -0.0407,  0.0437,  0.0445, -0.0130,  0.0292,\n","          -0.0429, -0.0418,  0.0283,  0.0392, -0.0461, -0.0479,  0.0423,\n","          -0.0472, -0.0537,  0.0514,  0.0378,  0.0436, -0.0373,  0.0207,\n","          -0.0487,  0.0432, -0.0395,  0.0177, -0.0137, -0.0244,  0.0462,\n","           0.0427,  0.0459, -0.0372, -0.0507,  0.0411, -0.0335,  0.0388,\n","          -0.0396,  0.0318, -0.0344,  0.0433,  0.0431,  0.0349, -0.0324,\n","           0.0458,  0.0431, -0.0393, -0.0465,  0.0254,  0.0358, -0.0415,\n","           0.0285, -0.0393,  0.0337, -0.0420,  0.0256, -0.0031,  0.0113,\n","           0.0390, -0.0437,  0.0421, -0.0177, -0.0377, -0.0459,  0.0482,\n","          -0.0217, -0.0449,  0.0074, -0.0513,  0.0411, -0.0076, -0.0272,\n","          -0.0272, -0.0418, -0.0441,  0.0218, -0.0499,  0.0236,  0.0472,\n","          -0.0339,  0.0409, -0.0383,  0.0433, -0.0334, -0.0455, -0.0446,\n","           0.0291, -0.0475,  0.0342,  0.0280,  0.0455, -0.0434,  0.0440,\n","           0.0456, -0.0261, -0.0479,  0.0474,  0.0400,  0.0032, -0.0429,\n","          -0.0396,  0.0472, -0.0473, -0.0432,  0.0392,  0.0380, -0.0388,\n","           0.0448,  0.0356, -0.0397, -0.0464,  0.0108,  0.0332,  0.0462,\n","           0.0275,  0.0420,  0.0469, -0.0423,  0.0378,  0.0460, -0.0044,\n","           0.0460,  0.0415, -0.0191, -0.0351,  0.0366,  0.0227, -0.0391,\n","           0.0034, -0.0388, -0.0270, -0.0262,  0.0460, -0.0174, -0.0291,\n","          -0.0183,  0.0124,  0.0475, -0.0456,  0.0495,  0.0303,  0.0367,\n","          -0.0192,  0.0267,  0.0103,  0.0193,  0.0001, -0.0306,  0.0392,\n","           0.0431, -0.0364,  0.0457,  0.0460, -0.0297, -0.0224, -0.0307,\n","           0.0360]]], device='cuda:0', grad_fn=<CudnnRnnBackward0>)\n","hidden\n","shape :  torch.Size([1, 1, 512])\n","tensor([[[ 0.0248, -0.0500,  0.0443,  0.0229, -0.0463, -0.0398,  0.0434,\n","          -0.0301,  0.0463, -0.0105, -0.0432, -0.0144, -0.0434,  0.0398,\n","           0.0366, -0.0488, -0.0240, -0.0425,  0.0341,  0.0105,  0.0488,\n","          -0.0512, -0.0374, -0.0460, -0.0215, -0.0369,  0.0231, -0.0366,\n","          -0.0358,  0.0395,  0.0428,  0.0387,  0.0264, -0.0163, -0.0321,\n","          -0.0189,  0.0282,  0.0407, -0.0419,  0.0510, -0.0345,  0.0362,\n","           0.0351,  0.0312, -0.0045, -0.0373, -0.0460,  0.0222, -0.0215,\n","          -0.0088, -0.0368,  0.0402,  0.0388,  0.0356,  0.0412, -0.0457,\n","          -0.0557,  0.0383, -0.0112, -0.0050, -0.0404, -0.0457, -0.0462,\n","          -0.0477,  0.0407, -0.0124, -0.0367, -0.0222,  0.0404, -0.0388,\n","           0.0332, -0.0242, -0.0419,  0.0304,  0.0455, -0.0208, -0.0045,\n","          -0.0333, -0.0393, -0.0347, -0.0347, -0.0255,  0.0392, -0.0203,\n","          -0.0443, -0.0461, -0.0201, -0.0447,  0.0444, -0.0326,  0.0064,\n","           0.0393,  0.0222, -0.0377, -0.0411, -0.0321,  0.0346, -0.0465,\n","          -0.0360, -0.0025,  0.0327,  0.0525, -0.0174, -0.0474,  0.0203,\n","           0.0391,  0.0323, -0.0440,  0.0349,  0.0402, -0.0437,  0.0434,\n","           0.0383, -0.0302, -0.0151, -0.0246, -0.0269,  0.0205,  0.0433,\n","           0.0424, -0.0420,  0.0439, -0.0113,  0.0308,  0.0060, -0.0490,\n","          -0.0405,  0.0429,  0.0490,  0.0414, -0.0100, -0.0224, -0.0403,\n","           0.0398, -0.0480, -0.0389, -0.0451,  0.0451, -0.0373,  0.0383,\n","          -0.0166, -0.0426,  0.0372, -0.0179,  0.0391,  0.0380,  0.0353,\n","          -0.0059,  0.0452,  0.0455, -0.0423, -0.0110, -0.0361, -0.0411,\n","          -0.0221,  0.0411, -0.0351,  0.0371,  0.0393, -0.0383, -0.0377,\n","           0.0422, -0.0195,  0.0416,  0.0469,  0.0401, -0.0469,  0.0383,\n","          -0.0426,  0.0466, -0.0301,  0.0448, -0.0154,  0.0024, -0.0364,\n","          -0.0174,  0.0332, -0.0474, -0.0450, -0.0354, -0.0493, -0.0232,\n","          -0.0404,  0.0164,  0.0416,  0.0440,  0.0414,  0.0382, -0.0456,\n","           0.0424,  0.0206, -0.0258, -0.0379,  0.0399,  0.0366,  0.0429,\n","          -0.0375,  0.0374, -0.0400,  0.0288,  0.0499,  0.0388,  0.0177,\n","          -0.0441,  0.0425,  0.0374, -0.0418,  0.0374,  0.0227,  0.0409,\n","          -0.0428,  0.0369, -0.0496, -0.0438,  0.0064, -0.0451,  0.0359,\n","           0.0415,  0.0476, -0.0384,  0.0460,  0.0116, -0.0441, -0.0482,\n","          -0.0354, -0.0324, -0.0425, -0.0389, -0.0203,  0.0460,  0.0362,\n","          -0.0262,  0.0416,  0.0347, -0.0468, -0.0063,  0.0520,  0.0259,\n","           0.0452,  0.0403,  0.0432, -0.0422,  0.0417, -0.0367,  0.0456,\n","           0.0415, -0.0439, -0.0427,  0.0365,  0.0398,  0.0165, -0.0410,\n","           0.0229,  0.0452, -0.0278,  0.0463,  0.0418, -0.0197, -0.0463,\n","           0.0214, -0.0412,  0.0272,  0.0422, -0.0443, -0.0434,  0.0115,\n","           0.0484,  0.0446,  0.0142, -0.0397, -0.0487,  0.0125, -0.0160,\n","          -0.0458, -0.0443, -0.0420, -0.0468,  0.0473, -0.0284, -0.0042,\n","          -0.0382,  0.0398,  0.0388, -0.0165,  0.0440,  0.0347,  0.0360,\n","          -0.0400,  0.0223,  0.0440,  0.0374,  0.0335,  0.0454,  0.0287,\n","           0.0187, -0.0483,  0.0443,  0.0051,  0.0137, -0.0247,  0.0285,\n","           0.0261, -0.0252, -0.0337,  0.0361,  0.0490,  0.0363,  0.0288,\n","          -0.0240,  0.0418,  0.0381, -0.0499,  0.0429, -0.0288,  0.0361,\n","          -0.0452, -0.0452, -0.0279, -0.0457, -0.0420,  0.0494, -0.0418,\n","          -0.0375, -0.0494, -0.0474,  0.0318, -0.0093,  0.0448,  0.0441,\n","           0.0406,  0.0458, -0.0380,  0.0380, -0.0431, -0.0343,  0.0284,\n","           0.0453,  0.0291,  0.0519, -0.0471,  0.0460, -0.0374, -0.0377,\n","           0.0211,  0.0058,  0.0472,  0.0435,  0.0399, -0.0359, -0.0486,\n","           0.0441,  0.0454,  0.0452, -0.0224,  0.0460, -0.0165,  0.0384,\n","          -0.0505, -0.0438, -0.0407,  0.0437,  0.0445, -0.0130,  0.0292,\n","          -0.0429, -0.0418,  0.0283,  0.0392, -0.0461, -0.0479,  0.0423,\n","          -0.0472, -0.0537,  0.0514,  0.0378,  0.0436, -0.0373,  0.0207,\n","          -0.0487,  0.0432, -0.0395,  0.0177, -0.0137, -0.0244,  0.0462,\n","           0.0427,  0.0459, -0.0372, -0.0507,  0.0411, -0.0335,  0.0388,\n","          -0.0396,  0.0318, -0.0344,  0.0433,  0.0431,  0.0349, -0.0324,\n","           0.0458,  0.0431, -0.0393, -0.0465,  0.0254,  0.0358, -0.0415,\n","           0.0285, -0.0393,  0.0337, -0.0420,  0.0256, -0.0031,  0.0113,\n","           0.0390, -0.0437,  0.0421, -0.0177, -0.0377, -0.0459,  0.0482,\n","          -0.0217, -0.0449,  0.0074, -0.0513,  0.0411, -0.0076, -0.0272,\n","          -0.0272, -0.0418, -0.0441,  0.0218, -0.0499,  0.0236,  0.0472,\n","          -0.0339,  0.0409, -0.0383,  0.0433, -0.0334, -0.0455, -0.0446,\n","           0.0291, -0.0475,  0.0342,  0.0280,  0.0455, -0.0434,  0.0440,\n","           0.0456, -0.0261, -0.0479,  0.0474,  0.0400,  0.0032, -0.0429,\n","          -0.0396,  0.0472, -0.0473, -0.0432,  0.0392,  0.0380, -0.0388,\n","           0.0448,  0.0356, -0.0397, -0.0464,  0.0108,  0.0332,  0.0462,\n","           0.0275,  0.0420,  0.0469, -0.0423,  0.0378,  0.0460, -0.0044,\n","           0.0460,  0.0415, -0.0191, -0.0351,  0.0366,  0.0227, -0.0391,\n","           0.0034, -0.0388, -0.0270, -0.0262,  0.0460, -0.0174, -0.0291,\n","          -0.0183,  0.0124,  0.0475, -0.0456,  0.0495,  0.0303,  0.0367,\n","          -0.0192,  0.0267,  0.0103,  0.0193,  0.0001, -0.0306,  0.0392,\n","           0.0431, -0.0364,  0.0457,  0.0460, -0.0297, -0.0224, -0.0307,\n","           0.0360]]], device='cuda:0', grad_fn=<CudnnRnnBackward0>)\n","prediction\n","shape :  torch.Size([1, 5893])\n","tensor([[ 0.0893, -0.0788, -0.0802,  ..., -0.0808, -0.0812, -0.0815]],\n","       device='cuda:0', grad_fn=<AddmmBackward0>)\n","input\n","shape :  torch.Size([1, 1])\n","tensor([[33]], device='cuda:0')\n","embedded\n","shape :  torch.Size([1, 1, 256])\n","tensor([[[-0.0352,  0.0000, -0.0279,  0.0000,  0.0025,  0.0000,  0.0000,\n","           0.0000,  0.0000,  0.0000,  0.0000,  0.0109,  0.0166, -0.0017,\n","           0.0000, -0.0182,  0.0184, -0.0203, -0.0108,  0.0152,  0.0178,\n","           0.0000,  0.0000,  0.0156,  0.0013, -0.0145,  0.0123,  0.0028,\n","          -0.0047, -0.0167,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","           0.0000, -0.0217, -0.0076,  0.0014,  0.0325,  0.0000,  0.0242,\n","           0.0000, -0.0342,  0.0143,  0.0092,  0.0000,  0.0416,  0.0000,\n","           0.0000,  0.0000,  0.0000,  0.0000,  0.0003,  0.0296,  0.0000,\n","          -0.0170, -0.0154,  0.0246,  0.0000,  0.0000,  0.0389,  0.0000,\n","           0.0290,  0.0000,  0.0000,  0.0000,  0.0290,  0.0000,  0.0310,\n","          -0.0117,  0.0313,  0.0000,  0.0000,  0.0105, -0.0124,  0.0093,\n","           0.0273, -0.0451,  0.0000,  0.0000,  0.0033,  0.0000,  0.0083,\n","          -0.0099,  0.0000, -0.0159,  0.0000,  0.0000,  0.0211, -0.0105,\n","           0.0000,  0.0000, -0.0162, -0.0227,  0.0000,  0.0000, -0.0533,\n","           0.0000,  0.0000,  0.0000,  0.0171,  0.0000,  0.0000,  0.0104,\n","           0.0233,  0.0000,  0.0000, -0.0227, -0.0235,  0.0000,  0.0000,\n","           0.0000,  0.0002,  0.0000,  0.0000,  0.0000,  0.0052, -0.0150,\n","          -0.0113,  0.0000, -0.0139, -0.0184,  0.0120,  0.0000,  0.0000,\n","           0.0243,  0.0055, -0.0242, -0.0103, -0.0003,  0.0000, -0.0133,\n","           0.0000,  0.0094,  0.0227,  0.0000,  0.0000, -0.0079, -0.0007,\n","           0.0000,  0.0314,  0.0000, -0.0543,  0.0198,  0.0087, -0.0093,\n","           0.0000,  0.0000,  0.0000,  0.0000,  0.0070,  0.0000, -0.0236,\n","           0.0000,  0.0087, -0.0153,  0.0000,  0.0000,  0.0000, -0.0346,\n","           0.0000,  0.0169,  0.0000, -0.0260, -0.0041,  0.0261,  0.0000,\n","           0.0000,  0.0208, -0.0125, -0.0103,  0.0000,  0.0000, -0.0084,\n","          -0.0040,  0.0000, -0.0322, -0.0380,  0.0000,  0.0236,  0.0000,\n","           0.0000,  0.0000, -0.0301,  0.0000,  0.0372,  0.0000, -0.0133,\n","          -0.0174, -0.0030,  0.0272,  0.0078, -0.0105,  0.0000,  0.0010,\n","           0.0000,  0.0000,  0.0000, -0.0168,  0.0018,  0.0000,  0.0000,\n","           0.0000,  0.0000,  0.0014, -0.0112,  0.0000,  0.0000,  0.0000,\n","           0.0000,  0.0143,  0.0134, -0.0168,  0.0109, -0.0140,  0.0000,\n","           0.0000,  0.0000, -0.0023,  0.0000,  0.0000, -0.0255, -0.0023,\n","           0.0000, -0.0181,  0.0109,  0.0000,  0.0646,  0.0000,  0.0032,\n","           0.0000, -0.0154,  0.0000,  0.0242,  0.0000,  0.0000,  0.0000,\n","           0.0000,  0.0480, -0.0086,  0.0000,  0.0134,  0.0000,  0.0000,\n","           0.0000,  0.0012,  0.0207,  0.0087,  0.0000,  0.0223,  0.0000,\n","           0.0259,  0.0036,  0.0023,  0.0195]]], device='cuda:0',\n","       grad_fn=<FusedDropoutBackward0>)\n","hidden :  torch.Size([1, 512])\n","hidden_unsqueeze:  torch.Size([1, 17, 512])\n","encoder_outputs :  torch.Size([17, 1, 1024])\n","encoder_outputs_permute :  torch.Size([1, 17, 1024])\n","energy\n","shape :  torch.Size([1, 17, 512])\n","tensor([[[-0.0207,  0.0241, -0.0081,  ...,  0.0229,  0.0013,  0.0229],\n","         [-0.0218,  0.0278, -0.0100,  ...,  0.0257,  0.0028,  0.0249],\n","         [-0.0214,  0.0282, -0.0114,  ...,  0.0270,  0.0053,  0.0263],\n","         ...,\n","         [-0.0208,  0.0269, -0.0075,  ...,  0.0224,  0.0046,  0.0234],\n","         [-0.0204,  0.0271, -0.0084,  ...,  0.0221,  0.0052,  0.0230],\n","         [-0.0192,  0.0247, -0.0065,  ...,  0.0192,  0.0044,  0.0206]]],\n","       device='cuda:0', grad_fn=<TanhBackward0>)\n","attention\n","shape:  torch.Size([1, 17])\n","tensor([[0.0579, 0.0585, 0.0589, 0.0592, 0.0592, 0.0594, 0.0594, 0.0595, 0.0591,\n","         0.0589, 0.0586, 0.0590, 0.0587, 0.0587, 0.0583, 0.0585, 0.0581]],\n","       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n","a\n","shape :  torch.Size([1, 17])\n","tensor([[0.0579, 0.0585, 0.0589, 0.0592, 0.0592, 0.0594, 0.0594, 0.0595, 0.0591,\n","         0.0589, 0.0586, 0.0590, 0.0587, 0.0587, 0.0583, 0.0585, 0.0581]],\n","       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n","a_unsqueeze\n","shape :  torch.Size([1, 1, 17])\n","tensor([[[0.0579, 0.0585, 0.0589, 0.0592, 0.0592, 0.0594, 0.0594, 0.0595,\n","          0.0591, 0.0589, 0.0586, 0.0590, 0.0587, 0.0587, 0.0583, 0.0585,\n","          0.0581]]], device='cuda:0', grad_fn=<UnsqueezeBackward0>)\n","weighted\n","shape :  torch.Size([1, 1, 1024])\n","tensor([[[ 0.0015,  0.0127, -0.0103,  ..., -0.0120, -0.0075,  0.0108]]],\n","       device='cuda:0', grad_fn=<BmmBackward0>)\n","output\n","shape :  torch.Size([1, 1, 512])\n","tensor([[[ 0.0364, -0.0573,  0.0469,  0.0195, -0.0452, -0.0422,  0.0488,\n","          -0.0318,  0.0462, -0.0083, -0.0497, -0.0123, -0.0429,  0.0394,\n","           0.0400, -0.0524, -0.0276, -0.0451,  0.0399,  0.0069,  0.0495,\n","          -0.0520, -0.0397, -0.0485, -0.0333, -0.0368,  0.0260, -0.0373,\n","          -0.0415,  0.0445,  0.0491,  0.0431,  0.0337, -0.0242, -0.0441,\n","          -0.0304,  0.0359,  0.0468, -0.0468,  0.0564, -0.0421,  0.0402,\n","           0.0391,  0.0341, -0.0097, -0.0374, -0.0504,  0.0325, -0.0279,\n","          -0.0184, -0.0376,  0.0446,  0.0407,  0.0364,  0.0481, -0.0483,\n","          -0.0554,  0.0416, -0.0034, -0.0144, -0.0419, -0.0493, -0.0514,\n","          -0.0527,  0.0437, -0.0239, -0.0380, -0.0182,  0.0455, -0.0424,\n","           0.0417, -0.0315, -0.0420,  0.0385,  0.0466, -0.0221,  0.0031,\n","          -0.0370, -0.0423, -0.0372, -0.0358, -0.0227,  0.0443, -0.0215,\n","          -0.0488, -0.0463, -0.0213, -0.0479,  0.0485, -0.0347,  0.0129,\n","           0.0416,  0.0188, -0.0424, -0.0466, -0.0416,  0.0370, -0.0512,\n","          -0.0436, -0.0042,  0.0381,  0.0560, -0.0299, -0.0482,  0.0335,\n","           0.0402,  0.0312, -0.0484,  0.0393,  0.0439, -0.0489,  0.0491,\n","           0.0437, -0.0403, -0.0114, -0.0225, -0.0249,  0.0206,  0.0474,\n","           0.0423, -0.0454,  0.0496, -0.0032,  0.0380,  0.0148, -0.0532,\n","          -0.0426,  0.0471,  0.0505,  0.0437, -0.0183, -0.0321, -0.0480,\n","           0.0435, -0.0511, -0.0429, -0.0499,  0.0498, -0.0388,  0.0432,\n","          -0.0234, -0.0483,  0.0397, -0.0146,  0.0428,  0.0377,  0.0388,\n","          -0.0136,  0.0519,  0.0489, -0.0476, -0.0211, -0.0410, -0.0457,\n","          -0.0215,  0.0441, -0.0454,  0.0378,  0.0397, -0.0396, -0.0387,\n","           0.0459, -0.0144,  0.0461,  0.0485,  0.0469, -0.0529,  0.0401,\n","          -0.0439,  0.0500, -0.0280,  0.0489, -0.0205,  0.0096, -0.0391,\n","          -0.0225,  0.0394, -0.0494, -0.0508, -0.0384, -0.0555, -0.0226,\n","          -0.0450,  0.0113,  0.0454,  0.0460,  0.0476,  0.0409, -0.0511,\n","           0.0434,  0.0317, -0.0235, -0.0441,  0.0442,  0.0478,  0.0430,\n","          -0.0439,  0.0397, -0.0429,  0.0362,  0.0555,  0.0443,  0.0160,\n","          -0.0474,  0.0428,  0.0419, -0.0438,  0.0435,  0.0142,  0.0448,\n","          -0.0487,  0.0439, -0.0509, -0.0477,  0.0037, -0.0486,  0.0428,\n","           0.0415,  0.0529, -0.0412,  0.0478,  0.0062, -0.0452, -0.0514,\n","          -0.0447, -0.0424, -0.0452, -0.0446, -0.0195,  0.0476,  0.0430,\n","          -0.0218,  0.0455,  0.0387, -0.0555, -0.0039,  0.0533,  0.0227,\n","           0.0517,  0.0420,  0.0464, -0.0457,  0.0443, -0.0380,  0.0503,\n","           0.0446, -0.0505, -0.0461,  0.0359,  0.0451,  0.0250, -0.0443,\n","           0.0198,  0.0498, -0.0380,  0.0501,  0.0452, -0.0268, -0.0508,\n","           0.0314, -0.0440,  0.0202,  0.0423, -0.0515, -0.0483,  0.0206,\n","           0.0499,  0.0475,  0.0219, -0.0458, -0.0492,  0.0091, -0.0129,\n","          -0.0473, -0.0488, -0.0474, -0.0518,  0.0523, -0.0286, -0.0068,\n","          -0.0424,  0.0417,  0.0422, -0.0254,  0.0467,  0.0401,  0.0343,\n","          -0.0478,  0.0316,  0.0480,  0.0400,  0.0362,  0.0493,  0.0381,\n","           0.0304, -0.0497,  0.0501,  0.0134,  0.0216, -0.0253,  0.0334,\n","           0.0297, -0.0306, -0.0426,  0.0394,  0.0543,  0.0396,  0.0343,\n","          -0.0264,  0.0478,  0.0405, -0.0545,  0.0449, -0.0394,  0.0467,\n","          -0.0466, -0.0519, -0.0318, -0.0506, -0.0478,  0.0557, -0.0458,\n","          -0.0391, -0.0519, -0.0488,  0.0312, -0.0045,  0.0483,  0.0442,\n","           0.0453,  0.0492, -0.0378,  0.0389, -0.0480, -0.0344,  0.0362,\n","           0.0510,  0.0365,  0.0595, -0.0507,  0.0500, -0.0387, -0.0399,\n","           0.0206, -0.0006,  0.0497,  0.0490,  0.0395, -0.0376, -0.0482,\n","           0.0500,  0.0472,  0.0462, -0.0237,  0.0519, -0.0168,  0.0457,\n","          -0.0529, -0.0458, -0.0442,  0.0450,  0.0496, -0.0084,  0.0271,\n","          -0.0454, -0.0440,  0.0382,  0.0392, -0.0511, -0.0495,  0.0457,\n","          -0.0503, -0.0581,  0.0584,  0.0397,  0.0448, -0.0418,  0.0305,\n","          -0.0511,  0.0437, -0.0425,  0.0178, -0.0060, -0.0239,  0.0482,\n","           0.0448,  0.0493, -0.0401, -0.0567,  0.0460, -0.0324,  0.0397,\n","          -0.0411,  0.0325, -0.0349,  0.0460,  0.0465,  0.0394, -0.0379,\n","           0.0492,  0.0461, -0.0436, -0.0518,  0.0364,  0.0390, -0.0515,\n","           0.0410, -0.0420,  0.0383, -0.0472,  0.0236, -0.0003,  0.0217,\n","           0.0432, -0.0454,  0.0453, -0.0316, -0.0381, -0.0519,  0.0499,\n","          -0.0151, -0.0449,  0.0052, -0.0541,  0.0464, -0.0018, -0.0221,\n","          -0.0285, -0.0456, -0.0481,  0.0203, -0.0524,  0.0284,  0.0507,\n","          -0.0462,  0.0417, -0.0402,  0.0487, -0.0385, -0.0494, -0.0507,\n","           0.0337, -0.0529,  0.0380,  0.0374,  0.0506, -0.0488,  0.0453,\n","           0.0510, -0.0351, -0.0510,  0.0528,  0.0457,  0.0043, -0.0450,\n","          -0.0438,  0.0533, -0.0486, -0.0490,  0.0423,  0.0407, -0.0428,\n","           0.0518,  0.0383, -0.0478, -0.0499,  0.0186,  0.0356,  0.0500,\n","           0.0396,  0.0495,  0.0557, -0.0488,  0.0456,  0.0519, -0.0171,\n","           0.0482,  0.0444, -0.0236, -0.0410,  0.0350,  0.0229, -0.0407,\n","          -0.0036, -0.0392, -0.0321, -0.0237,  0.0482, -0.0257, -0.0379,\n","          -0.0307,  0.0084,  0.0522, -0.0473,  0.0524,  0.0286,  0.0382,\n","          -0.0185,  0.0348,  0.0115,  0.0183,  0.0087, -0.0409,  0.0423,\n","           0.0461, -0.0429,  0.0497,  0.0507, -0.0403, -0.0323, -0.0321,\n","           0.0391]]], device='cuda:0', grad_fn=<CudnnRnnBackward0>)\n","hidden\n","shape :  torch.Size([1, 1, 512])\n","tensor([[[ 0.0364, -0.0573,  0.0469,  0.0195, -0.0452, -0.0422,  0.0488,\n","          -0.0318,  0.0462, -0.0083, -0.0497, -0.0123, -0.0429,  0.0394,\n","           0.0400, -0.0524, -0.0276, -0.0451,  0.0399,  0.0069,  0.0495,\n","          -0.0520, -0.0397, -0.0485, -0.0333, -0.0368,  0.0260, -0.0373,\n","          -0.0415,  0.0445,  0.0491,  0.0431,  0.0337, -0.0242, -0.0441,\n","          -0.0304,  0.0359,  0.0468, -0.0468,  0.0564, -0.0421,  0.0402,\n","           0.0391,  0.0341, -0.0097, -0.0374, -0.0504,  0.0325, -0.0279,\n","          -0.0184, -0.0376,  0.0446,  0.0407,  0.0364,  0.0481, -0.0483,\n","          -0.0554,  0.0416, -0.0034, -0.0144, -0.0419, -0.0493, -0.0514,\n","          -0.0527,  0.0437, -0.0239, -0.0380, -0.0182,  0.0455, -0.0424,\n","           0.0417, -0.0315, -0.0420,  0.0385,  0.0466, -0.0221,  0.0031,\n","          -0.0370, -0.0423, -0.0372, -0.0358, -0.0227,  0.0443, -0.0215,\n","          -0.0488, -0.0463, -0.0213, -0.0479,  0.0485, -0.0347,  0.0129,\n","           0.0416,  0.0188, -0.0424, -0.0466, -0.0416,  0.0370, -0.0512,\n","          -0.0436, -0.0042,  0.0381,  0.0560, -0.0299, -0.0482,  0.0335,\n","           0.0402,  0.0312, -0.0484,  0.0393,  0.0439, -0.0489,  0.0491,\n","           0.0437, -0.0403, -0.0114, -0.0225, -0.0249,  0.0206,  0.0474,\n","           0.0423, -0.0454,  0.0496, -0.0032,  0.0380,  0.0148, -0.0532,\n","          -0.0426,  0.0471,  0.0505,  0.0437, -0.0183, -0.0321, -0.0480,\n","           0.0435, -0.0511, -0.0429, -0.0499,  0.0498, -0.0388,  0.0432,\n","          -0.0234, -0.0483,  0.0397, -0.0146,  0.0428,  0.0377,  0.0388,\n","          -0.0136,  0.0519,  0.0489, -0.0476, -0.0211, -0.0410, -0.0457,\n","          -0.0215,  0.0441, -0.0454,  0.0378,  0.0397, -0.0396, -0.0387,\n","           0.0459, -0.0144,  0.0461,  0.0485,  0.0469, -0.0529,  0.0401,\n","          -0.0439,  0.0500, -0.0280,  0.0489, -0.0205,  0.0096, -0.0391,\n","          -0.0225,  0.0394, -0.0494, -0.0508, -0.0384, -0.0555, -0.0226,\n","          -0.0450,  0.0113,  0.0454,  0.0460,  0.0476,  0.0409, -0.0511,\n","           0.0434,  0.0317, -0.0235, -0.0441,  0.0442,  0.0478,  0.0430,\n","          -0.0439,  0.0397, -0.0429,  0.0362,  0.0555,  0.0443,  0.0160,\n","          -0.0474,  0.0428,  0.0419, -0.0438,  0.0435,  0.0142,  0.0448,\n","          -0.0487,  0.0439, -0.0509, -0.0477,  0.0037, -0.0486,  0.0428,\n","           0.0415,  0.0529, -0.0412,  0.0478,  0.0062, -0.0452, -0.0514,\n","          -0.0447, -0.0424, -0.0452, -0.0446, -0.0195,  0.0476,  0.0430,\n","          -0.0218,  0.0455,  0.0387, -0.0555, -0.0039,  0.0533,  0.0227,\n","           0.0517,  0.0420,  0.0464, -0.0457,  0.0443, -0.0380,  0.0503,\n","           0.0446, -0.0505, -0.0461,  0.0359,  0.0451,  0.0250, -0.0443,\n","           0.0198,  0.0498, -0.0380,  0.0501,  0.0452, -0.0268, -0.0508,\n","           0.0314, -0.0440,  0.0202,  0.0423, -0.0515, -0.0483,  0.0206,\n","           0.0499,  0.0475,  0.0219, -0.0458, -0.0492,  0.0091, -0.0129,\n","          -0.0473, -0.0488, -0.0474, -0.0518,  0.0523, -0.0286, -0.0068,\n","          -0.0424,  0.0417,  0.0422, -0.0254,  0.0467,  0.0401,  0.0343,\n","          -0.0478,  0.0316,  0.0480,  0.0400,  0.0362,  0.0493,  0.0381,\n","           0.0304, -0.0497,  0.0501,  0.0134,  0.0216, -0.0253,  0.0334,\n","           0.0297, -0.0306, -0.0426,  0.0394,  0.0543,  0.0396,  0.0343,\n","          -0.0264,  0.0478,  0.0405, -0.0545,  0.0449, -0.0394,  0.0467,\n","          -0.0466, -0.0519, -0.0318, -0.0506, -0.0478,  0.0557, -0.0458,\n","          -0.0391, -0.0519, -0.0488,  0.0312, -0.0045,  0.0483,  0.0442,\n","           0.0453,  0.0492, -0.0378,  0.0389, -0.0480, -0.0344,  0.0362,\n","           0.0510,  0.0365,  0.0595, -0.0507,  0.0500, -0.0387, -0.0399,\n","           0.0206, -0.0006,  0.0497,  0.0490,  0.0395, -0.0376, -0.0482,\n","           0.0500,  0.0472,  0.0462, -0.0237,  0.0519, -0.0168,  0.0457,\n","          -0.0529, -0.0458, -0.0442,  0.0450,  0.0496, -0.0084,  0.0271,\n","          -0.0454, -0.0440,  0.0382,  0.0392, -0.0511, -0.0495,  0.0457,\n","          -0.0503, -0.0581,  0.0584,  0.0397,  0.0448, -0.0418,  0.0305,\n","          -0.0511,  0.0437, -0.0425,  0.0178, -0.0060, -0.0239,  0.0482,\n","           0.0448,  0.0493, -0.0401, -0.0567,  0.0460, -0.0324,  0.0397,\n","          -0.0411,  0.0325, -0.0349,  0.0460,  0.0465,  0.0394, -0.0379,\n","           0.0492,  0.0461, -0.0436, -0.0518,  0.0364,  0.0390, -0.0515,\n","           0.0410, -0.0420,  0.0383, -0.0472,  0.0236, -0.0003,  0.0217,\n","           0.0432, -0.0454,  0.0453, -0.0316, -0.0381, -0.0519,  0.0499,\n","          -0.0151, -0.0449,  0.0052, -0.0541,  0.0464, -0.0018, -0.0221,\n","          -0.0285, -0.0456, -0.0481,  0.0203, -0.0524,  0.0284,  0.0507,\n","          -0.0462,  0.0417, -0.0402,  0.0487, -0.0385, -0.0494, -0.0507,\n","           0.0337, -0.0529,  0.0380,  0.0374,  0.0506, -0.0488,  0.0453,\n","           0.0510, -0.0351, -0.0510,  0.0528,  0.0457,  0.0043, -0.0450,\n","          -0.0438,  0.0533, -0.0486, -0.0490,  0.0423,  0.0407, -0.0428,\n","           0.0518,  0.0383, -0.0478, -0.0499,  0.0186,  0.0356,  0.0500,\n","           0.0396,  0.0495,  0.0557, -0.0488,  0.0456,  0.0519, -0.0171,\n","           0.0482,  0.0444, -0.0236, -0.0410,  0.0350,  0.0229, -0.0407,\n","          -0.0036, -0.0392, -0.0321, -0.0237,  0.0482, -0.0257, -0.0379,\n","          -0.0307,  0.0084,  0.0522, -0.0473,  0.0524,  0.0286,  0.0382,\n","          -0.0185,  0.0348,  0.0115,  0.0183,  0.0087, -0.0409,  0.0423,\n","           0.0461, -0.0429,  0.0497,  0.0507, -0.0403, -0.0323, -0.0321,\n","           0.0391]]], device='cuda:0', grad_fn=<CudnnRnnBackward0>)\n","prediction\n","shape :  torch.Size([1, 5893])\n","tensor([[ 0.0872, -0.0787, -0.0767,  ..., -0.0833, -0.0809, -0.0763]],\n","       device='cuda:0', grad_fn=<AddmmBackward0>)\n","input\n","shape :  torch.Size([1, 1])\n","tensor([[972]], device='cuda:0')\n","embedded\n","shape :  torch.Size([1, 1, 256])\n","tensor([[[-0.0215,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","           0.0045, -0.0096,  0.0033,  0.0000,  0.0000,  0.0000,  0.0181,\n","          -0.0001,  0.0000,  0.0000,  0.0005,  0.0000,  0.0066, -0.0373,\n","           0.0190,  0.0000,  0.0000, -0.0026,  0.0000, -0.0147, -0.0792,\n","           0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0253, -0.0132,\n","           0.0014, -0.0165, -0.0057, -0.0310, -0.0097,  0.0303,  0.0000,\n","           0.0284,  0.0000,  0.0000, -0.0469,  0.0000,  0.0000, -0.0027,\n","          -0.0028,  0.0000, -0.0156,  0.0000,  0.0000, -0.0183,  0.0000,\n","           0.0261,  0.0193, -0.0113,  0.0000,  0.0000,  0.0000,  0.0000,\n","          -0.0094,  0.0097, -0.0090,  0.0000,  0.0000,  0.0137,  0.0017,\n","           0.0101,  0.0000,  0.0000,  0.0000, -0.0010,  0.0000,  0.0000,\n","           0.0000, -0.0061,  0.0000,  0.0175,  0.0041,  0.0176,  0.0000,\n","           0.0000,  0.0071, -0.0086,  0.0000,  0.0115, -0.0259,  0.0000,\n","           0.0000,  0.0000,  0.0154, -0.0249,  0.0301, -0.0044, -0.0127,\n","          -0.0070, -0.0098,  0.0243,  0.0000, -0.0299,  0.0000, -0.0074,\n","           0.0000, -0.0152,  0.0000,  0.0290,  0.0000, -0.0049,  0.0000,\n","           0.0000,  0.0000,  0.0000,  0.0245, -0.0124,  0.0000,  0.0090,\n","           0.0202,  0.0021,  0.0340,  0.0000,  0.0000,  0.0000,  0.0120,\n","           0.0000,  0.0049,  0.0000,  0.0144,  0.0128,  0.0000,  0.0000,\n","           0.0000, -0.0156,  0.0104,  0.0000,  0.0000,  0.0000,  0.0000,\n","           0.0000,  0.0000,  0.0000,  0.0000, -0.0173,  0.0560,  0.0530,\n","           0.0000,  0.0000, -0.0074,  0.0069,  0.0000,  0.0000,  0.0000,\n","           0.0177, -0.0096,  0.0000, -0.0195,  0.0216, -0.0246,  0.0062,\n","           0.0000,  0.0000,  0.0000,  0.0063,  0.0422,  0.0231, -0.0037,\n","           0.0000,  0.0154,  0.0000, -0.0349,  0.0169,  0.0000,  0.0002,\n","           0.0000,  0.0000,  0.0000,  0.0265,  0.0000,  0.0090,  0.0000,\n","           0.0143,  0.0000,  0.0001,  0.0000, -0.0227, -0.0091,  0.0223,\n","          -0.0091, -0.0244, -0.0192,  0.0249,  0.0000, -0.0132,  0.0000,\n","          -0.0052,  0.0329,  0.0000,  0.0000,  0.0408, -0.0178,  0.0140,\n","           0.0000,  0.0000, -0.0064,  0.0000,  0.0013,  0.0000, -0.0212,\n","          -0.0222, -0.0064, -0.0164,  0.0000,  0.0000, -0.0171,  0.0000,\n","          -0.0138,  0.0000,  0.0000,  0.0000, -0.0158, -0.0150, -0.0244,\n","           0.0000,  0.0000, -0.0118,  0.0000,  0.0212, -0.0193,  0.0010,\n","          -0.0116,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0063,\n","           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","           0.0000,  0.0000,  0.0000,  0.0000,  0.0077, -0.0121,  0.0000,\n","           0.0000,  0.0000,  0.0118,  0.0000]]], device='cuda:0',\n","       grad_fn=<FusedDropoutBackward0>)\n","hidden :  torch.Size([1, 512])\n","hidden_unsqueeze:  torch.Size([1, 17, 512])\n","encoder_outputs :  torch.Size([17, 1, 1024])\n","encoder_outputs_permute :  torch.Size([1, 17, 1024])\n","energy\n","shape :  torch.Size([1, 17, 512])\n","tensor([[[-0.0210,  0.0268, -0.0074,  ...,  0.0251, -0.0002,  0.0217],\n","         [-0.0222,  0.0304, -0.0092,  ...,  0.0279,  0.0012,  0.0238],\n","         [-0.0218,  0.0308, -0.0107,  ...,  0.0293,  0.0037,  0.0252],\n","         ...,\n","         [-0.0211,  0.0295, -0.0068,  ...,  0.0246,  0.0030,  0.0223],\n","         [-0.0207,  0.0297, -0.0077,  ...,  0.0244,  0.0037,  0.0219],\n","         [-0.0196,  0.0273, -0.0058,  ...,  0.0215,  0.0029,  0.0195]]],\n","       device='cuda:0', grad_fn=<TanhBackward0>)\n","attention\n","shape:  torch.Size([1, 17])\n","tensor([[0.0579, 0.0585, 0.0589, 0.0592, 0.0592, 0.0594, 0.0594, 0.0595, 0.0591,\n","         0.0589, 0.0586, 0.0590, 0.0587, 0.0587, 0.0583, 0.0585, 0.0581]],\n","       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n","a\n","shape :  torch.Size([1, 17])\n","tensor([[0.0579, 0.0585, 0.0589, 0.0592, 0.0592, 0.0594, 0.0594, 0.0595, 0.0591,\n","         0.0589, 0.0586, 0.0590, 0.0587, 0.0587, 0.0583, 0.0585, 0.0581]],\n","       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n","a_unsqueeze\n","shape :  torch.Size([1, 1, 17])\n","tensor([[[0.0579, 0.0585, 0.0589, 0.0592, 0.0592, 0.0594, 0.0594, 0.0595,\n","          0.0591, 0.0589, 0.0586, 0.0590, 0.0587, 0.0587, 0.0583, 0.0585,\n","          0.0581]]], device='cuda:0', grad_fn=<UnsqueezeBackward0>)\n","weighted\n","shape :  torch.Size([1, 1, 1024])\n","tensor([[[ 0.0015,  0.0127, -0.0103,  ..., -0.0120, -0.0075,  0.0108]]],\n","       device='cuda:0', grad_fn=<BmmBackward0>)\n","output\n","shape :  torch.Size([1, 1, 512])\n","tensor([[[ 0.0448, -0.0630,  0.0488,  0.0178, -0.0458, -0.0475,  0.0528,\n","          -0.0318,  0.0500, -0.0068, -0.0541, -0.0090, -0.0418,  0.0409,\n","           0.0421, -0.0554, -0.0281, -0.0468,  0.0405,  0.0056,  0.0537,\n","          -0.0546, -0.0430, -0.0528, -0.0414, -0.0379,  0.0290, -0.0376,\n","          -0.0439,  0.0476,  0.0515,  0.0459,  0.0381, -0.0301, -0.0511,\n","          -0.0360,  0.0399,  0.0511, -0.0504,  0.0599, -0.0469,  0.0443,\n","           0.0410,  0.0361, -0.0164, -0.0359, -0.0555,  0.0387, -0.0324,\n","          -0.0263, -0.0439,  0.0490,  0.0420,  0.0381,  0.0513, -0.0503,\n","          -0.0581,  0.0461, -0.0022, -0.0189, -0.0410, -0.0501, -0.0542,\n","          -0.0553,  0.0475, -0.0297, -0.0425, -0.0179,  0.0497, -0.0461,\n","           0.0495, -0.0362, -0.0434,  0.0430,  0.0508, -0.0213,  0.0037,\n","          -0.0418, -0.0434, -0.0386, -0.0365, -0.0236,  0.0475, -0.0212,\n","          -0.0537, -0.0498, -0.0240, -0.0506,  0.0558, -0.0325,  0.0143,\n","           0.0431,  0.0153, -0.0464, -0.0516, -0.0463,  0.0387, -0.0543,\n","          -0.0487, -0.0066,  0.0396,  0.0583, -0.0368, -0.0517,  0.0400,\n","           0.0395,  0.0316, -0.0537,  0.0437,  0.0459, -0.0493,  0.0521,\n","           0.0486, -0.0452, -0.0117, -0.0204, -0.0238,  0.0169,  0.0467,\n","           0.0463, -0.0500,  0.0529,  0.0014,  0.0401,  0.0192, -0.0579,\n","          -0.0454,  0.0505,  0.0543,  0.0459, -0.0243, -0.0401, -0.0529,\n","           0.0444, -0.0547, -0.0484, -0.0531,  0.0524, -0.0391,  0.0436,\n","          -0.0298, -0.0526,  0.0431, -0.0125,  0.0474,  0.0440,  0.0418,\n","          -0.0170,  0.0561,  0.0514, -0.0528, -0.0263, -0.0462, -0.0467,\n","          -0.0243,  0.0478, -0.0522,  0.0379,  0.0408, -0.0407, -0.0416,\n","           0.0495, -0.0111,  0.0489,  0.0527,  0.0506, -0.0556,  0.0453,\n","          -0.0515,  0.0539, -0.0275,  0.0510, -0.0238,  0.0143, -0.0423,\n","          -0.0270,  0.0443, -0.0547, -0.0573, -0.0423, -0.0600, -0.0211,\n","          -0.0471,  0.0079,  0.0444,  0.0500,  0.0538,  0.0423, -0.0561,\n","           0.0444,  0.0388, -0.0231, -0.0447,  0.0514,  0.0524,  0.0464,\n","          -0.0488,  0.0411, -0.0465,  0.0419,  0.0573,  0.0487,  0.0117,\n","          -0.0536,  0.0423,  0.0462, -0.0470,  0.0472,  0.0120,  0.0466,\n","          -0.0489,  0.0501, -0.0551, -0.0535,  0.0032, -0.0521,  0.0463,\n","           0.0427,  0.0591, -0.0448,  0.0498,  0.0034, -0.0458, -0.0538,\n","          -0.0516, -0.0490, -0.0489, -0.0489, -0.0185,  0.0508,  0.0437,\n","          -0.0188,  0.0472,  0.0445, -0.0623, -0.0043,  0.0567,  0.0215,\n","           0.0541,  0.0436,  0.0489, -0.0506,  0.0468, -0.0407,  0.0544,\n","           0.0499, -0.0561, -0.0495,  0.0375,  0.0499,  0.0291, -0.0433,\n","           0.0200,  0.0535, -0.0435,  0.0511,  0.0493, -0.0306, -0.0547,\n","           0.0392, -0.0486,  0.0213,  0.0450, -0.0587, -0.0498,  0.0263,\n","           0.0532,  0.0510,  0.0262, -0.0501, -0.0539,  0.0063, -0.0104,\n","          -0.0499, -0.0534, -0.0523, -0.0540,  0.0538, -0.0295, -0.0086,\n","          -0.0471,  0.0419,  0.0464, -0.0308,  0.0514,  0.0453,  0.0367,\n","          -0.0509,  0.0364,  0.0488,  0.0438,  0.0382,  0.0532,  0.0460,\n","           0.0365, -0.0532,  0.0538,  0.0169,  0.0246, -0.0273,  0.0403,\n","           0.0323, -0.0351, -0.0506,  0.0440,  0.0545,  0.0424,  0.0404,\n","          -0.0325,  0.0504,  0.0427, -0.0562,  0.0491, -0.0474,  0.0533,\n","          -0.0491, -0.0555, -0.0299, -0.0538, -0.0516,  0.0593, -0.0509,\n","          -0.0443, -0.0540, -0.0512,  0.0318, -0.0034,  0.0562,  0.0461,\n","           0.0485,  0.0533, -0.0390,  0.0421, -0.0521, -0.0350,  0.0417,\n","           0.0520,  0.0394,  0.0643, -0.0568,  0.0539, -0.0412, -0.0423,\n","           0.0221, -0.0050,  0.0567,  0.0545,  0.0434, -0.0423, -0.0520,\n","           0.0512,  0.0473,  0.0514, -0.0229,  0.0547, -0.0157,  0.0528,\n","          -0.0583, -0.0465, -0.0459,  0.0525,  0.0527, -0.0064,  0.0306,\n","          -0.0480, -0.0455,  0.0454,  0.0417, -0.0523, -0.0543,  0.0490,\n","          -0.0508, -0.0623,  0.0637,  0.0424,  0.0458, -0.0444,  0.0368,\n","          -0.0542,  0.0459, -0.0422,  0.0161, -0.0042, -0.0246,  0.0490,\n","           0.0466,  0.0516, -0.0448, -0.0609,  0.0479, -0.0332,  0.0395,\n","          -0.0416,  0.0346, -0.0337,  0.0496,  0.0507,  0.0426, -0.0419,\n","           0.0532,  0.0484, -0.0463, -0.0560,  0.0422,  0.0391, -0.0588,\n","           0.0473, -0.0455,  0.0427, -0.0511,  0.0202,  0.0014,  0.0292,\n","           0.0441, -0.0485,  0.0458, -0.0373, -0.0408, -0.0525,  0.0526,\n","          -0.0152, -0.0441,  0.0021, -0.0599,  0.0497, -0.0011, -0.0210,\n","          -0.0307, -0.0476, -0.0514,  0.0195, -0.0549,  0.0312,  0.0541,\n","          -0.0494,  0.0460, -0.0435,  0.0513, -0.0422, -0.0534, -0.0540,\n","           0.0399, -0.0567,  0.0406,  0.0440,  0.0550, -0.0505,  0.0504,\n","           0.0558, -0.0394, -0.0533,  0.0567,  0.0466,  0.0073, -0.0463,\n","          -0.0434,  0.0574, -0.0503, -0.0505,  0.0449,  0.0437, -0.0454,\n","           0.0550,  0.0387, -0.0528, -0.0513,  0.0242,  0.0404,  0.0562,\n","           0.0505,  0.0539,  0.0553, -0.0507,  0.0501,  0.0542, -0.0237,\n","           0.0508,  0.0494, -0.0237, -0.0423,  0.0375,  0.0242, -0.0454,\n","          -0.0055, -0.0404, -0.0372, -0.0201,  0.0504, -0.0314, -0.0436,\n","          -0.0386,  0.0071,  0.0561, -0.0504,  0.0584,  0.0280,  0.0409,\n","          -0.0182,  0.0415,  0.0155,  0.0190,  0.0111, -0.0445,  0.0458,\n","           0.0503, -0.0450,  0.0521,  0.0526, -0.0464, -0.0405, -0.0320,\n","           0.0423]]], device='cuda:0', grad_fn=<CudnnRnnBackward0>)\n","hidden\n","shape :  torch.Size([1, 1, 512])\n","tensor([[[ 0.0448, -0.0630,  0.0488,  0.0178, -0.0458, -0.0475,  0.0528,\n","          -0.0318,  0.0500, -0.0068, -0.0541, -0.0090, -0.0418,  0.0409,\n","           0.0421, -0.0554, -0.0281, -0.0468,  0.0405,  0.0056,  0.0537,\n","          -0.0546, -0.0430, -0.0528, -0.0414, -0.0379,  0.0290, -0.0376,\n","          -0.0439,  0.0476,  0.0515,  0.0459,  0.0381, -0.0301, -0.0511,\n","          -0.0360,  0.0399,  0.0511, -0.0504,  0.0599, -0.0469,  0.0443,\n","           0.0410,  0.0361, -0.0164, -0.0359, -0.0555,  0.0387, -0.0324,\n","          -0.0263, -0.0439,  0.0490,  0.0420,  0.0381,  0.0513, -0.0503,\n","          -0.0581,  0.0461, -0.0022, -0.0189, -0.0410, -0.0501, -0.0542,\n","          -0.0553,  0.0475, -0.0297, -0.0425, -0.0179,  0.0497, -0.0461,\n","           0.0495, -0.0362, -0.0434,  0.0430,  0.0508, -0.0213,  0.0037,\n","          -0.0418, -0.0434, -0.0386, -0.0365, -0.0236,  0.0475, -0.0212,\n","          -0.0537, -0.0498, -0.0240, -0.0506,  0.0558, -0.0325,  0.0143,\n","           0.0431,  0.0153, -0.0464, -0.0516, -0.0463,  0.0387, -0.0543,\n","          -0.0487, -0.0066,  0.0396,  0.0583, -0.0368, -0.0517,  0.0400,\n","           0.0395,  0.0316, -0.0537,  0.0437,  0.0459, -0.0493,  0.0521,\n","           0.0486, -0.0452, -0.0117, -0.0204, -0.0238,  0.0169,  0.0467,\n","           0.0463, -0.0500,  0.0529,  0.0014,  0.0401,  0.0192, -0.0579,\n","          -0.0454,  0.0505,  0.0543,  0.0459, -0.0243, -0.0401, -0.0529,\n","           0.0444, -0.0547, -0.0484, -0.0531,  0.0524, -0.0391,  0.0436,\n","          -0.0298, -0.0526,  0.0431, -0.0125,  0.0474,  0.0440,  0.0418,\n","          -0.0170,  0.0561,  0.0514, -0.0528, -0.0263, -0.0462, -0.0467,\n","          -0.0243,  0.0478, -0.0522,  0.0379,  0.0408, -0.0407, -0.0416,\n","           0.0495, -0.0111,  0.0489,  0.0527,  0.0506, -0.0556,  0.0453,\n","          -0.0515,  0.0539, -0.0275,  0.0510, -0.0238,  0.0143, -0.0423,\n","          -0.0270,  0.0443, -0.0547, -0.0573, -0.0423, -0.0600, -0.0211,\n","          -0.0471,  0.0079,  0.0444,  0.0500,  0.0538,  0.0423, -0.0561,\n","           0.0444,  0.0388, -0.0231, -0.0447,  0.0514,  0.0524,  0.0464,\n","          -0.0488,  0.0411, -0.0465,  0.0419,  0.0573,  0.0487,  0.0117,\n","          -0.0536,  0.0423,  0.0462, -0.0470,  0.0472,  0.0120,  0.0466,\n","          -0.0489,  0.0501, -0.0551, -0.0535,  0.0032, -0.0521,  0.0463,\n","           0.0427,  0.0591, -0.0448,  0.0498,  0.0034, -0.0458, -0.0538,\n","          -0.0516, -0.0490, -0.0489, -0.0489, -0.0185,  0.0508,  0.0437,\n","          -0.0188,  0.0472,  0.0445, -0.0623, -0.0043,  0.0567,  0.0215,\n","           0.0541,  0.0436,  0.0489, -0.0506,  0.0468, -0.0407,  0.0544,\n","           0.0499, -0.0561, -0.0495,  0.0375,  0.0499,  0.0291, -0.0433,\n","           0.0200,  0.0535, -0.0435,  0.0511,  0.0493, -0.0306, -0.0547,\n","           0.0392, -0.0486,  0.0213,  0.0450, -0.0587, -0.0498,  0.0263,\n","           0.0532,  0.0510,  0.0262, -0.0501, -0.0539,  0.0063, -0.0104,\n","          -0.0499, -0.0534, -0.0523, -0.0540,  0.0538, -0.0295, -0.0086,\n","          -0.0471,  0.0419,  0.0464, -0.0308,  0.0514,  0.0453,  0.0367,\n","          -0.0509,  0.0364,  0.0488,  0.0438,  0.0382,  0.0532,  0.0460,\n","           0.0365, -0.0532,  0.0538,  0.0169,  0.0246, -0.0273,  0.0403,\n","           0.0323, -0.0351, -0.0506,  0.0440,  0.0545,  0.0424,  0.0404,\n","          -0.0325,  0.0504,  0.0427, -0.0562,  0.0491, -0.0474,  0.0533,\n","          -0.0491, -0.0555, -0.0299, -0.0538, -0.0516,  0.0593, -0.0509,\n","          -0.0443, -0.0540, -0.0512,  0.0318, -0.0034,  0.0562,  0.0461,\n","           0.0485,  0.0533, -0.0390,  0.0421, -0.0521, -0.0350,  0.0417,\n","           0.0520,  0.0394,  0.0643, -0.0568,  0.0539, -0.0412, -0.0423,\n","           0.0221, -0.0050,  0.0567,  0.0545,  0.0434, -0.0423, -0.0520,\n","           0.0512,  0.0473,  0.0514, -0.0229,  0.0547, -0.0157,  0.0528,\n","          -0.0583, -0.0465, -0.0459,  0.0525,  0.0527, -0.0064,  0.0306,\n","          -0.0480, -0.0455,  0.0454,  0.0417, -0.0523, -0.0543,  0.0490,\n","          -0.0508, -0.0623,  0.0637,  0.0424,  0.0458, -0.0444,  0.0368,\n","          -0.0542,  0.0459, -0.0422,  0.0161, -0.0042, -0.0246,  0.0490,\n","           0.0466,  0.0516, -0.0448, -0.0609,  0.0479, -0.0332,  0.0395,\n","          -0.0416,  0.0346, -0.0337,  0.0496,  0.0507,  0.0426, -0.0419,\n","           0.0532,  0.0484, -0.0463, -0.0560,  0.0422,  0.0391, -0.0588,\n","           0.0473, -0.0455,  0.0427, -0.0511,  0.0202,  0.0014,  0.0292,\n","           0.0441, -0.0485,  0.0458, -0.0373, -0.0408, -0.0525,  0.0526,\n","          -0.0152, -0.0441,  0.0021, -0.0599,  0.0497, -0.0011, -0.0210,\n","          -0.0307, -0.0476, -0.0514,  0.0195, -0.0549,  0.0312,  0.0541,\n","          -0.0494,  0.0460, -0.0435,  0.0513, -0.0422, -0.0534, -0.0540,\n","           0.0399, -0.0567,  0.0406,  0.0440,  0.0550, -0.0505,  0.0504,\n","           0.0558, -0.0394, -0.0533,  0.0567,  0.0466,  0.0073, -0.0463,\n","          -0.0434,  0.0574, -0.0503, -0.0505,  0.0449,  0.0437, -0.0454,\n","           0.0550,  0.0387, -0.0528, -0.0513,  0.0242,  0.0404,  0.0562,\n","           0.0505,  0.0539,  0.0553, -0.0507,  0.0501,  0.0542, -0.0237,\n","           0.0508,  0.0494, -0.0237, -0.0423,  0.0375,  0.0242, -0.0454,\n","          -0.0055, -0.0404, -0.0372, -0.0201,  0.0504, -0.0314, -0.0436,\n","          -0.0386,  0.0071,  0.0561, -0.0504,  0.0584,  0.0280,  0.0409,\n","          -0.0182,  0.0415,  0.0155,  0.0190,  0.0111, -0.0445,  0.0458,\n","           0.0503, -0.0450,  0.0521,  0.0526, -0.0464, -0.0405, -0.0320,\n","           0.0423]]], device='cuda:0', grad_fn=<CudnnRnnBackward0>)\n","prediction\n","shape :  torch.Size([1, 5893])\n","tensor([[ 0.0939, -0.0778, -0.0831,  ..., -0.0871, -0.0852, -0.0835]],\n","       device='cuda:0', grad_fn=<AddmmBackward0>)\n","input\n","shape :  torch.Size([1, 1])\n","tensor([[1324]], device='cuda:0')\n","embedded\n","shape :  torch.Size([1, 1, 256])\n","tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0185, -0.0045,  0.0000,\n","           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0117,\n","          -0.0127,  0.0000,  0.0125,  0.0000,  0.0000,  0.0000, -0.0132,\n","          -0.0014,  0.0095,  0.0000,  0.0000, -0.0077,  0.0000,  0.0031,\n","           0.0000,  0.0000,  0.0000,  0.0312,  0.0356, -0.0104,  0.0000,\n","          -0.0190,  0.0103, -0.0152,  0.0073,  0.0000,  0.0000,  0.0000,\n","           0.0283,  0.0053,  0.0000,  0.0298,  0.0000,  0.0000,  0.0164,\n","           0.0000, -0.0091, -0.0145, -0.0465,  0.0000,  0.0000,  0.0352,\n","          -0.0261,  0.0000,  0.0000,  0.0000,  0.0000,  0.0322,  0.0000,\n","           0.0299,  0.0000, -0.0177, -0.0063, -0.0065,  0.0058,  0.0000,\n","           0.0000,  0.0000,  0.0000, -0.0010,  0.0000,  0.0006,  0.0012,\n","           0.0000,  0.0253,  0.0000,  0.0000, -0.0148,  0.0000,  0.0216,\n","          -0.0204, -0.0101,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","           0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0191,  0.0000,\n","          -0.0304, -0.0048, -0.0234,  0.0000,  0.0000,  0.0000,  0.0000,\n","          -0.0340,  0.0000, -0.0203,  0.0146, -0.0249,  0.0000, -0.0005,\n","          -0.0207,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","           0.0097,  0.0207,  0.0000, -0.0081, -0.0346, -0.0050,  0.0000,\n","           0.0228,  0.0007, -0.0260, -0.0007,  0.0000,  0.0000,  0.0000,\n","           0.0270,  0.0000,  0.0000,  0.0000,  0.0000, -0.0028,  0.0112,\n","           0.0000, -0.0136,  0.0033,  0.0000,  0.0000,  0.0000,  0.0214,\n","           0.0493,  0.0000,  0.0000,  0.0212,  0.0326,  0.0034,  0.0083,\n","          -0.0348, -0.0166,  0.0000,  0.0000,  0.0000, -0.0061,  0.0090,\n","          -0.0058,  0.0215,  0.0000,  0.0088,  0.0000,  0.0040,  0.0000,\n","          -0.0236, -0.0168,  0.0000, -0.0092,  0.0074,  0.0000,  0.0000,\n","           0.0000,  0.0247,  0.0000,  0.0000, -0.0115,  0.0100, -0.0212,\n","           0.0299,  0.0108,  0.0000,  0.0000, -0.0121,  0.0000,  0.0057,\n","           0.0000,  0.0199,  0.0193,  0.0341,  0.0000, -0.0432,  0.0000,\n","           0.0000,  0.0000, -0.0004,  0.0044, -0.0257,  0.0166, -0.0063,\n","           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0207,  0.0234,\n","          -0.0010,  0.0306, -0.0512,  0.0314,  0.0000, -0.0014,  0.0205,\n","           0.0048,  0.0000,  0.0000,  0.0000,  0.0000,  0.0244,  0.0000,\n","           0.0000,  0.0046,  0.0209, -0.0115, -0.0145, -0.0056,  0.0000,\n","           0.0025,  0.0000, -0.0192,  0.0000,  0.0000,  0.0000,  0.0094,\n","           0.0000,  0.0000, -0.0107, -0.0341,  0.0090,  0.0000,  0.0000,\n","          -0.0210, -0.0114, -0.0005, -0.0128, -0.0508,  0.0000,  0.0000,\n","           0.0089,  0.0000,  0.0000,  0.0000]]], device='cuda:0',\n","       grad_fn=<FusedDropoutBackward0>)\n","hidden :  torch.Size([1, 512])\n","hidden_unsqueeze:  torch.Size([1, 17, 512])\n","encoder_outputs :  torch.Size([17, 1, 1024])\n","encoder_outputs_permute :  torch.Size([1, 17, 1024])\n","energy\n","shape :  torch.Size([1, 17, 512])\n","tensor([[[-0.0217,  0.0285, -0.0066,  ...,  0.0258, -0.0016,  0.0221],\n","         [-0.0229,  0.0321, -0.0085,  ...,  0.0286, -0.0002,  0.0241],\n","         [-0.0225,  0.0325, -0.0099,  ...,  0.0300,  0.0023,  0.0255],\n","         ...,\n","         [-0.0218,  0.0312, -0.0060,  ...,  0.0253,  0.0016,  0.0226],\n","         [-0.0215,  0.0314, -0.0069,  ...,  0.0251,  0.0023,  0.0222],\n","         [-0.0203,  0.0290, -0.0050,  ...,  0.0222,  0.0015,  0.0198]]],\n","       device='cuda:0', grad_fn=<TanhBackward0>)\n","attention\n","shape:  torch.Size([1, 17])\n","tensor([[0.0579, 0.0585, 0.0589, 0.0592, 0.0592, 0.0594, 0.0594, 0.0595, 0.0591,\n","         0.0589, 0.0586, 0.0590, 0.0587, 0.0587, 0.0583, 0.0585, 0.0581]],\n","       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n","a\n","shape :  torch.Size([1, 17])\n","tensor([[0.0579, 0.0585, 0.0589, 0.0592, 0.0592, 0.0594, 0.0594, 0.0595, 0.0591,\n","         0.0589, 0.0586, 0.0590, 0.0587, 0.0587, 0.0583, 0.0585, 0.0581]],\n","       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n","a_unsqueeze\n","shape :  torch.Size([1, 1, 17])\n","tensor([[[0.0579, 0.0585, 0.0589, 0.0592, 0.0592, 0.0594, 0.0594, 0.0595,\n","          0.0591, 0.0589, 0.0586, 0.0590, 0.0587, 0.0587, 0.0583, 0.0585,\n","          0.0581]]], device='cuda:0', grad_fn=<UnsqueezeBackward0>)\n","weighted\n","shape :  torch.Size([1, 1, 1024])\n","tensor([[[ 0.0015,  0.0127, -0.0103,  ..., -0.0120, -0.0075,  0.0108]]],\n","       device='cuda:0', grad_fn=<BmmBackward0>)\n","output\n","shape :  torch.Size([1, 1, 512])\n","tensor([[[ 0.0521, -0.0657,  0.0493,  0.0142, -0.0474, -0.0505,  0.0554,\n","          -0.0327,  0.0546, -0.0068, -0.0570, -0.0075, -0.0433,  0.0399,\n","           0.0447, -0.0603, -0.0258, -0.0484,  0.0410,  0.0067,  0.0554,\n","          -0.0561, -0.0441, -0.0522, -0.0443, -0.0379,  0.0309, -0.0391,\n","          -0.0466,  0.0537,  0.0526,  0.0490,  0.0404, -0.0328, -0.0536,\n","          -0.0386,  0.0408,  0.0535, -0.0557,  0.0606, -0.0502,  0.0453,\n","           0.0438,  0.0404, -0.0180, -0.0376, -0.0590,  0.0445, -0.0355,\n","          -0.0319, -0.0469,  0.0492,  0.0420,  0.0373,  0.0549, -0.0519,\n","          -0.0579,  0.0490, -0.0023, -0.0224, -0.0423, -0.0531, -0.0550,\n","          -0.0565,  0.0515, -0.0338, -0.0431, -0.0183,  0.0514, -0.0498,\n","           0.0507, -0.0380, -0.0435,  0.0452,  0.0560, -0.0202,  0.0070,\n","          -0.0418, -0.0444, -0.0382, -0.0349, -0.0244,  0.0509, -0.0215,\n","          -0.0540, -0.0500, -0.0271, -0.0515,  0.0595, -0.0304,  0.0161,\n","           0.0433,  0.0146, -0.0479, -0.0544, -0.0502,  0.0404, -0.0560,\n","          -0.0506, -0.0067,  0.0436,  0.0606, -0.0392, -0.0535,  0.0425,\n","           0.0410,  0.0333, -0.0544,  0.0458,  0.0479, -0.0518,  0.0561,\n","           0.0513, -0.0483, -0.0106, -0.0193, -0.0214,  0.0186,  0.0440,\n","           0.0491, -0.0513,  0.0566,  0.0041,  0.0432,  0.0196, -0.0608,\n","          -0.0475,  0.0524,  0.0566,  0.0484, -0.0270, -0.0428, -0.0569,\n","           0.0474, -0.0566, -0.0521, -0.0556,  0.0534, -0.0403,  0.0434,\n","          -0.0340, -0.0568,  0.0422, -0.0139,  0.0484,  0.0450,  0.0424,\n","          -0.0183,  0.0601,  0.0504, -0.0527, -0.0311, -0.0490, -0.0495,\n","          -0.0253,  0.0493, -0.0565,  0.0395,  0.0429, -0.0427, -0.0437,\n","           0.0497, -0.0132,  0.0532,  0.0557,  0.0535, -0.0553,  0.0481,\n","          -0.0558,  0.0555, -0.0268,  0.0521, -0.0269,  0.0175, -0.0446,\n","          -0.0305,  0.0486, -0.0551, -0.0566, -0.0447, -0.0609, -0.0211,\n","          -0.0477,  0.0072,  0.0444,  0.0539,  0.0536,  0.0406, -0.0583,\n","           0.0455,  0.0434, -0.0237, -0.0485,  0.0518,  0.0567,  0.0494,\n","          -0.0523,  0.0394, -0.0474,  0.0449,  0.0613,  0.0505,  0.0102,\n","          -0.0600,  0.0436,  0.0477, -0.0471,  0.0487,  0.0095,  0.0485,\n","          -0.0520,  0.0540, -0.0574, -0.0559,  0.0043, -0.0534,  0.0498,\n","           0.0440,  0.0584, -0.0471,  0.0523,  0.0043, -0.0484, -0.0562,\n","          -0.0573, -0.0502, -0.0521, -0.0507, -0.0221,  0.0523,  0.0425,\n","          -0.0184,  0.0493,  0.0496, -0.0643, -0.0036,  0.0604,  0.0213,\n","           0.0561,  0.0458,  0.0490, -0.0500,  0.0510, -0.0398,  0.0582,\n","           0.0511, -0.0603, -0.0523,  0.0385,  0.0492,  0.0334, -0.0447,\n","           0.0198,  0.0551, -0.0451,  0.0525,  0.0511, -0.0341, -0.0562,\n","           0.0445, -0.0504,  0.0188,  0.0439, -0.0611, -0.0518,  0.0296,\n","           0.0568,  0.0537,  0.0311, -0.0541, -0.0546,  0.0090, -0.0091,\n","          -0.0507, -0.0566, -0.0565, -0.0566,  0.0563, -0.0310, -0.0099,\n","          -0.0502,  0.0427,  0.0474, -0.0330,  0.0515,  0.0471,  0.0360,\n","          -0.0529,  0.0407,  0.0512,  0.0444,  0.0388,  0.0557,  0.0464,\n","           0.0379, -0.0546,  0.0563,  0.0206,  0.0248, -0.0305,  0.0392,\n","           0.0343, -0.0363, -0.0536,  0.0466,  0.0582,  0.0424,  0.0414,\n","          -0.0343,  0.0503,  0.0432, -0.0594,  0.0487, -0.0523,  0.0542,\n","          -0.0495, -0.0586, -0.0325, -0.0547, -0.0531,  0.0631, -0.0511,\n","          -0.0473, -0.0556, -0.0518,  0.0341, -0.0024,  0.0583,  0.0501,\n","           0.0518,  0.0534, -0.0434,  0.0434, -0.0554, -0.0349,  0.0447,\n","           0.0524,  0.0442,  0.0671, -0.0559,  0.0592, -0.0399, -0.0435,\n","           0.0227, -0.0069,  0.0573,  0.0561,  0.0435, -0.0431, -0.0503,\n","           0.0543,  0.0490,  0.0553, -0.0237,  0.0577, -0.0155,  0.0554,\n","          -0.0602, -0.0475, -0.0460,  0.0571,  0.0557, -0.0053,  0.0293,\n","          -0.0492, -0.0478,  0.0476,  0.0440, -0.0558, -0.0554,  0.0506,\n","          -0.0521, -0.0638,  0.0660,  0.0438,  0.0448, -0.0460,  0.0394,\n","          -0.0542,  0.0479, -0.0400,  0.0178, -0.0001, -0.0255,  0.0479,\n","           0.0478,  0.0525, -0.0442, -0.0633,  0.0500, -0.0351,  0.0398,\n","          -0.0430,  0.0360, -0.0342,  0.0525,  0.0532,  0.0425, -0.0470,\n","           0.0549,  0.0509, -0.0483, -0.0602,  0.0438,  0.0395, -0.0614,\n","           0.0508, -0.0466,  0.0440, -0.0521,  0.0212,  0.0044,  0.0302,\n","           0.0444, -0.0493,  0.0487, -0.0393, -0.0400, -0.0521,  0.0545,\n","          -0.0146, -0.0449, -0.0013, -0.0592,  0.0522, -0.0015, -0.0174,\n","          -0.0295, -0.0471, -0.0538,  0.0155, -0.0577,  0.0318,  0.0563,\n","          -0.0507,  0.0488, -0.0447,  0.0519, -0.0437, -0.0551, -0.0551,\n","           0.0416, -0.0598,  0.0418,  0.0468,  0.0548, -0.0522,  0.0522,\n","           0.0568, -0.0418, -0.0544,  0.0611,  0.0483,  0.0056, -0.0481,\n","          -0.0450,  0.0602, -0.0504, -0.0531,  0.0450,  0.0437, -0.0475,\n","           0.0554,  0.0387, -0.0546, -0.0527,  0.0286,  0.0454,  0.0605,\n","           0.0572,  0.0578,  0.0605, -0.0550,  0.0515,  0.0579, -0.0256,\n","           0.0523,  0.0506, -0.0249, -0.0407,  0.0368,  0.0196, -0.0471,\n","          -0.0095, -0.0405, -0.0362, -0.0204,  0.0521, -0.0378, -0.0455,\n","          -0.0443,  0.0064,  0.0576, -0.0537,  0.0608,  0.0268,  0.0432,\n","          -0.0185,  0.0442,  0.0154,  0.0208,  0.0129, -0.0485,  0.0488,\n","           0.0523, -0.0464,  0.0536,  0.0550, -0.0492, -0.0440, -0.0327,\n","           0.0425]]], device='cuda:0', grad_fn=<CudnnRnnBackward0>)\n","hidden\n","shape :  torch.Size([1, 1, 512])\n","tensor([[[ 0.0521, -0.0657,  0.0493,  0.0142, -0.0474, -0.0505,  0.0554,\n","          -0.0327,  0.0546, -0.0068, -0.0570, -0.0075, -0.0433,  0.0399,\n","           0.0447, -0.0603, -0.0258, -0.0484,  0.0410,  0.0067,  0.0554,\n","          -0.0561, -0.0441, -0.0522, -0.0443, -0.0379,  0.0309, -0.0391,\n","          -0.0466,  0.0537,  0.0526,  0.0490,  0.0404, -0.0328, -0.0536,\n","          -0.0386,  0.0408,  0.0535, -0.0557,  0.0606, -0.0502,  0.0453,\n","           0.0438,  0.0404, -0.0180, -0.0376, -0.0590,  0.0445, -0.0355,\n","          -0.0319, -0.0469,  0.0492,  0.0420,  0.0373,  0.0549, -0.0519,\n","          -0.0579,  0.0490, -0.0023, -0.0224, -0.0423, -0.0531, -0.0550,\n","          -0.0565,  0.0515, -0.0338, -0.0431, -0.0183,  0.0514, -0.0498,\n","           0.0507, -0.0380, -0.0435,  0.0452,  0.0560, -0.0202,  0.0070,\n","          -0.0418, -0.0444, -0.0382, -0.0349, -0.0244,  0.0509, -0.0215,\n","          -0.0540, -0.0500, -0.0271, -0.0515,  0.0595, -0.0304,  0.0161,\n","           0.0433,  0.0146, -0.0479, -0.0544, -0.0502,  0.0404, -0.0560,\n","          -0.0506, -0.0067,  0.0436,  0.0606, -0.0392, -0.0535,  0.0425,\n","           0.0410,  0.0333, -0.0544,  0.0458,  0.0479, -0.0518,  0.0561,\n","           0.0513, -0.0483, -0.0106, -0.0193, -0.0214,  0.0186,  0.0440,\n","           0.0491, -0.0513,  0.0566,  0.0041,  0.0432,  0.0196, -0.0608,\n","          -0.0475,  0.0524,  0.0566,  0.0484, -0.0270, -0.0428, -0.0569,\n","           0.0474, -0.0566, -0.0521, -0.0556,  0.0534, -0.0403,  0.0434,\n","          -0.0340, -0.0568,  0.0422, -0.0139,  0.0484,  0.0450,  0.0424,\n","          -0.0183,  0.0601,  0.0504, -0.0527, -0.0311, -0.0490, -0.0495,\n","          -0.0253,  0.0493, -0.0565,  0.0395,  0.0429, -0.0427, -0.0437,\n","           0.0497, -0.0132,  0.0532,  0.0557,  0.0535, -0.0553,  0.0481,\n","          -0.0558,  0.0555, -0.0268,  0.0521, -0.0269,  0.0175, -0.0446,\n","          -0.0305,  0.0486, -0.0551, -0.0566, -0.0447, -0.0609, -0.0211,\n","          -0.0477,  0.0072,  0.0444,  0.0539,  0.0536,  0.0406, -0.0583,\n","           0.0455,  0.0434, -0.0237, -0.0485,  0.0518,  0.0567,  0.0494,\n","          -0.0523,  0.0394, -0.0474,  0.0449,  0.0613,  0.0505,  0.0102,\n","          -0.0600,  0.0436,  0.0477, -0.0471,  0.0487,  0.0095,  0.0485,\n","          -0.0520,  0.0540, -0.0574, -0.0559,  0.0043, -0.0534,  0.0498,\n","           0.0440,  0.0584, -0.0471,  0.0523,  0.0043, -0.0484, -0.0562,\n","          -0.0573, -0.0502, -0.0521, -0.0507, -0.0221,  0.0523,  0.0425,\n","          -0.0184,  0.0493,  0.0496, -0.0643, -0.0036,  0.0604,  0.0213,\n","           0.0561,  0.0458,  0.0490, -0.0500,  0.0510, -0.0398,  0.0582,\n","           0.0511, -0.0603, -0.0523,  0.0385,  0.0492,  0.0334, -0.0447,\n","           0.0198,  0.0551, -0.0451,  0.0525,  0.0511, -0.0341, -0.0562,\n","           0.0445, -0.0504,  0.0188,  0.0439, -0.0611, -0.0518,  0.0296,\n","           0.0568,  0.0537,  0.0311, -0.0541, -0.0546,  0.0090, -0.0091,\n","          -0.0507, -0.0566, -0.0565, -0.0566,  0.0563, -0.0310, -0.0099,\n","          -0.0502,  0.0427,  0.0474, -0.0330,  0.0515,  0.0471,  0.0360,\n","          -0.0529,  0.0407,  0.0512,  0.0444,  0.0388,  0.0557,  0.0464,\n","           0.0379, -0.0546,  0.0563,  0.0206,  0.0248, -0.0305,  0.0392,\n","           0.0343, -0.0363, -0.0536,  0.0466,  0.0582,  0.0424,  0.0414,\n","          -0.0343,  0.0503,  0.0432, -0.0594,  0.0487, -0.0523,  0.0542,\n","          -0.0495, -0.0586, -0.0325, -0.0547, -0.0531,  0.0631, -0.0511,\n","          -0.0473, -0.0556, -0.0518,  0.0341, -0.0024,  0.0583,  0.0501,\n","           0.0518,  0.0534, -0.0434,  0.0434, -0.0554, -0.0349,  0.0447,\n","           0.0524,  0.0442,  0.0671, -0.0559,  0.0592, -0.0399, -0.0435,\n","           0.0227, -0.0069,  0.0573,  0.0561,  0.0435, -0.0431, -0.0503,\n","           0.0543,  0.0490,  0.0553, -0.0237,  0.0577, -0.0155,  0.0554,\n","          -0.0602, -0.0475, -0.0460,  0.0571,  0.0557, -0.0053,  0.0293,\n","          -0.0492, -0.0478,  0.0476,  0.0440, -0.0558, -0.0554,  0.0506,\n","          -0.0521, -0.0638,  0.0660,  0.0438,  0.0448, -0.0460,  0.0394,\n","          -0.0542,  0.0479, -0.0400,  0.0178, -0.0001, -0.0255,  0.0479,\n","           0.0478,  0.0525, -0.0442, -0.0633,  0.0500, -0.0351,  0.0398,\n","          -0.0430,  0.0360, -0.0342,  0.0525,  0.0532,  0.0425, -0.0470,\n","           0.0549,  0.0509, -0.0483, -0.0602,  0.0438,  0.0395, -0.0614,\n","           0.0508, -0.0466,  0.0440, -0.0521,  0.0212,  0.0044,  0.0302,\n","           0.0444, -0.0493,  0.0487, -0.0393, -0.0400, -0.0521,  0.0545,\n","          -0.0146, -0.0449, -0.0013, -0.0592,  0.0522, -0.0015, -0.0174,\n","          -0.0295, -0.0471, -0.0538,  0.0155, -0.0577,  0.0318,  0.0563,\n","          -0.0507,  0.0488, -0.0447,  0.0519, -0.0437, -0.0551, -0.0551,\n","           0.0416, -0.0598,  0.0418,  0.0468,  0.0548, -0.0522,  0.0522,\n","           0.0568, -0.0418, -0.0544,  0.0611,  0.0483,  0.0056, -0.0481,\n","          -0.0450,  0.0602, -0.0504, -0.0531,  0.0450,  0.0437, -0.0475,\n","           0.0554,  0.0387, -0.0546, -0.0527,  0.0286,  0.0454,  0.0605,\n","           0.0572,  0.0578,  0.0605, -0.0550,  0.0515,  0.0579, -0.0256,\n","           0.0523,  0.0506, -0.0249, -0.0407,  0.0368,  0.0196, -0.0471,\n","          -0.0095, -0.0405, -0.0362, -0.0204,  0.0521, -0.0378, -0.0455,\n","          -0.0443,  0.0064,  0.0576, -0.0537,  0.0608,  0.0268,  0.0432,\n","          -0.0185,  0.0442,  0.0154,  0.0208,  0.0129, -0.0485,  0.0488,\n","           0.0523, -0.0464,  0.0536,  0.0550, -0.0492, -0.0440, -0.0327,\n","           0.0425]]], device='cuda:0', grad_fn=<CudnnRnnBackward0>)\n","prediction\n","shape :  torch.Size([1, 5893])\n","tensor([[ 0.0932, -0.0776, -0.0829,  ..., -0.0872, -0.0819, -0.0849]],\n","       device='cuda:0', grad_fn=<AddmmBackward0>)\n","input\n","shape :  torch.Size([1, 1])\n","tensor([[4]], device='cuda:0')\n","embedded\n","shape :  torch.Size([1, 1, 256])\n","tensor([[[ 1.6198e-02,  3.4645e-02,  0.0000e+00,  0.0000e+00,  2.4405e-02,\n","           0.0000e+00,  5.7786e-03,  1.2209e-02, -1.2546e-02, -1.1846e-03,\n","           0.0000e+00,  6.7084e-03,  0.0000e+00,  0.0000e+00,  3.5461e-02,\n","           0.0000e+00,  1.2650e-02,  2.2573e-02,  0.0000e+00,  0.0000e+00,\n","           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n","          -3.9367e-02,  1.2866e-03,  0.0000e+00,  0.0000e+00, -4.5220e-03,\n","           3.2308e-03, -7.6678e-03,  1.8470e-02,  3.5479e-02,  0.0000e+00,\n","          -4.5039e-03,  0.0000e+00,  8.7471e-03,  0.0000e+00, -3.4529e-02,\n","           0.0000e+00, -6.7019e-03, -5.3435e-02,  4.3452e-02,  4.8505e-03,\n","           0.0000e+00,  0.0000e+00,  4.1270e-02,  0.0000e+00,  0.0000e+00,\n","           4.9885e-02,  0.0000e+00,  0.0000e+00,  1.5164e-02,  0.0000e+00,\n","           9.6168e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n","           0.0000e+00, -2.1976e-02,  0.0000e+00,  1.8052e-03, -2.6164e-02,\n","          -2.7826e-02,  0.0000e+00, -1.7205e-02, -4.2019e-03,  0.0000e+00,\n","          -1.6763e-02,  0.0000e+00,  2.9043e-02,  5.0927e-03, -1.6040e-02,\n","          -1.9211e-02,  0.0000e+00,  0.0000e+00, -1.2598e-02,  0.0000e+00,\n","           0.0000e+00,  0.0000e+00,  2.2715e-02,  0.0000e+00,  0.0000e+00,\n","           3.5517e-02,  0.0000e+00,  1.5028e-02,  1.7465e-02, -9.3194e-03,\n","           0.0000e+00,  1.3733e-02,  0.0000e+00,  5.3709e-03, -8.3767e-03,\n","           2.6509e-02, -1.7144e-02, -2.0959e-02, -5.3746e-03,  4.2007e-02,\n","           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n","           9.2942e-03,  2.9442e-02,  4.2129e-02,  0.0000e+00,  0.0000e+00,\n","           7.6755e-03,  5.9982e-02, -3.7044e-02,  0.0000e+00, -3.0980e-02,\n","           0.0000e+00, -7.6056e-03,  0.0000e+00,  9.5881e-03,  2.5596e-02,\n","          -8.8254e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00, -2.8633e-02,\n","           0.0000e+00,  0.0000e+00, -2.8256e-02,  1.1942e-02,  0.0000e+00,\n","           0.0000e+00,  1.8211e-02,  7.2169e-03,  0.0000e+00,  0.0000e+00,\n","          -4.4891e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  2.8670e-02,\n","          -2.3086e-02,  0.0000e+00,  3.2835e-02,  1.1692e-02,  0.0000e+00,\n","           0.0000e+00,  0.0000e+00,  0.0000e+00,  8.4739e-03,  7.0089e-03,\n","          -4.0073e-03,  5.7542e-03,  1.7637e-02,  4.7561e-02,  1.1780e-02,\n","          -5.6103e-03,  0.0000e+00, -8.5997e-03, -1.6569e-02,  0.0000e+00,\n","           2.6258e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n","          -2.4264e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  2.9027e-02,\n","           0.0000e+00,  0.0000e+00, -1.1072e-03, -4.3250e-03,  0.0000e+00,\n","           1.5267e-02, -1.6279e-02,  4.2635e-02,  0.0000e+00, -3.9349e-02,\n","           1.0634e-02,  2.7508e-02,  1.4568e-02,  0.0000e+00,  2.0003e-03,\n","           0.0000e+00,  1.4402e-02,  7.3313e-03,  0.0000e+00,  0.0000e+00,\n","           0.0000e+00,  0.0000e+00,  0.0000e+00,  7.9835e-03,  0.0000e+00,\n","           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.5033e-02,\n","           0.0000e+00, -1.0403e-02,  2.8080e-02,  0.0000e+00,  0.0000e+00,\n","           1.8396e-02,  1.5733e-03,  8.0419e-05,  0.0000e+00,  0.0000e+00,\n","           0.0000e+00,  0.0000e+00,  1.2975e-02,  1.4988e-02,  4.9258e-03,\n","          -1.1707e-02,  5.0956e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n","           0.0000e+00, -2.4792e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n","          -1.8195e-02, -6.5817e-03,  9.2606e-03,  1.8188e-02,  0.0000e+00,\n","          -3.8982e-02,  0.0000e+00,  1.3340e-02, -3.4385e-03,  0.0000e+00,\n","           0.0000e+00, -1.7875e-02,  4.5340e-03,  0.0000e+00,  1.9467e-02,\n","           0.0000e+00,  2.4676e-02,  0.0000e+00, -1.1276e-02, -2.8979e-03,\n","           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n","          -9.4923e-03,  0.0000e+00, -2.9705e-02, -2.5812e-02, -8.6323e-03,\n","           1.2824e-02]]], device='cuda:0', grad_fn=<FusedDropoutBackward0>)\n","hidden :  torch.Size([1, 512])\n","hidden_unsqueeze:  torch.Size([1, 17, 512])\n","encoder_outputs :  torch.Size([17, 1, 1024])\n","encoder_outputs_permute :  torch.Size([1, 17, 1024])\n","energy\n","shape :  torch.Size([1, 17, 512])\n","tensor([[[-0.0220,  0.0298, -0.0068,  ...,  0.0262, -0.0025,  0.0215],\n","         [-0.0232,  0.0335, -0.0087,  ...,  0.0290, -0.0010,  0.0235],\n","         [-0.0228,  0.0339, -0.0101,  ...,  0.0304,  0.0015,  0.0249],\n","         ...,\n","         [-0.0221,  0.0326, -0.0062,  ...,  0.0257,  0.0008,  0.0220],\n","         [-0.0217,  0.0328, -0.0071,  ...,  0.0255,  0.0014,  0.0216],\n","         [-0.0206,  0.0304, -0.0052,  ...,  0.0226,  0.0006,  0.0192]]],\n","       device='cuda:0', grad_fn=<TanhBackward0>)\n","attention\n","shape:  torch.Size([1, 17])\n","tensor([[0.0579, 0.0585, 0.0589, 0.0592, 0.0592, 0.0594, 0.0594, 0.0595, 0.0591,\n","         0.0589, 0.0586, 0.0590, 0.0587, 0.0587, 0.0583, 0.0585, 0.0581]],\n","       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n","a\n","shape :  torch.Size([1, 17])\n","tensor([[0.0579, 0.0585, 0.0589, 0.0592, 0.0592, 0.0594, 0.0594, 0.0595, 0.0591,\n","         0.0589, 0.0586, 0.0590, 0.0587, 0.0587, 0.0583, 0.0585, 0.0581]],\n","       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n","a_unsqueeze\n","shape :  torch.Size([1, 1, 17])\n","tensor([[[0.0579, 0.0585, 0.0589, 0.0592, 0.0592, 0.0594, 0.0594, 0.0595,\n","          0.0591, 0.0589, 0.0586, 0.0590, 0.0587, 0.0587, 0.0583, 0.0585,\n","          0.0581]]], device='cuda:0', grad_fn=<UnsqueezeBackward0>)\n","weighted\n","shape :  torch.Size([1, 1, 1024])\n","tensor([[[ 0.0015,  0.0127, -0.0103,  ..., -0.0120, -0.0075,  0.0108]]],\n","       device='cuda:0', grad_fn=<BmmBackward0>)\n","output\n","shape :  torch.Size([1, 1, 512])\n","tensor([[[ 0.0563, -0.0720,  0.0529,  0.0157, -0.0491, -0.0554,  0.0589,\n","          -0.0351,  0.0559, -0.0065, -0.0589, -0.0068, -0.0471,  0.0440,\n","           0.0470, -0.0649, -0.0276, -0.0563,  0.0434,  0.0078,  0.0619,\n","          -0.0623, -0.0481, -0.0565, -0.0465, -0.0386,  0.0346, -0.0430,\n","          -0.0498,  0.0569,  0.0579,  0.0548,  0.0444, -0.0356, -0.0595,\n","          -0.0419,  0.0475,  0.0583, -0.0582,  0.0677, -0.0588,  0.0500,\n","           0.0477,  0.0430, -0.0209, -0.0397, -0.0629,  0.0486, -0.0409,\n","          -0.0352, -0.0518,  0.0504,  0.0458,  0.0384,  0.0591, -0.0567,\n","          -0.0668,  0.0525,  0.0016, -0.0244, -0.0475, -0.0586, -0.0596,\n","          -0.0595,  0.0531, -0.0367, -0.0457, -0.0223,  0.0550, -0.0548,\n","           0.0555, -0.0446, -0.0444,  0.0514,  0.0613, -0.0216,  0.0090,\n","          -0.0458, -0.0482, -0.0426, -0.0402, -0.0253,  0.0558, -0.0212,\n","          -0.0576, -0.0537, -0.0267, -0.0549,  0.0605, -0.0337,  0.0189,\n","           0.0490,  0.0168, -0.0522, -0.0596, -0.0543,  0.0423, -0.0600,\n","          -0.0557, -0.0057,  0.0460,  0.0650, -0.0438, -0.0609,  0.0454,\n","           0.0424,  0.0359, -0.0587,  0.0512,  0.0516, -0.0530,  0.0606,\n","           0.0552, -0.0529, -0.0109, -0.0230, -0.0213,  0.0187,  0.0464,\n","           0.0531, -0.0557,  0.0595,  0.0063,  0.0478,  0.0214, -0.0676,\n","          -0.0542,  0.0545,  0.0639,  0.0497, -0.0304, -0.0466, -0.0632,\n","           0.0535, -0.0590, -0.0577, -0.0621,  0.0580, -0.0426,  0.0480,\n","          -0.0393, -0.0608,  0.0427, -0.0149,  0.0522,  0.0511,  0.0466,\n","          -0.0228,  0.0615,  0.0558, -0.0583, -0.0341, -0.0534, -0.0529,\n","          -0.0263,  0.0557, -0.0628,  0.0415,  0.0462, -0.0466, -0.0490,\n","           0.0554, -0.0137,  0.0570,  0.0584,  0.0582, -0.0588,  0.0523,\n","          -0.0579,  0.0618, -0.0277,  0.0563, -0.0255,  0.0192, -0.0478,\n","          -0.0344,  0.0526, -0.0612, -0.0636, -0.0482, -0.0661, -0.0229,\n","          -0.0526,  0.0068,  0.0509,  0.0567,  0.0576,  0.0448, -0.0654,\n","           0.0513,  0.0486, -0.0231, -0.0516,  0.0567,  0.0624,  0.0530,\n","          -0.0574,  0.0428, -0.0477,  0.0488,  0.0672,  0.0549,  0.0112,\n","          -0.0630,  0.0491,  0.0518, -0.0515,  0.0526,  0.0106,  0.0507,\n","          -0.0563,  0.0588, -0.0632, -0.0594,  0.0008, -0.0546,  0.0536,\n","           0.0487,  0.0634, -0.0495,  0.0563,  0.0038, -0.0531, -0.0603,\n","          -0.0587, -0.0546, -0.0561, -0.0581, -0.0226,  0.0588,  0.0441,\n","          -0.0169,  0.0550,  0.0553, -0.0704, -0.0070,  0.0655,  0.0194,\n","           0.0606,  0.0494,  0.0548, -0.0537,  0.0553, -0.0441,  0.0628,\n","           0.0533, -0.0654, -0.0543,  0.0387,  0.0552,  0.0382, -0.0503,\n","           0.0223,  0.0590, -0.0470,  0.0564,  0.0590, -0.0402, -0.0625,\n","           0.0461, -0.0543,  0.0203,  0.0489, -0.0657, -0.0558,  0.0329,\n","           0.0620,  0.0565,  0.0337, -0.0584, -0.0603,  0.0107, -0.0087,\n","          -0.0557, -0.0632, -0.0608, -0.0600,  0.0597, -0.0334, -0.0107,\n","          -0.0549,  0.0451,  0.0524, -0.0375,  0.0566,  0.0520,  0.0385,\n","          -0.0564,  0.0448,  0.0567,  0.0468,  0.0396,  0.0601,  0.0532,\n","           0.0423, -0.0609,  0.0617,  0.0233,  0.0281, -0.0325,  0.0404,\n","           0.0330, -0.0429, -0.0577,  0.0509,  0.0648,  0.0478,  0.0476,\n","          -0.0386,  0.0545,  0.0476, -0.0642,  0.0549, -0.0557,  0.0569,\n","          -0.0554, -0.0632, -0.0361, -0.0616, -0.0610,  0.0672, -0.0546,\n","          -0.0517, -0.0617, -0.0565,  0.0372,  0.0011,  0.0630,  0.0527,\n","           0.0542,  0.0571, -0.0458,  0.0459, -0.0579, -0.0366,  0.0513,\n","           0.0554,  0.0485,  0.0725, -0.0620,  0.0606, -0.0461, -0.0505,\n","           0.0244, -0.0095,  0.0638,  0.0587,  0.0490, -0.0454, -0.0551,\n","           0.0582,  0.0545,  0.0602, -0.0212,  0.0630, -0.0184,  0.0624,\n","          -0.0634, -0.0500, -0.0475,  0.0642,  0.0612, -0.0068,  0.0300,\n","          -0.0586, -0.0527,  0.0521,  0.0477, -0.0581, -0.0598,  0.0568,\n","          -0.0596, -0.0701,  0.0710,  0.0492,  0.0496, -0.0496,  0.0434,\n","          -0.0600,  0.0526, -0.0446,  0.0179,  0.0007, -0.0259,  0.0543,\n","           0.0528,  0.0589, -0.0467, -0.0700,  0.0553, -0.0403,  0.0403,\n","          -0.0457,  0.0410, -0.0370,  0.0551,  0.0569,  0.0478, -0.0502,\n","           0.0602,  0.0545, -0.0527, -0.0643,  0.0486,  0.0438, -0.0658,\n","           0.0561, -0.0496,  0.0472, -0.0592,  0.0212,  0.0033,  0.0351,\n","           0.0480, -0.0560,  0.0537, -0.0450, -0.0428, -0.0580,  0.0630,\n","          -0.0173, -0.0482, -0.0018, -0.0665,  0.0554,  0.0002, -0.0190,\n","          -0.0307, -0.0505, -0.0588,  0.0163, -0.0625,  0.0345,  0.0622,\n","          -0.0575,  0.0526, -0.0489,  0.0564, -0.0490, -0.0606, -0.0595,\n","           0.0459, -0.0645,  0.0437,  0.0531,  0.0620, -0.0566,  0.0573,\n","           0.0597, -0.0492, -0.0584,  0.0657,  0.0514,  0.0087, -0.0500,\n","          -0.0485,  0.0659, -0.0556, -0.0566,  0.0489,  0.0467, -0.0517,\n","           0.0585,  0.0396, -0.0578, -0.0573,  0.0305,  0.0499,  0.0649,\n","           0.0606,  0.0634,  0.0624, -0.0567,  0.0582,  0.0627, -0.0261,\n","           0.0586,  0.0544, -0.0273, -0.0454,  0.0399,  0.0227, -0.0529,\n","          -0.0113, -0.0458, -0.0416, -0.0201,  0.0574, -0.0426, -0.0510,\n","          -0.0456,  0.0043,  0.0617, -0.0567,  0.0678,  0.0274,  0.0464,\n","          -0.0214,  0.0476,  0.0176,  0.0219,  0.0159, -0.0543,  0.0491,\n","           0.0577, -0.0462,  0.0552,  0.0593, -0.0562, -0.0509, -0.0316,\n","           0.0457]]], device='cuda:0', grad_fn=<CudnnRnnBackward0>)\n","hidden\n","shape :  torch.Size([1, 1, 512])\n","tensor([[[ 0.0563, -0.0720,  0.0529,  0.0157, -0.0491, -0.0554,  0.0589,\n","          -0.0351,  0.0559, -0.0065, -0.0589, -0.0068, -0.0471,  0.0440,\n","           0.0470, -0.0649, -0.0276, -0.0563,  0.0434,  0.0078,  0.0619,\n","          -0.0623, -0.0481, -0.0565, -0.0465, -0.0386,  0.0346, -0.0430,\n","          -0.0498,  0.0569,  0.0579,  0.0548,  0.0444, -0.0356, -0.0595,\n","          -0.0419,  0.0475,  0.0583, -0.0582,  0.0677, -0.0588,  0.0500,\n","           0.0477,  0.0430, -0.0209, -0.0397, -0.0629,  0.0486, -0.0409,\n","          -0.0352, -0.0518,  0.0504,  0.0458,  0.0384,  0.0591, -0.0567,\n","          -0.0668,  0.0525,  0.0016, -0.0244, -0.0475, -0.0586, -0.0596,\n","          -0.0595,  0.0531, -0.0367, -0.0457, -0.0223,  0.0550, -0.0548,\n","           0.0555, -0.0446, -0.0444,  0.0514,  0.0613, -0.0216,  0.0090,\n","          -0.0458, -0.0482, -0.0426, -0.0402, -0.0253,  0.0558, -0.0212,\n","          -0.0576, -0.0537, -0.0267, -0.0549,  0.0605, -0.0337,  0.0189,\n","           0.0490,  0.0168, -0.0522, -0.0596, -0.0543,  0.0423, -0.0600,\n","          -0.0557, -0.0057,  0.0460,  0.0650, -0.0438, -0.0609,  0.0454,\n","           0.0424,  0.0359, -0.0587,  0.0512,  0.0516, -0.0530,  0.0606,\n","           0.0552, -0.0529, -0.0109, -0.0230, -0.0213,  0.0187,  0.0464,\n","           0.0531, -0.0557,  0.0595,  0.0063,  0.0478,  0.0214, -0.0676,\n","          -0.0542,  0.0545,  0.0639,  0.0497, -0.0304, -0.0466, -0.0632,\n","           0.0535, -0.0590, -0.0577, -0.0621,  0.0580, -0.0426,  0.0480,\n","          -0.0393, -0.0608,  0.0427, -0.0149,  0.0522,  0.0511,  0.0466,\n","          -0.0228,  0.0615,  0.0558, -0.0583, -0.0341, -0.0534, -0.0529,\n","          -0.0263,  0.0557, -0.0628,  0.0415,  0.0462, -0.0466, -0.0490,\n","           0.0554, -0.0137,  0.0570,  0.0584,  0.0582, -0.0588,  0.0523,\n","          -0.0579,  0.0618, -0.0277,  0.0563, -0.0255,  0.0192, -0.0478,\n","          -0.0344,  0.0526, -0.0612, -0.0636, -0.0482, -0.0661, -0.0229,\n","          -0.0526,  0.0068,  0.0509,  0.0567,  0.0576,  0.0448, -0.0654,\n","           0.0513,  0.0486, -0.0231, -0.0516,  0.0567,  0.0624,  0.0530,\n","          -0.0574,  0.0428, -0.0477,  0.0488,  0.0672,  0.0549,  0.0112,\n","          -0.0630,  0.0491,  0.0518, -0.0515,  0.0526,  0.0106,  0.0507,\n","          -0.0563,  0.0588, -0.0632, -0.0594,  0.0008, -0.0546,  0.0536,\n","           0.0487,  0.0634, -0.0495,  0.0563,  0.0038, -0.0531, -0.0603,\n","          -0.0587, -0.0546, -0.0561, -0.0581, -0.0226,  0.0588,  0.0441,\n","          -0.0169,  0.0550,  0.0553, -0.0704, -0.0070,  0.0655,  0.0194,\n","           0.0606,  0.0494,  0.0548, -0.0537,  0.0553, -0.0441,  0.0628,\n","           0.0533, -0.0654, -0.0543,  0.0387,  0.0552,  0.0382, -0.0503,\n","           0.0223,  0.0590, -0.0470,  0.0564,  0.0590, -0.0402, -0.0625,\n","           0.0461, -0.0543,  0.0203,  0.0489, -0.0657, -0.0558,  0.0329,\n","           0.0620,  0.0565,  0.0337, -0.0584, -0.0603,  0.0107, -0.0087,\n","          -0.0557, -0.0632, -0.0608, -0.0600,  0.0597, -0.0334, -0.0107,\n","          -0.0549,  0.0451,  0.0524, -0.0375,  0.0566,  0.0520,  0.0385,\n","          -0.0564,  0.0448,  0.0567,  0.0468,  0.0396,  0.0601,  0.0532,\n","           0.0423, -0.0609,  0.0617,  0.0233,  0.0281, -0.0325,  0.0404,\n","           0.0330, -0.0429, -0.0577,  0.0509,  0.0648,  0.0478,  0.0476,\n","          -0.0386,  0.0545,  0.0476, -0.0642,  0.0549, -0.0557,  0.0569,\n","          -0.0554, -0.0632, -0.0361, -0.0616, -0.0610,  0.0672, -0.0546,\n","          -0.0517, -0.0617, -0.0565,  0.0372,  0.0011,  0.0630,  0.0527,\n","           0.0542,  0.0571, -0.0458,  0.0459, -0.0579, -0.0366,  0.0513,\n","           0.0554,  0.0485,  0.0725, -0.0620,  0.0606, -0.0461, -0.0505,\n","           0.0244, -0.0095,  0.0638,  0.0587,  0.0490, -0.0454, -0.0551,\n","           0.0582,  0.0545,  0.0602, -0.0212,  0.0630, -0.0184,  0.0624,\n","          -0.0634, -0.0500, -0.0475,  0.0642,  0.0612, -0.0068,  0.0300,\n","          -0.0586, -0.0527,  0.0521,  0.0477, -0.0581, -0.0598,  0.0568,\n","          -0.0596, -0.0701,  0.0710,  0.0492,  0.0496, -0.0496,  0.0434,\n","          -0.0600,  0.0526, -0.0446,  0.0179,  0.0007, -0.0259,  0.0543,\n","           0.0528,  0.0589, -0.0467, -0.0700,  0.0553, -0.0403,  0.0403,\n","          -0.0457,  0.0410, -0.0370,  0.0551,  0.0569,  0.0478, -0.0502,\n","           0.0602,  0.0545, -0.0527, -0.0643,  0.0486,  0.0438, -0.0658,\n","           0.0561, -0.0496,  0.0472, -0.0592,  0.0212,  0.0033,  0.0351,\n","           0.0480, -0.0560,  0.0537, -0.0450, -0.0428, -0.0580,  0.0630,\n","          -0.0173, -0.0482, -0.0018, -0.0665,  0.0554,  0.0002, -0.0190,\n","          -0.0307, -0.0505, -0.0588,  0.0163, -0.0625,  0.0345,  0.0622,\n","          -0.0575,  0.0526, -0.0489,  0.0564, -0.0490, -0.0606, -0.0595,\n","           0.0459, -0.0645,  0.0437,  0.0531,  0.0620, -0.0566,  0.0573,\n","           0.0597, -0.0492, -0.0584,  0.0657,  0.0514,  0.0087, -0.0500,\n","          -0.0485,  0.0659, -0.0556, -0.0566,  0.0489,  0.0467, -0.0517,\n","           0.0585,  0.0396, -0.0578, -0.0573,  0.0305,  0.0499,  0.0649,\n","           0.0606,  0.0634,  0.0624, -0.0567,  0.0582,  0.0627, -0.0261,\n","           0.0586,  0.0544, -0.0273, -0.0454,  0.0399,  0.0227, -0.0529,\n","          -0.0113, -0.0458, -0.0416, -0.0201,  0.0574, -0.0426, -0.0510,\n","          -0.0456,  0.0043,  0.0617, -0.0567,  0.0678,  0.0274,  0.0464,\n","          -0.0214,  0.0476,  0.0176,  0.0219,  0.0159, -0.0543,  0.0491,\n","           0.0577, -0.0462,  0.0552,  0.0593, -0.0562, -0.0509, -0.0316,\n","           0.0457]]], device='cuda:0', grad_fn=<CudnnRnnBackward0>)\n","prediction\n","shape :  torch.Size([1, 5893])\n","tensor([[ 0.1098, -0.0910, -0.0926,  ..., -0.1000, -0.0956, -0.0967]],\n","       device='cuda:0', grad_fn=<AddmmBackward0>)\n","input\n","shape :  torch.Size([1, 1])\n","tensor([[4]], device='cuda:0')\n","embedded\n","shape :  torch.Size([1, 1, 256])\n","tensor([[[ 0.0000,  0.0346,  0.0000,  0.0000,  0.0000,  0.0214,  0.0058,\n","           0.0122,  0.0000, -0.0012,  0.0000,  0.0067,  0.0000,  0.0266,\n","           0.0000,  0.0000,  0.0127,  0.0000,  0.0236, -0.0102,  0.0000,\n","           0.0000,  0.0000,  0.0000, -0.0012,  0.0000,  0.0000,  0.0000,\n","           0.0220, -0.0045,  0.0000, -0.0077,  0.0000,  0.0000, -0.0115,\n","           0.0000, -0.0344,  0.0087,  0.0000,  0.0000,  0.0000,  0.0000,\n","          -0.0534,  0.0435,  0.0049,  0.0000,  0.0000,  0.0000,  0.0022,\n","           0.0000,  0.0499,  0.0016,  0.0000,  0.0152,  0.0263,  0.0000,\n","           0.0246,  0.0000,  0.0000,  0.0000, -0.0282, -0.0220, -0.0502,\n","           0.0018, -0.0262,  0.0000,  0.0000, -0.0172,  0.0000,  0.0000,\n","          -0.0168,  0.0000,  0.0000,  0.0051, -0.0160, -0.0192,  0.0000,\n","           0.0030,  0.0000,  0.0000, -0.0090,  0.0000,  0.0227,  0.0000,\n","          -0.0009,  0.0000, -0.0087,  0.0150,  0.0000,  0.0000,  0.0000,\n","           0.0000, -0.0240,  0.0000, -0.0084,  0.0000, -0.0171, -0.0210,\n","          -0.0054,  0.0000,  0.0159,  0.0022, -0.0139, -0.0010,  0.0167,\n","           0.0000,  0.0000,  0.0421,  0.0000,  0.0056,  0.0000,  0.0600,\n","           0.0000,  0.0181,  0.0000,  0.0278, -0.0076,  0.0000,  0.0096,\n","           0.0256, -0.0088,  0.0000, -0.0204, -0.0187, -0.0286,  0.0000,\n","           0.0000, -0.0283,  0.0000,  0.0000,  0.0000,  0.0182,  0.0072,\n","           0.0356,  0.0168, -0.0449,  0.0000, -0.0261,  0.0000,  0.0287,\n","           0.0000, -0.0258,  0.0328,  0.0000, -0.0173,  0.0166,  0.0000,\n","          -0.0094,  0.0085,  0.0070,  0.0000,  0.0000,  0.0000,  0.0476,\n","           0.0118,  0.0000, -0.0034, -0.0086,  0.0000, -0.0263,  0.0263,\n","           0.0000, -0.0175, -0.0034, -0.0339,  0.0000,  0.0000, -0.0073,\n","           0.0000,  0.0290,  0.0000, -0.0133, -0.0011, -0.0043, -0.0154,\n","           0.0153, -0.0163,  0.0000,  0.0000, -0.0393,  0.0000,  0.0000,\n","           0.0146,  0.0000,  0.0020,  0.0000,  0.0000,  0.0073,  0.0000,\n","           0.0000,  0.0078,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","           0.0000,  0.0000, -0.0015,  0.0000,  0.0224, -0.0104,  0.0281,\n","           0.0000,  0.0000,  0.0184,  0.0000,  0.0000,  0.0129,  0.0000,\n","           0.0000,  0.0308,  0.0130,  0.0000,  0.0000,  0.0000,  0.0000,\n","          -0.0135,  0.0238,  0.0000, -0.0149, -0.0025, -0.0110, -0.0139,\n","           0.0274,  0.0000, -0.0066,  0.0093,  0.0182,  0.0222, -0.0390,\n","          -0.0094,  0.0000,  0.0000,  0.0000,  0.0167,  0.0000,  0.0045,\n","           0.0000,  0.0195,  0.0000,  0.0000,  0.0000,  0.0000, -0.0029,\n","           0.0353, -0.0195,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n","           0.0000,  0.0000,  0.0000,  0.0128]]], device='cuda:0',\n","       grad_fn=<FusedDropoutBackward0>)\n","hidden :  torch.Size([1, 512])\n","hidden_unsqueeze:  torch.Size([1, 17, 512])\n","encoder_outputs :  torch.Size([17, 1, 1024])\n","encoder_outputs_permute :  torch.Size([1, 17, 1024])\n","energy\n","shape :  torch.Size([1, 17, 512])\n","tensor([[[-0.0229,  0.0321, -0.0068,  ...,  0.0269, -0.0035,  0.0217],\n","         [-0.0240,  0.0357, -0.0086,  ...,  0.0297, -0.0021,  0.0238],\n","         [-0.0236,  0.0361, -0.0100,  ...,  0.0311,  0.0004,  0.0252],\n","         ...,\n","         [-0.0229,  0.0348, -0.0061,  ...,  0.0265, -0.0003,  0.0223],\n","         [-0.0226,  0.0350, -0.0071,  ...,  0.0262,  0.0004,  0.0219],\n","         [-0.0214,  0.0326, -0.0052,  ...,  0.0233, -0.0004,  0.0195]]],\n","       device='cuda:0', grad_fn=<TanhBackward0>)\n","attention\n","shape:  torch.Size([1, 17])\n","tensor([[0.0579, 0.0585, 0.0589, 0.0592, 0.0592, 0.0594, 0.0594, 0.0595, 0.0591,\n","         0.0589, 0.0586, 0.0590, 0.0587, 0.0587, 0.0583, 0.0585, 0.0581]],\n","       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n","a\n","shape :  torch.Size([1, 17])\n","tensor([[0.0579, 0.0585, 0.0589, 0.0592, 0.0592, 0.0594, 0.0594, 0.0595, 0.0591,\n","         0.0589, 0.0586, 0.0590, 0.0587, 0.0587, 0.0583, 0.0585, 0.0581]],\n","       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n","a_unsqueeze\n","shape :  torch.Size([1, 1, 17])\n","tensor([[[0.0579, 0.0585, 0.0589, 0.0592, 0.0592, 0.0594, 0.0594, 0.0595,\n","          0.0591, 0.0589, 0.0586, 0.0590, 0.0587, 0.0587, 0.0583, 0.0585,\n","          0.0581]]], device='cuda:0', grad_fn=<UnsqueezeBackward0>)\n","weighted\n","shape :  torch.Size([1, 1, 1024])\n","tensor([[[ 0.0015,  0.0127, -0.0103,  ..., -0.0120, -0.0075,  0.0108]]],\n","       device='cuda:0', grad_fn=<BmmBackward0>)\n","output\n","shape :  torch.Size([1, 1, 512])\n","tensor([[[ 0.0592, -0.0779,  0.0562,  0.0177, -0.0506, -0.0578,  0.0633,\n","          -0.0354,  0.0578, -0.0071, -0.0621, -0.0085, -0.0510,  0.0482,\n","           0.0490, -0.0660, -0.0304, -0.0592,  0.0464,  0.0094,  0.0675,\n","          -0.0648, -0.0507, -0.0592, -0.0500, -0.0377,  0.0376, -0.0465,\n","          -0.0522,  0.0576,  0.0604,  0.0585,  0.0487, -0.0380, -0.0649,\n","          -0.0469,  0.0536,  0.0621, -0.0622,  0.0709, -0.0630,  0.0566,\n","           0.0499,  0.0433, -0.0249, -0.0407, -0.0666,  0.0513, -0.0455,\n","          -0.0385, -0.0541,  0.0507,  0.0485,  0.0398,  0.0637, -0.0593,\n","          -0.0707,  0.0552,  0.0027, -0.0263, -0.0506, -0.0628, -0.0635,\n","          -0.0614,  0.0554, -0.0389, -0.0469, -0.0232,  0.0583, -0.0579,\n","           0.0575, -0.0475, -0.0459,  0.0555,  0.0642, -0.0224,  0.0119,\n","          -0.0497, -0.0495, -0.0468, -0.0434, -0.0260,  0.0597, -0.0212,\n","          -0.0605, -0.0581, -0.0290, -0.0588,  0.0633, -0.0361,  0.0199,\n","           0.0533,  0.0190, -0.0559, -0.0630, -0.0597,  0.0443, -0.0643,\n","          -0.0610, -0.0045,  0.0473,  0.0680, -0.0484, -0.0640,  0.0514,\n","           0.0429,  0.0380, -0.0627,  0.0516,  0.0550, -0.0541,  0.0644,\n","           0.0579, -0.0554, -0.0109, -0.0250, -0.0230,  0.0193,  0.0501,\n","           0.0548, -0.0590,  0.0609,  0.0077,  0.0513,  0.0219, -0.0723,\n","          -0.0552,  0.0568,  0.0666,  0.0520, -0.0331, -0.0523, -0.0665,\n","           0.0552, -0.0618, -0.0605, -0.0660,  0.0619, -0.0448,  0.0503,\n","          -0.0430, -0.0659,  0.0460, -0.0164,  0.0545,  0.0564,  0.0510,\n","          -0.0252,  0.0645,  0.0603, -0.0624, -0.0364, -0.0564, -0.0569,\n","          -0.0260,  0.0578, -0.0688,  0.0444,  0.0475, -0.0482, -0.0507,\n","           0.0583, -0.0149,  0.0586,  0.0609,  0.0621, -0.0611,  0.0555,\n","          -0.0612,  0.0636, -0.0276,  0.0599, -0.0268,  0.0210, -0.0492,\n","          -0.0371,  0.0560, -0.0648, -0.0678, -0.0511, -0.0709, -0.0237,\n","          -0.0559,  0.0051,  0.0547,  0.0593,  0.0615,  0.0477, -0.0685,\n","           0.0538,  0.0515, -0.0255, -0.0535,  0.0611,  0.0685,  0.0561,\n","          -0.0610,  0.0462, -0.0506,  0.0519,  0.0705,  0.0586,  0.0113,\n","          -0.0667,  0.0525,  0.0533, -0.0544,  0.0567,  0.0117,  0.0536,\n","          -0.0582,  0.0620, -0.0683, -0.0634, -0.0020, -0.0578,  0.0566,\n","           0.0540,  0.0679, -0.0537,  0.0611,  0.0009, -0.0537, -0.0649,\n","          -0.0614, -0.0567, -0.0586, -0.0615, -0.0230,  0.0633,  0.0464,\n","          -0.0153,  0.0580,  0.0593, -0.0767, -0.0064,  0.0689,  0.0181,\n","           0.0640,  0.0530,  0.0596, -0.0576,  0.0585, -0.0450,  0.0664,\n","           0.0554, -0.0703, -0.0596,  0.0418,  0.0588,  0.0401, -0.0551,\n","           0.0229,  0.0623, -0.0493,  0.0596,  0.0647, -0.0412, -0.0654,\n","           0.0493, -0.0576,  0.0202,  0.0530, -0.0674, -0.0586,  0.0350,\n","           0.0662,  0.0598,  0.0365, -0.0619, -0.0635,  0.0115, -0.0078,\n","          -0.0581, -0.0678, -0.0639, -0.0634,  0.0634, -0.0356, -0.0114,\n","          -0.0579,  0.0481,  0.0540, -0.0415,  0.0598,  0.0553,  0.0389,\n","          -0.0584,  0.0476,  0.0606,  0.0486,  0.0409,  0.0624,  0.0540,\n","           0.0466, -0.0641,  0.0665,  0.0246,  0.0300, -0.0347,  0.0435,\n","           0.0335, -0.0470, -0.0618,  0.0555,  0.0683,  0.0504,  0.0496,\n","          -0.0401,  0.0557,  0.0504, -0.0681,  0.0588, -0.0601,  0.0607,\n","          -0.0576, -0.0667, -0.0384, -0.0640, -0.0642,  0.0716, -0.0583,\n","          -0.0539, -0.0665, -0.0588,  0.0404,  0.0035,  0.0659,  0.0545,\n","           0.0553,  0.0629, -0.0469,  0.0490, -0.0617, -0.0377,  0.0552,\n","           0.0591,  0.0512,  0.0756, -0.0663,  0.0644, -0.0484, -0.0539,\n","           0.0259, -0.0108,  0.0686,  0.0627,  0.0525, -0.0478, -0.0579,\n","           0.0617,  0.0579,  0.0632, -0.0213,  0.0672, -0.0188,  0.0686,\n","          -0.0672, -0.0539, -0.0489,  0.0673,  0.0657, -0.0060,  0.0302,\n","          -0.0631, -0.0573,  0.0565,  0.0486, -0.0608, -0.0630,  0.0622,\n","          -0.0640, -0.0711,  0.0756,  0.0510,  0.0523, -0.0498,  0.0467,\n","          -0.0640,  0.0556, -0.0499,  0.0178,  0.0003, -0.0272,  0.0573,\n","           0.0574,  0.0630, -0.0493, -0.0747,  0.0590, -0.0439,  0.0403,\n","          -0.0477,  0.0414, -0.0367,  0.0584,  0.0622,  0.0502, -0.0540,\n","           0.0649,  0.0573, -0.0579, -0.0677,  0.0521,  0.0463, -0.0704,\n","           0.0620, -0.0529,  0.0500, -0.0637,  0.0198,  0.0032,  0.0380,\n","           0.0509, -0.0585,  0.0582, -0.0479, -0.0445, -0.0632,  0.0655,\n","          -0.0162, -0.0506, -0.0005, -0.0695,  0.0597,  0.0007, -0.0197,\n","          -0.0311, -0.0549, -0.0612,  0.0172, -0.0666,  0.0380,  0.0679,\n","          -0.0601,  0.0558, -0.0524,  0.0587, -0.0525, -0.0670, -0.0621,\n","           0.0474, -0.0689,  0.0465,  0.0566,  0.0674, -0.0575,  0.0620,\n","           0.0626, -0.0523, -0.0603,  0.0703,  0.0542,  0.0090, -0.0526,\n","          -0.0502,  0.0688, -0.0603, -0.0613,  0.0511,  0.0484, -0.0545,\n","           0.0628,  0.0405, -0.0613, -0.0598,  0.0329,  0.0536,  0.0677,\n","           0.0653,  0.0654,  0.0651, -0.0607,  0.0613,  0.0648, -0.0291,\n","           0.0624,  0.0573, -0.0278, -0.0493,  0.0436,  0.0245, -0.0560,\n","          -0.0136, -0.0475, -0.0454, -0.0201,  0.0623, -0.0467, -0.0544,\n","          -0.0500,  0.0024,  0.0659, -0.0591,  0.0726,  0.0280,  0.0484,\n","          -0.0216,  0.0499,  0.0184,  0.0235,  0.0156, -0.0575,  0.0519,\n","           0.0632, -0.0476,  0.0573,  0.0609, -0.0603, -0.0554, -0.0299,\n","           0.0488]]], device='cuda:0', grad_fn=<CudnnRnnBackward0>)\n","hidden\n","shape :  torch.Size([1, 1, 512])\n","tensor([[[ 0.0592, -0.0779,  0.0562,  0.0177, -0.0506, -0.0578,  0.0633,\n","          -0.0354,  0.0578, -0.0071, -0.0621, -0.0085, -0.0510,  0.0482,\n","           0.0490, -0.0660, -0.0304, -0.0592,  0.0464,  0.0094,  0.0675,\n","          -0.0648, -0.0507, -0.0592, -0.0500, -0.0377,  0.0376, -0.0465,\n","          -0.0522,  0.0576,  0.0604,  0.0585,  0.0487, -0.0380, -0.0649,\n","          -0.0469,  0.0536,  0.0621, -0.0622,  0.0709, -0.0630,  0.0566,\n","           0.0499,  0.0433, -0.0249, -0.0407, -0.0666,  0.0513, -0.0455,\n","          -0.0385, -0.0541,  0.0507,  0.0485,  0.0398,  0.0637, -0.0593,\n","          -0.0707,  0.0552,  0.0027, -0.0263, -0.0506, -0.0628, -0.0635,\n","          -0.0614,  0.0554, -0.0389, -0.0469, -0.0232,  0.0583, -0.0579,\n","           0.0575, -0.0475, -0.0459,  0.0555,  0.0642, -0.0224,  0.0119,\n","          -0.0497, -0.0495, -0.0468, -0.0434, -0.0260,  0.0597, -0.0212,\n","          -0.0605, -0.0581, -0.0290, -0.0588,  0.0633, -0.0361,  0.0199,\n","           0.0533,  0.0190, -0.0559, -0.0630, -0.0597,  0.0443, -0.0643,\n","          -0.0610, -0.0045,  0.0473,  0.0680, -0.0484, -0.0640,  0.0514,\n","           0.0429,  0.0380, -0.0627,  0.0516,  0.0550, -0.0541,  0.0644,\n","           0.0579, -0.0554, -0.0109, -0.0250, -0.0230,  0.0193,  0.0501,\n","           0.0548, -0.0590,  0.0609,  0.0077,  0.0513,  0.0219, -0.0723,\n","          -0.0552,  0.0568,  0.0666,  0.0520, -0.0331, -0.0523, -0.0665,\n","           0.0552, -0.0618, -0.0605, -0.0660,  0.0619, -0.0448,  0.0503,\n","          -0.0430, -0.0659,  0.0460, -0.0164,  0.0545,  0.0564,  0.0510,\n","          -0.0252,  0.0645,  0.0603, -0.0624, -0.0364, -0.0564, -0.0569,\n","          -0.0260,  0.0578, -0.0688,  0.0444,  0.0475, -0.0482, -0.0507,\n","           0.0583, -0.0149,  0.0586,  0.0609,  0.0621, -0.0611,  0.0555,\n","          -0.0612,  0.0636, -0.0276,  0.0599, -0.0268,  0.0210, -0.0492,\n","          -0.0371,  0.0560, -0.0648, -0.0678, -0.0511, -0.0709, -0.0237,\n","          -0.0559,  0.0051,  0.0547,  0.0593,  0.0615,  0.0477, -0.0685,\n","           0.0538,  0.0515, -0.0255, -0.0535,  0.0611,  0.0685,  0.0561,\n","          -0.0610,  0.0462, -0.0506,  0.0519,  0.0705,  0.0586,  0.0113,\n","          -0.0667,  0.0525,  0.0533, -0.0544,  0.0567,  0.0117,  0.0536,\n","          -0.0582,  0.0620, -0.0683, -0.0634, -0.0020, -0.0578,  0.0566,\n","           0.0540,  0.0679, -0.0537,  0.0611,  0.0009, -0.0537, -0.0649,\n","          -0.0614, -0.0567, -0.0586, -0.0615, -0.0230,  0.0633,  0.0464,\n","          -0.0153,  0.0580,  0.0593, -0.0767, -0.0064,  0.0689,  0.0181,\n","           0.0640,  0.0530,  0.0596, -0.0576,  0.0585, -0.0450,  0.0664,\n","           0.0554, -0.0703, -0.0596,  0.0418,  0.0588,  0.0401, -0.0551,\n","           0.0229,  0.0623, -0.0493,  0.0596,  0.0647, -0.0412, -0.0654,\n","           0.0493, -0.0576,  0.0202,  0.0530, -0.0674, -0.0586,  0.0350,\n","           0.0662,  0.0598,  0.0365, -0.0619, -0.0635,  0.0115, -0.0078,\n","          -0.0581, -0.0678, -0.0639, -0.0634,  0.0634, -0.0356, -0.0114,\n","          -0.0579,  0.0481,  0.0540, -0.0415,  0.0598,  0.0553,  0.0389,\n","          -0.0584,  0.0476,  0.0606,  0.0486,  0.0409,  0.0624,  0.0540,\n","           0.0466, -0.0641,  0.0665,  0.0246,  0.0300, -0.0347,  0.0435,\n","           0.0335, -0.0470, -0.0618,  0.0555,  0.0683,  0.0504,  0.0496,\n","          -0.0401,  0.0557,  0.0504, -0.0681,  0.0588, -0.0601,  0.0607,\n","          -0.0576, -0.0667, -0.0384, -0.0640, -0.0642,  0.0716, -0.0583,\n","          -0.0539, -0.0665, -0.0588,  0.0404,  0.0035,  0.0659,  0.0545,\n","           0.0553,  0.0629, -0.0469,  0.0490, -0.0617, -0.0377,  0.0552,\n","           0.0591,  0.0512,  0.0756, -0.0663,  0.0644, -0.0484, -0.0539,\n","           0.0259, -0.0108,  0.0686,  0.0627,  0.0525, -0.0478, -0.0579,\n","           0.0617,  0.0579,  0.0632, -0.0213,  0.0672, -0.0188,  0.0686,\n","          -0.0672, -0.0539, -0.0489,  0.0673,  0.0657, -0.0060,  0.0302,\n","          -0.0631, -0.0573,  0.0565,  0.0486, -0.0608, -0.0630,  0.0622,\n","          -0.0640, -0.0711,  0.0756,  0.0510,  0.0523, -0.0498,  0.0467,\n","          -0.0640,  0.0556, -0.0499,  0.0178,  0.0003, -0.0272,  0.0573,\n","           0.0574,  0.0630, -0.0493, -0.0747,  0.0590, -0.0439,  0.0403,\n","          -0.0477,  0.0414, -0.0367,  0.0584,  0.0622,  0.0502, -0.0540,\n","           0.0649,  0.0573, -0.0579, -0.0677,  0.0521,  0.0463, -0.0704,\n","           0.0620, -0.0529,  0.0500, -0.0637,  0.0198,  0.0032,  0.0380,\n","           0.0509, -0.0585,  0.0582, -0.0479, -0.0445, -0.0632,  0.0655,\n","          -0.0162, -0.0506, -0.0005, -0.0695,  0.0597,  0.0007, -0.0197,\n","          -0.0311, -0.0549, -0.0612,  0.0172, -0.0666,  0.0380,  0.0679,\n","          -0.0601,  0.0558, -0.0524,  0.0587, -0.0525, -0.0670, -0.0621,\n","           0.0474, -0.0689,  0.0465,  0.0566,  0.0674, -0.0575,  0.0620,\n","           0.0626, -0.0523, -0.0603,  0.0703,  0.0542,  0.0090, -0.0526,\n","          -0.0502,  0.0688, -0.0603, -0.0613,  0.0511,  0.0484, -0.0545,\n","           0.0628,  0.0405, -0.0613, -0.0598,  0.0329,  0.0536,  0.0677,\n","           0.0653,  0.0654,  0.0651, -0.0607,  0.0613,  0.0648, -0.0291,\n","           0.0624,  0.0573, -0.0278, -0.0493,  0.0436,  0.0245, -0.0560,\n","          -0.0136, -0.0475, -0.0454, -0.0201,  0.0623, -0.0467, -0.0544,\n","          -0.0500,  0.0024,  0.0659, -0.0591,  0.0726,  0.0280,  0.0484,\n","          -0.0216,  0.0499,  0.0184,  0.0235,  0.0156, -0.0575,  0.0519,\n","           0.0632, -0.0476,  0.0573,  0.0609, -0.0603, -0.0554, -0.0299,\n","           0.0488]]], device='cuda:0', grad_fn=<CudnnRnnBackward0>)\n","prediction\n","shape :  torch.Size([1, 5893])\n","tensor([[ 0.1124, -0.0959, -0.0967,  ..., -0.1001, -0.0979, -0.0974]],\n","       device='cuda:0', grad_fn=<AddmmBackward0>)\n","input\n","shape :  torch.Size([1, 1])\n","tensor([[4]], device='cuda:0')\n","embedded\n","shape :  torch.Size([1, 1, 256])\n","tensor([[[ 1.6198e-02,  3.4645e-02,  3.3285e-02,  0.0000e+00,  2.4405e-02,\n","           0.0000e+00,  5.7786e-03,  0.0000e+00, -1.2546e-02,  0.0000e+00,\n","           0.0000e+00,  0.0000e+00,  0.0000e+00,  2.6623e-02,  0.0000e+00,\n","          -9.3843e-03,  1.2650e-02,  2.2573e-02,  2.3558e-02, -1.0166e-02,\n","           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n","          -3.9367e-02,  1.2866e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n","           3.2308e-03,  0.0000e+00,  1.8470e-02,  0.0000e+00,  0.0000e+00,\n","          -4.5039e-03, -3.4397e-02,  0.0000e+00,  1.8450e-02, -3.4529e-02,\n","           0.0000e+00,  0.0000e+00,  0.0000e+00,  4.3452e-02,  4.8505e-03,\n","           0.0000e+00,  3.3629e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n","           0.0000e+00,  0.0000e+00, -1.7271e-02,  0.0000e+00,  2.6292e-02,\n","           9.6168e-03,  2.4577e-02,  0.0000e+00, -4.0109e-03,  0.0000e+00,\n","          -2.8231e-02, -2.1976e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n","           0.0000e+00,  3.8585e-02, -1.7205e-02,  0.0000e+00,  0.0000e+00,\n","           0.0000e+00,  0.0000e+00,  0.0000e+00,  5.0927e-03, -1.6040e-02,\n","           0.0000e+00,  2.1168e-02,  0.0000e+00, -1.2598e-02, -1.1366e-02,\n","           0.0000e+00,  1.0481e-02,  2.2715e-02, -4.5176e-03, -8.6039e-04,\n","           0.0000e+00, -8.7161e-03,  1.5028e-02,  0.0000e+00, -9.3194e-03,\n","          -6.4439e-05,  0.0000e+00, -2.3955e-02,  5.3709e-03,  0.0000e+00,\n","           0.0000e+00, -1.7144e-02, -2.0959e-02, -5.3746e-03,  0.0000e+00,\n","           1.5936e-02,  0.0000e+00,  0.0000e+00, -1.0165e-03,  0.0000e+00,\n","           9.2942e-03,  2.9442e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n","           0.0000e+00,  0.0000e+00, -3.7044e-02,  1.8093e-02, -3.0980e-02,\n","           0.0000e+00, -7.6056e-03,  2.3016e-04,  9.5881e-03,  2.5596e-02,\n","          -8.8254e-03,  0.0000e+00, -2.0376e-02,  0.0000e+00, -2.8633e-02,\n","           0.0000e+00,  2.2462e-02,  0.0000e+00,  1.1942e-02,  2.8394e-02,\n","           3.1326e-02,  1.8211e-02,  0.0000e+00,  3.5586e-02,  1.6795e-02,\n","          -4.4891e-02,  0.0000e+00, -2.6115e-02,  3.8957e-03,  2.8670e-02,\n","          -2.3086e-02, -2.5836e-02,  3.2835e-02,  0.0000e+00,  0.0000e+00,\n","           0.0000e+00, -1.9388e-02,  0.0000e+00,  8.4739e-03,  7.0089e-03,\n","          -4.0073e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n","          -5.6103e-03, -3.4060e-03, -8.5997e-03,  0.0000e+00,  0.0000e+00,\n","           2.6258e-02,  0.0000e+00, -1.7497e-02, -3.4400e-03,  0.0000e+00,\n","          -2.4264e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  2.9027e-02,\n","           0.0000e+00,  0.0000e+00, -1.1072e-03,  0.0000e+00, -1.5395e-02,\n","           1.5267e-02,  0.0000e+00,  4.2635e-02, -3.3425e-05, -3.9349e-02,\n","           1.0634e-02,  0.0000e+00,  0.0000e+00,  1.5970e-02,  0.0000e+00,\n","           0.0000e+00,  0.0000e+00,  0.0000e+00, -2.4396e-02,  0.0000e+00,\n","           0.0000e+00,  0.0000e+00,  1.8444e-02,  0.0000e+00,  5.9375e-03,\n","           0.0000e+00,  0.0000e+00,  0.0000e+00, -1.5369e-03,  0.0000e+00,\n","           2.2354e-02,  0.0000e+00,  0.0000e+00,  1.3459e-02,  0.0000e+00,\n","           1.8396e-02,  0.0000e+00,  8.0419e-05,  1.2856e-02,  0.0000e+00,\n","          -1.1032e-02,  3.0802e-02,  1.2975e-02,  0.0000e+00,  0.0000e+00,\n","          -1.1707e-02,  0.0000e+00, -1.3479e-02,  2.3792e-02,  2.2339e-02,\n","           0.0000e+00, -2.4792e-03, -1.1039e-02,  0.0000e+00,  0.0000e+00,\n","           0.0000e+00,  0.0000e+00,  9.2606e-03,  1.8188e-02,  2.2171e-02,\n","           0.0000e+00, -9.3985e-03,  0.0000e+00,  0.0000e+00, -5.1313e-02,\n","           0.0000e+00, -1.7875e-02,  0.0000e+00, -2.0020e-02,  0.0000e+00,\n","           0.0000e+00,  2.4676e-02, -1.9487e-03, -1.1276e-02,  0.0000e+00,\n","           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n","           0.0000e+00,  6.0281e-03, -2.9705e-02,  0.0000e+00,  0.0000e+00,\n","           1.2824e-02]]], device='cuda:0', grad_fn=<FusedDropoutBackward0>)\n","hidden :  torch.Size([1, 512])\n","hidden_unsqueeze:  torch.Size([1, 17, 512])\n","encoder_outputs :  torch.Size([17, 1, 1024])\n","encoder_outputs_permute :  torch.Size([1, 17, 1024])\n","energy\n","shape :  torch.Size([1, 17, 512])\n","tensor([[[-0.0235,  0.0337, -0.0064,  ...,  0.0278, -0.0042,  0.0216],\n","         [-0.0247,  0.0373, -0.0083,  ...,  0.0306, -0.0028,  0.0236],\n","         [-0.0243,  0.0377, -0.0097,  ...,  0.0320, -0.0002,  0.0250],\n","         ...,\n","         [-0.0236,  0.0364, -0.0058,  ...,  0.0274, -0.0009,  0.0221],\n","         [-0.0232,  0.0366, -0.0067,  ...,  0.0271, -0.0003,  0.0217],\n","         [-0.0221,  0.0342, -0.0048,  ...,  0.0242, -0.0011,  0.0193]]],\n","       device='cuda:0', grad_fn=<TanhBackward0>)\n","attention\n","shape:  torch.Size([1, 17])\n","tensor([[0.0579, 0.0585, 0.0589, 0.0592, 0.0592, 0.0594, 0.0594, 0.0595, 0.0591,\n","         0.0589, 0.0586, 0.0590, 0.0587, 0.0587, 0.0583, 0.0585, 0.0581]],\n","       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n","a\n","shape :  torch.Size([1, 17])\n","tensor([[0.0579, 0.0585, 0.0589, 0.0592, 0.0592, 0.0594, 0.0594, 0.0595, 0.0591,\n","         0.0589, 0.0586, 0.0590, 0.0587, 0.0587, 0.0583, 0.0585, 0.0581]],\n","       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n","a_unsqueeze\n","shape :  torch.Size([1, 1, 17])\n","tensor([[[0.0579, 0.0585, 0.0589, 0.0592, 0.0592, 0.0594, 0.0594, 0.0595,\n","          0.0591, 0.0589, 0.0586, 0.0590, 0.0587, 0.0587, 0.0583, 0.0585,\n","          0.0581]]], device='cuda:0', grad_fn=<UnsqueezeBackward0>)\n","weighted\n","shape :  torch.Size([1, 1, 1024])\n","tensor([[[ 0.0015,  0.0127, -0.0103,  ..., -0.0120, -0.0075,  0.0108]]],\n","       device='cuda:0', grad_fn=<BmmBackward0>)\n","output\n","shape :  torch.Size([1, 1, 512])\n","tensor([[[ 0.0617, -0.0816,  0.0561,  0.0176, -0.0527, -0.0590,  0.0662,\n","          -0.0374,  0.0607, -0.0073, -0.0651, -0.0078, -0.0524,  0.0481,\n","           0.0522, -0.0669, -0.0297, -0.0603,  0.0477,  0.0088,  0.0689,\n","          -0.0676, -0.0530, -0.0608, -0.0514, -0.0384,  0.0417, -0.0489,\n","          -0.0531,  0.0597,  0.0622,  0.0595,  0.0506, -0.0390, -0.0685,\n","          -0.0497,  0.0544,  0.0644, -0.0653,  0.0749, -0.0650,  0.0604,\n","           0.0511,  0.0465, -0.0258, -0.0410, -0.0702,  0.0526, -0.0460,\n","          -0.0406, -0.0572,  0.0534,  0.0504,  0.0422,  0.0665, -0.0621,\n","          -0.0745,  0.0581,  0.0046, -0.0296, -0.0526, -0.0643, -0.0664,\n","          -0.0636,  0.0585, -0.0389, -0.0487, -0.0240,  0.0590, -0.0599,\n","           0.0589, -0.0507, -0.0483,  0.0575,  0.0667, -0.0223,  0.0134,\n","          -0.0523, -0.0508, -0.0475, -0.0459, -0.0265,  0.0610, -0.0222,\n","          -0.0631, -0.0607, -0.0275, -0.0606,  0.0645, -0.0384,  0.0203,\n","           0.0562,  0.0193, -0.0604, -0.0665, -0.0627,  0.0467, -0.0660,\n","          -0.0651, -0.0024,  0.0500,  0.0715, -0.0507, -0.0672,  0.0517,\n","           0.0440,  0.0396, -0.0654,  0.0532,  0.0594, -0.0545,  0.0672,\n","           0.0588, -0.0585, -0.0115, -0.0249, -0.0222,  0.0206,  0.0514,\n","           0.0571, -0.0617,  0.0637,  0.0097,  0.0514,  0.0234, -0.0749,\n","          -0.0583,  0.0575,  0.0690,  0.0519, -0.0334, -0.0548, -0.0712,\n","           0.0580, -0.0640, -0.0617, -0.0701,  0.0642, -0.0467,  0.0520,\n","          -0.0448, -0.0678,  0.0489, -0.0174,  0.0571,  0.0584,  0.0524,\n","          -0.0264,  0.0674,  0.0624, -0.0644, -0.0379, -0.0594, -0.0599,\n","          -0.0271,  0.0601, -0.0709,  0.0468,  0.0495, -0.0496, -0.0529,\n","           0.0610, -0.0151,  0.0605,  0.0626,  0.0631, -0.0625,  0.0597,\n","          -0.0639,  0.0666, -0.0297,  0.0613, -0.0273,  0.0217, -0.0525,\n","          -0.0389,  0.0578, -0.0670, -0.0685, -0.0538, -0.0735, -0.0242,\n","          -0.0587,  0.0053,  0.0546,  0.0610,  0.0647,  0.0478, -0.0720,\n","           0.0563,  0.0544, -0.0275, -0.0551,  0.0640,  0.0724,  0.0580,\n","          -0.0636,  0.0466, -0.0530,  0.0534,  0.0736,  0.0604,  0.0109,\n","          -0.0696,  0.0537,  0.0544, -0.0566,  0.0585,  0.0107,  0.0545,\n","          -0.0602,  0.0637, -0.0688, -0.0661, -0.0013, -0.0605,  0.0606,\n","           0.0543,  0.0719, -0.0568,  0.0652, -0.0005, -0.0563, -0.0674,\n","          -0.0642, -0.0602, -0.0607, -0.0627, -0.0244,  0.0650,  0.0483,\n","          -0.0165,  0.0593,  0.0615, -0.0797, -0.0051,  0.0698,  0.0179,\n","           0.0665,  0.0541,  0.0629, -0.0601,  0.0609, -0.0490,  0.0695,\n","           0.0596, -0.0735, -0.0617,  0.0439,  0.0609,  0.0413, -0.0573,\n","           0.0255,  0.0623, -0.0504,  0.0620,  0.0668, -0.0450, -0.0688,\n","           0.0511, -0.0606,  0.0208,  0.0545, -0.0703, -0.0601,  0.0375,\n","           0.0674,  0.0615,  0.0377, -0.0653, -0.0660,  0.0102, -0.0076,\n","          -0.0591, -0.0701, -0.0661, -0.0671,  0.0652, -0.0373, -0.0111,\n","          -0.0599,  0.0505,  0.0548, -0.0429,  0.0634,  0.0565,  0.0407,\n","          -0.0610,  0.0515,  0.0622,  0.0532,  0.0430,  0.0659,  0.0556,\n","           0.0482, -0.0660,  0.0684,  0.0252,  0.0314, -0.0359,  0.0469,\n","           0.0335, -0.0484, -0.0627,  0.0581,  0.0709,  0.0506,  0.0514,\n","          -0.0415,  0.0590,  0.0549, -0.0708,  0.0597, -0.0620,  0.0614,\n","          -0.0606, -0.0698, -0.0386, -0.0659, -0.0667,  0.0762, -0.0587,\n","          -0.0569, -0.0684, -0.0618,  0.0425,  0.0038,  0.0696,  0.0571,\n","           0.0600,  0.0670, -0.0496,  0.0493, -0.0648, -0.0371,  0.0582,\n","           0.0596,  0.0519,  0.0798, -0.0705,  0.0673, -0.0495, -0.0556,\n","           0.0274, -0.0105,  0.0709,  0.0652,  0.0531, -0.0484, -0.0597,\n","           0.0651,  0.0616,  0.0654, -0.0212,  0.0691, -0.0183,  0.0720,\n","          -0.0710, -0.0566, -0.0494,  0.0712,  0.0693, -0.0070,  0.0310,\n","          -0.0661, -0.0592,  0.0598,  0.0504, -0.0636, -0.0660,  0.0640,\n","          -0.0657, -0.0763,  0.0784,  0.0521,  0.0527, -0.0520,  0.0519,\n","          -0.0654,  0.0570, -0.0494,  0.0171,  0.0006, -0.0273,  0.0602,\n","           0.0599,  0.0643, -0.0505, -0.0778,  0.0610, -0.0448,  0.0409,\n","          -0.0458,  0.0403, -0.0381,  0.0599,  0.0652,  0.0517, -0.0552,\n","           0.0680,  0.0609, -0.0610, -0.0707,  0.0531,  0.0463, -0.0721,\n","           0.0640, -0.0558,  0.0505, -0.0649,  0.0208,  0.0032,  0.0375,\n","           0.0514, -0.0595,  0.0578, -0.0483, -0.0462, -0.0641,  0.0686,\n","          -0.0180, -0.0516, -0.0020, -0.0721,  0.0632,  0.0011, -0.0184,\n","          -0.0317, -0.0564, -0.0663,  0.0157, -0.0694,  0.0397,  0.0716,\n","          -0.0644,  0.0601, -0.0552,  0.0616, -0.0538, -0.0700, -0.0650,\n","           0.0480, -0.0730,  0.0476,  0.0599,  0.0708, -0.0594,  0.0662,\n","           0.0668, -0.0563, -0.0635,  0.0733,  0.0570,  0.0115, -0.0536,\n","          -0.0516,  0.0726, -0.0632, -0.0637,  0.0542,  0.0518, -0.0565,\n","           0.0670,  0.0419, -0.0627, -0.0625,  0.0341,  0.0550,  0.0715,\n","           0.0684,  0.0684,  0.0684, -0.0627,  0.0644,  0.0695, -0.0304,\n","           0.0662,  0.0590, -0.0286, -0.0518,  0.0444,  0.0247, -0.0606,\n","          -0.0141, -0.0477, -0.0470, -0.0213,  0.0645, -0.0482, -0.0562,\n","          -0.0524,  0.0013,  0.0693, -0.0628,  0.0756,  0.0294,  0.0500,\n","          -0.0232,  0.0508,  0.0180,  0.0235,  0.0168, -0.0594,  0.0544,\n","           0.0669, -0.0488,  0.0597,  0.0631, -0.0637, -0.0566, -0.0319,\n","           0.0492]]], device='cuda:0', grad_fn=<CudnnRnnBackward0>)\n","hidden\n","shape :  torch.Size([1, 1, 512])\n","tensor([[[ 0.0617, -0.0816,  0.0561,  0.0176, -0.0527, -0.0590,  0.0662,\n","          -0.0374,  0.0607, -0.0073, -0.0651, -0.0078, -0.0524,  0.0481,\n","           0.0522, -0.0669, -0.0297, -0.0603,  0.0477,  0.0088,  0.0689,\n","          -0.0676, -0.0530, -0.0608, -0.0514, -0.0384,  0.0417, -0.0489,\n","          -0.0531,  0.0597,  0.0622,  0.0595,  0.0506, -0.0390, -0.0685,\n","          -0.0497,  0.0544,  0.0644, -0.0653,  0.0749, -0.0650,  0.0604,\n","           0.0511,  0.0465, -0.0258, -0.0410, -0.0702,  0.0526, -0.0460,\n","          -0.0406, -0.0572,  0.0534,  0.0504,  0.0422,  0.0665, -0.0621,\n","          -0.0745,  0.0581,  0.0046, -0.0296, -0.0526, -0.0643, -0.0664,\n","          -0.0636,  0.0585, -0.0389, -0.0487, -0.0240,  0.0590, -0.0599,\n","           0.0589, -0.0507, -0.0483,  0.0575,  0.0667, -0.0223,  0.0134,\n","          -0.0523, -0.0508, -0.0475, -0.0459, -0.0265,  0.0610, -0.0222,\n","          -0.0631, -0.0607, -0.0275, -0.0606,  0.0645, -0.0384,  0.0203,\n","           0.0562,  0.0193, -0.0604, -0.0665, -0.0627,  0.0467, -0.0660,\n","          -0.0651, -0.0024,  0.0500,  0.0715, -0.0507, -0.0672,  0.0517,\n","           0.0440,  0.0396, -0.0654,  0.0532,  0.0594, -0.0545,  0.0672,\n","           0.0588, -0.0585, -0.0115, -0.0249, -0.0222,  0.0206,  0.0514,\n","           0.0571, -0.0617,  0.0637,  0.0097,  0.0514,  0.0234, -0.0749,\n","          -0.0583,  0.0575,  0.0690,  0.0519, -0.0334, -0.0548, -0.0712,\n","           0.0580, -0.0640, -0.0617, -0.0701,  0.0642, -0.0467,  0.0520,\n","          -0.0448, -0.0678,  0.0489, -0.0174,  0.0571,  0.0584,  0.0524,\n","          -0.0264,  0.0674,  0.0624, -0.0644, -0.0379, -0.0594, -0.0599,\n","          -0.0271,  0.0601, -0.0709,  0.0468,  0.0495, -0.0496, -0.0529,\n","           0.0610, -0.0151,  0.0605,  0.0626,  0.0631, -0.0625,  0.0597,\n","          -0.0639,  0.0666, -0.0297,  0.0613, -0.0273,  0.0217, -0.0525,\n","          -0.0389,  0.0578, -0.0670, -0.0685, -0.0538, -0.0735, -0.0242,\n","          -0.0587,  0.0053,  0.0546,  0.0610,  0.0647,  0.0478, -0.0720,\n","           0.0563,  0.0544, -0.0275, -0.0551,  0.0640,  0.0724,  0.0580,\n","          -0.0636,  0.0466, -0.0530,  0.0534,  0.0736,  0.0604,  0.0109,\n","          -0.0696,  0.0537,  0.0544, -0.0566,  0.0585,  0.0107,  0.0545,\n","          -0.0602,  0.0637, -0.0688, -0.0661, -0.0013, -0.0605,  0.0606,\n","           0.0543,  0.0719, -0.0568,  0.0652, -0.0005, -0.0563, -0.0674,\n","          -0.0642, -0.0602, -0.0607, -0.0627, -0.0244,  0.0650,  0.0483,\n","          -0.0165,  0.0593,  0.0615, -0.0797, -0.0051,  0.0698,  0.0179,\n","           0.0665,  0.0541,  0.0629, -0.0601,  0.0609, -0.0490,  0.0695,\n","           0.0596, -0.0735, -0.0617,  0.0439,  0.0609,  0.0413, -0.0573,\n","           0.0255,  0.0623, -0.0504,  0.0620,  0.0668, -0.0450, -0.0688,\n","           0.0511, -0.0606,  0.0208,  0.0545, -0.0703, -0.0601,  0.0375,\n","           0.0674,  0.0615,  0.0377, -0.0653, -0.0660,  0.0102, -0.0076,\n","          -0.0591, -0.0701, -0.0661, -0.0671,  0.0652, -0.0373, -0.0111,\n","          -0.0599,  0.0505,  0.0548, -0.0429,  0.0634,  0.0565,  0.0407,\n","          -0.0610,  0.0515,  0.0622,  0.0532,  0.0430,  0.0659,  0.0556,\n","           0.0482, -0.0660,  0.0684,  0.0252,  0.0314, -0.0359,  0.0469,\n","           0.0335, -0.0484, -0.0627,  0.0581,  0.0709,  0.0506,  0.0514,\n","          -0.0415,  0.0590,  0.0549, -0.0708,  0.0597, -0.0620,  0.0614,\n","          -0.0606, -0.0698, -0.0386, -0.0659, -0.0667,  0.0762, -0.0587,\n","          -0.0569, -0.0684, -0.0618,  0.0425,  0.0038,  0.0696,  0.0571,\n","           0.0600,  0.0670, -0.0496,  0.0493, -0.0648, -0.0371,  0.0582,\n","           0.0596,  0.0519,  0.0798, -0.0705,  0.0673, -0.0495, -0.0556,\n","           0.0274, -0.0105,  0.0709,  0.0652,  0.0531, -0.0484, -0.0597,\n","           0.0651,  0.0616,  0.0654, -0.0212,  0.0691, -0.0183,  0.0720,\n","          -0.0710, -0.0566, -0.0494,  0.0712,  0.0693, -0.0070,  0.0310,\n","          -0.0661, -0.0592,  0.0598,  0.0504, -0.0636, -0.0660,  0.0640,\n","          -0.0657, -0.0763,  0.0784,  0.0521,  0.0527, -0.0520,  0.0519,\n","          -0.0654,  0.0570, -0.0494,  0.0171,  0.0006, -0.0273,  0.0602,\n","           0.0599,  0.0643, -0.0505, -0.0778,  0.0610, -0.0448,  0.0409,\n","          -0.0458,  0.0403, -0.0381,  0.0599,  0.0652,  0.0517, -0.0552,\n","           0.0680,  0.0609, -0.0610, -0.0707,  0.0531,  0.0463, -0.0721,\n","           0.0640, -0.0558,  0.0505, -0.0649,  0.0208,  0.0032,  0.0375,\n","           0.0514, -0.0595,  0.0578, -0.0483, -0.0462, -0.0641,  0.0686,\n","          -0.0180, -0.0516, -0.0020, -0.0721,  0.0632,  0.0011, -0.0184,\n","          -0.0317, -0.0564, -0.0663,  0.0157, -0.0694,  0.0397,  0.0716,\n","          -0.0644,  0.0601, -0.0552,  0.0616, -0.0538, -0.0700, -0.0650,\n","           0.0480, -0.0730,  0.0476,  0.0599,  0.0708, -0.0594,  0.0662,\n","           0.0668, -0.0563, -0.0635,  0.0733,  0.0570,  0.0115, -0.0536,\n","          -0.0516,  0.0726, -0.0632, -0.0637,  0.0542,  0.0518, -0.0565,\n","           0.0670,  0.0419, -0.0627, -0.0625,  0.0341,  0.0550,  0.0715,\n","           0.0684,  0.0684,  0.0684, -0.0627,  0.0644,  0.0695, -0.0304,\n","           0.0662,  0.0590, -0.0286, -0.0518,  0.0444,  0.0247, -0.0606,\n","          -0.0141, -0.0477, -0.0470, -0.0213,  0.0645, -0.0482, -0.0562,\n","          -0.0524,  0.0013,  0.0693, -0.0628,  0.0756,  0.0294,  0.0500,\n","          -0.0232,  0.0508,  0.0180,  0.0235,  0.0168, -0.0594,  0.0544,\n","           0.0669, -0.0488,  0.0597,  0.0631, -0.0637, -0.0566, -0.0319,\n","           0.0492]]], device='cuda:0', grad_fn=<CudnnRnnBackward0>)\n","prediction\n","shape :  torch.Size([1, 5893])\n","tensor([[ 0.1187, -0.0956, -0.0980,  ..., -0.1058, -0.0993, -0.1043]],\n","       device='cuda:0', grad_fn=<AddmmBackward0>)\n","input\n","shape :  torch.Size([1, 1])\n","tensor([[2067]], device='cuda:0')\n","embedded\n","shape :  torch.Size([1, 1, 256])\n","tensor([[[-2.9868e-02, -4.6478e-03, -3.9790e-02, -3.6896e-02,  0.0000e+00,\n","           4.1671e-03,  0.0000e+00, -5.3606e-02, -2.6881e-02, -7.5345e-03,\n","          -2.8641e-02,  9.8520e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n","           0.0000e+00,  0.0000e+00,  2.3779e-02, -3.6536e-03,  0.0000e+00,\n","           0.0000e+00, -2.5559e-02,  0.0000e+00,  0.0000e+00,  1.7411e-02,\n","           0.0000e+00, -1.7069e-03, -3.2371e-02,  0.0000e+00,  0.0000e+00,\n","           5.4407e-03,  5.7098e-03, -2.3822e-02, -2.1985e-03,  0.0000e+00,\n","           0.0000e+00,  0.0000e+00,  2.1099e-02, -6.7423e-03,  0.0000e+00,\n","           3.1427e-03,  7.5490e-03,  0.0000e+00,  0.0000e+00,  4.8486e-02,\n","           5.1238e-02,  0.0000e+00,  4.6378e-03,  0.0000e+00,  1.9099e-02,\n","          -1.1629e-02,  0.0000e+00, -1.2775e-02,  3.5026e-02,  0.0000e+00,\n","           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n","           0.0000e+00, -1.2457e-02,  7.7319e-03,  0.0000e+00, -7.4160e-04,\n","           3.6817e-02,  1.5098e-02,  0.0000e+00,  0.0000e+00, -2.4363e-02,\n","           5.2713e-03,  0.0000e+00,  0.0000e+00,  5.6880e-03, -1.5587e-02,\n","          -2.2920e-04,  0.0000e+00,  0.0000e+00,  8.6325e-03,  0.0000e+00,\n","          -3.5339e-03,  7.5845e-03,  0.0000e+00, -1.0756e-02,  0.0000e+00,\n","          -1.1576e-02, -1.7158e-02,  2.0987e-02, -5.2775e-03,  0.0000e+00,\n","           1.2367e-02,  0.0000e+00,  0.0000e+00,  1.8215e-02,  0.0000e+00,\n","           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  3.7051e-03,\n","           0.0000e+00,  3.3952e-04,  0.0000e+00,  0.0000e+00, -9.1197e-03,\n","           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -4.2828e-02,\n","           0.0000e+00,  0.0000e+00,  4.3481e-03,  2.9238e-03,  2.6699e-03,\n","           0.0000e+00,  0.0000e+00, -9.8784e-03,  0.0000e+00,  0.0000e+00,\n","           0.0000e+00,  4.9601e-03,  1.9295e-02, -5.0334e-02,  0.0000e+00,\n","           0.0000e+00,  0.0000e+00,  0.0000e+00,  1.2813e-03, -7.7585e-03,\n","          -4.2560e-02,  0.0000e+00,  0.0000e+00,  1.6792e-02,  0.0000e+00,\n","           0.0000e+00,  0.0000e+00,  0.0000e+00, -8.2621e-03, -6.9349e-03,\n","           0.0000e+00, -5.0696e-03,  0.0000e+00,  0.0000e+00,  5.2007e-04,\n","          -3.5394e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00, -3.9289e-02,\n","           0.0000e+00, -3.2404e-02, -1.4109e-02,  0.0000e+00,  1.5368e-02,\n","           3.2477e-02,  1.5075e-02,  0.0000e+00, -1.7217e-03, -2.6131e-03,\n","           0.0000e+00,  3.1485e-03, -1.0890e-02,  1.3128e-02, -3.6072e-02,\n","           1.3912e-02,  0.0000e+00,  2.1538e-02, -7.7132e-03,  0.0000e+00,\n","           1.7828e-02,  0.0000e+00,  0.0000e+00, -1.1581e-03,  0.0000e+00,\n","           0.0000e+00,  0.0000e+00,  1.0151e-02,  5.7336e-02,  6.8491e-05,\n","          -2.1305e-02, -7.3697e-03,  0.0000e+00,  0.0000e+00, -8.4753e-03,\n","           0.0000e+00, -7.8473e-03,  0.0000e+00, -5.7589e-03, -2.2980e-02,\n","           0.0000e+00,  8.8527e-03,  1.5652e-02,  5.4413e-03,  7.6812e-03,\n","           1.1586e-02,  0.0000e+00,  0.0000e+00, -4.4442e-03,  0.0000e+00,\n","           1.8086e-03, -2.6311e-02, -6.5063e-03,  0.0000e+00,  0.0000e+00,\n","           0.0000e+00,  1.4468e-02,  0.0000e+00,  1.9997e-02, -5.7522e-03,\n","           0.0000e+00, -2.6573e-02,  0.0000e+00,  0.0000e+00,  1.2394e-02,\n","           0.0000e+00,  0.0000e+00,  0.0000e+00,  1.1038e-03,  0.0000e+00,\n","           0.0000e+00,  0.0000e+00,  1.0351e-02,  0.0000e+00, -4.0840e-02,\n","           0.0000e+00,  0.0000e+00,  4.6508e-02,  1.1676e-02, -3.4123e-02,\n","          -1.7285e-02, -2.6208e-02,  0.0000e+00, -2.2469e-02,  0.0000e+00,\n","          -1.2776e-02, -9.6928e-03, -5.2226e-02,  0.0000e+00,  8.4152e-03,\n","           0.0000e+00,  3.5067e-02,  0.0000e+00, -1.9769e-02,  4.0288e-04,\n","           0.0000e+00,  1.0998e-02, -2.0996e-02, -1.7592e-02, -1.1145e-02,\n","           0.0000e+00,  0.0000e+00,  1.5286e-02,  5.9471e-04,  0.0000e+00,\n","          -4.8654e-03]]], device='cuda:0', grad_fn=<FusedDropoutBackward0>)\n","hidden :  torch.Size([1, 512])\n","hidden_unsqueeze:  torch.Size([1, 17, 512])\n","encoder_outputs :  torch.Size([17, 1, 1024])\n","encoder_outputs_permute :  torch.Size([1, 17, 1024])\n","energy\n","shape :  torch.Size([1, 17, 512])\n","tensor([[[-0.0243,  0.0346, -0.0062,  ...,  0.0281, -0.0044,  0.0218],\n","         [-0.0255,  0.0383, -0.0081,  ...,  0.0309, -0.0029,  0.0239],\n","         [-0.0251,  0.0387, -0.0095,  ...,  0.0323, -0.0004,  0.0252],\n","         ...,\n","         [-0.0244,  0.0373, -0.0056,  ...,  0.0277, -0.0011,  0.0223],\n","         [-0.0240,  0.0376, -0.0065,  ...,  0.0274, -0.0005,  0.0219],\n","         [-0.0229,  0.0351, -0.0046,  ...,  0.0245, -0.0013,  0.0195]]],\n","       device='cuda:0', grad_fn=<TanhBackward0>)\n","attention\n","shape:  torch.Size([1, 17])\n","tensor([[0.0579, 0.0585, 0.0589, 0.0592, 0.0592, 0.0594, 0.0594, 0.0595, 0.0591,\n","         0.0589, 0.0586, 0.0590, 0.0587, 0.0587, 0.0583, 0.0585, 0.0581]],\n","       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n","a\n","shape :  torch.Size([1, 17])\n","tensor([[0.0579, 0.0585, 0.0589, 0.0592, 0.0592, 0.0594, 0.0594, 0.0595, 0.0591,\n","         0.0589, 0.0586, 0.0590, 0.0587, 0.0587, 0.0583, 0.0585, 0.0581]],\n","       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n","a_unsqueeze\n","shape :  torch.Size([1, 1, 17])\n","tensor([[[0.0579, 0.0585, 0.0589, 0.0592, 0.0592, 0.0594, 0.0594, 0.0595,\n","          0.0591, 0.0589, 0.0586, 0.0590, 0.0587, 0.0587, 0.0583, 0.0585,\n","          0.0581]]], device='cuda:0', grad_fn=<UnsqueezeBackward0>)\n","weighted\n","shape :  torch.Size([1, 1, 1024])\n","tensor([[[ 0.0015,  0.0127, -0.0103,  ..., -0.0120, -0.0075,  0.0108]]],\n","       device='cuda:0', grad_fn=<BmmBackward0>)\n","output\n","shape :  torch.Size([1, 1, 512])\n","tensor([[[ 0.0621, -0.0818,  0.0541,  0.0165, -0.0534, -0.0585,  0.0644,\n","          -0.0367,  0.0623, -0.0057, -0.0674, -0.0082, -0.0463,  0.0475,\n","           0.0506, -0.0645, -0.0253, -0.0605,  0.0468,  0.0075,  0.0691,\n","          -0.0668, -0.0540, -0.0630, -0.0521, -0.0383,  0.0406, -0.0484,\n","          -0.0557,  0.0602,  0.0614,  0.0560,  0.0475, -0.0408, -0.0664,\n","          -0.0479,  0.0524,  0.0645, -0.0688,  0.0721, -0.0647,  0.0571,\n","           0.0518,  0.0501, -0.0238, -0.0404, -0.0682,  0.0520, -0.0446,\n","          -0.0437, -0.0556,  0.0529,  0.0464,  0.0422,  0.0678, -0.0604,\n","          -0.0708,  0.0553,  0.0056, -0.0298, -0.0529, -0.0639, -0.0661,\n","          -0.0642,  0.0605, -0.0398, -0.0495, -0.0243,  0.0579, -0.0604,\n","           0.0581, -0.0498, -0.0457,  0.0535,  0.0653, -0.0230,  0.0113,\n","          -0.0499, -0.0499, -0.0466, -0.0451, -0.0289,  0.0622, -0.0225,\n","          -0.0631, -0.0604, -0.0308, -0.0602,  0.0665, -0.0374,  0.0192,\n","           0.0523,  0.0175, -0.0582, -0.0627, -0.0620,  0.0475, -0.0652,\n","          -0.0652, -0.0028,  0.0487,  0.0703, -0.0504, -0.0668,  0.0516,\n","           0.0430,  0.0395, -0.0647,  0.0506,  0.0574, -0.0567,  0.0669,\n","           0.0567, -0.0591, -0.0117, -0.0237, -0.0211,  0.0202,  0.0527,\n","           0.0566, -0.0599,  0.0644,  0.0082,  0.0509,  0.0201, -0.0754,\n","          -0.0565,  0.0566,  0.0656,  0.0531, -0.0312, -0.0517, -0.0680,\n","           0.0567, -0.0630, -0.0620, -0.0712,  0.0630, -0.0460,  0.0499,\n","          -0.0434, -0.0695,  0.0482, -0.0168,  0.0572,  0.0562,  0.0510,\n","          -0.0244,  0.0674,  0.0591, -0.0647, -0.0380, -0.0597, -0.0586,\n","          -0.0281,  0.0562, -0.0682,  0.0442,  0.0488, -0.0484, -0.0522,\n","           0.0614, -0.0132,  0.0601,  0.0645,  0.0605, -0.0634,  0.0565,\n","          -0.0628,  0.0655, -0.0291,  0.0626, -0.0270,  0.0236, -0.0539,\n","          -0.0361,  0.0584, -0.0667, -0.0706, -0.0517, -0.0706, -0.0237,\n","          -0.0571,  0.0036,  0.0511,  0.0605,  0.0667,  0.0491, -0.0696,\n","           0.0531,  0.0527, -0.0275, -0.0560,  0.0628,  0.0709,  0.0546,\n","          -0.0643,  0.0467, -0.0531,  0.0532,  0.0723,  0.0596,  0.0081,\n","          -0.0703,  0.0527,  0.0552, -0.0540,  0.0572,  0.0096,  0.0559,\n","          -0.0593,  0.0640, -0.0663, -0.0652,  0.0027, -0.0609,  0.0593,\n","           0.0498,  0.0713, -0.0572,  0.0618, -0.0016, -0.0539, -0.0671,\n","          -0.0667, -0.0604, -0.0606, -0.0613, -0.0221,  0.0632,  0.0471,\n","          -0.0145,  0.0579,  0.0613, -0.0791, -0.0012,  0.0691,  0.0168,\n","           0.0638,  0.0539,  0.0595, -0.0539,  0.0598, -0.0467,  0.0694,\n","           0.0589, -0.0706, -0.0613,  0.0444,  0.0587,  0.0405, -0.0557,\n","           0.0216,  0.0636, -0.0529,  0.0625,  0.0619, -0.0418, -0.0679,\n","           0.0521, -0.0579,  0.0219,  0.0526, -0.0679, -0.0617,  0.0394,\n","           0.0653,  0.0627,  0.0374, -0.0666, -0.0646,  0.0086, -0.0094,\n","          -0.0571, -0.0699, -0.0670, -0.0638,  0.0675, -0.0361, -0.0113,\n","          -0.0591,  0.0509,  0.0545, -0.0408,  0.0600,  0.0552,  0.0406,\n","          -0.0600,  0.0535,  0.0591,  0.0526,  0.0423,  0.0668,  0.0557,\n","           0.0487, -0.0649,  0.0702,  0.0234,  0.0307, -0.0356,  0.0458,\n","           0.0367, -0.0483, -0.0636,  0.0562,  0.0668,  0.0503,  0.0495,\n","          -0.0378,  0.0603,  0.0528, -0.0705,  0.0577, -0.0620,  0.0616,\n","          -0.0604, -0.0688, -0.0375, -0.0673, -0.0646,  0.0728, -0.0578,\n","          -0.0571, -0.0658, -0.0623,  0.0395,  0.0047,  0.0692,  0.0569,\n","           0.0601,  0.0662, -0.0523,  0.0484, -0.0676, -0.0389,  0.0569,\n","           0.0616,  0.0511,  0.0807, -0.0692,  0.0666, -0.0475, -0.0529,\n","           0.0284, -0.0099,  0.0717,  0.0686,  0.0514, -0.0482, -0.0593,\n","           0.0658,  0.0611,  0.0620, -0.0210,  0.0687, -0.0171,  0.0740,\n","          -0.0705, -0.0550, -0.0484,  0.0697,  0.0649, -0.0063,  0.0303,\n","          -0.0654, -0.0597,  0.0593,  0.0535, -0.0636, -0.0616,  0.0637,\n","          -0.0664, -0.0773,  0.0776,  0.0514,  0.0517, -0.0527,  0.0489,\n","          -0.0653,  0.0560, -0.0475,  0.0175,  0.0025, -0.0283,  0.0577,\n","           0.0559,  0.0664, -0.0521, -0.0766,  0.0611, -0.0442,  0.0427,\n","          -0.0472,  0.0410, -0.0363,  0.0581,  0.0641,  0.0493, -0.0533,\n","           0.0654,  0.0618, -0.0576, -0.0694,  0.0530,  0.0421, -0.0713,\n","           0.0646, -0.0571,  0.0519, -0.0645,  0.0183,  0.0055,  0.0391,\n","           0.0505, -0.0575,  0.0577, -0.0499, -0.0441, -0.0638,  0.0667,\n","          -0.0157, -0.0522, -0.0018, -0.0708,  0.0629,  0.0005, -0.0179,\n","          -0.0309, -0.0536, -0.0631,  0.0155, -0.0699,  0.0402,  0.0713,\n","          -0.0655,  0.0585, -0.0540,  0.0609, -0.0529, -0.0678, -0.0642,\n","           0.0494, -0.0758,  0.0470,  0.0607,  0.0703, -0.0604,  0.0636,\n","           0.0698, -0.0538, -0.0626,  0.0737,  0.0569,  0.0101, -0.0550,\n","          -0.0525,  0.0706, -0.0624, -0.0612,  0.0531,  0.0520, -0.0542,\n","           0.0688,  0.0430, -0.0607, -0.0610,  0.0343,  0.0563,  0.0712,\n","           0.0668,  0.0677,  0.0661, -0.0611,  0.0652,  0.0673, -0.0345,\n","           0.0654,  0.0580, -0.0281, -0.0522,  0.0395,  0.0254, -0.0574,\n","          -0.0146, -0.0444, -0.0454, -0.0193,  0.0656, -0.0460, -0.0532,\n","          -0.0526, -0.0007,  0.0683, -0.0628,  0.0746,  0.0277,  0.0480,\n","          -0.0203,  0.0521,  0.0190,  0.0231,  0.0173, -0.0592,  0.0543,\n","           0.0659, -0.0539,  0.0585,  0.0618, -0.0634, -0.0562, -0.0314,\n","           0.0481]]], device='cuda:0', grad_fn=<CudnnRnnBackward0>)\n","hidden\n","shape :  torch.Size([1, 1, 512])\n","tensor([[[ 0.0621, -0.0818,  0.0541,  0.0165, -0.0534, -0.0585,  0.0644,\n","          -0.0367,  0.0623, -0.0057, -0.0674, -0.0082, -0.0463,  0.0475,\n","           0.0506, -0.0645, -0.0253, -0.0605,  0.0468,  0.0075,  0.0691,\n","          -0.0668, -0.0540, -0.0630, -0.0521, -0.0383,  0.0406, -0.0484,\n","          -0.0557,  0.0602,  0.0614,  0.0560,  0.0475, -0.0408, -0.0664,\n","          -0.0479,  0.0524,  0.0645, -0.0688,  0.0721, -0.0647,  0.0571,\n","           0.0518,  0.0501, -0.0238, -0.0404, -0.0682,  0.0520, -0.0446,\n","          -0.0437, -0.0556,  0.0529,  0.0464,  0.0422,  0.0678, -0.0604,\n","          -0.0708,  0.0553,  0.0056, -0.0298, -0.0529, -0.0639, -0.0661,\n","          -0.0642,  0.0605, -0.0398, -0.0495, -0.0243,  0.0579, -0.0604,\n","           0.0581, -0.0498, -0.0457,  0.0535,  0.0653, -0.0230,  0.0113,\n","          -0.0499, -0.0499, -0.0466, -0.0451, -0.0289,  0.0622, -0.0225,\n","          -0.0631, -0.0604, -0.0308, -0.0602,  0.0665, -0.0374,  0.0192,\n","           0.0523,  0.0175, -0.0582, -0.0627, -0.0620,  0.0475, -0.0652,\n","          -0.0652, -0.0028,  0.0487,  0.0703, -0.0504, -0.0668,  0.0516,\n","           0.0430,  0.0395, -0.0647,  0.0506,  0.0574, -0.0567,  0.0669,\n","           0.0567, -0.0591, -0.0117, -0.0237, -0.0211,  0.0202,  0.0527,\n","           0.0566, -0.0599,  0.0644,  0.0082,  0.0509,  0.0201, -0.0754,\n","          -0.0565,  0.0566,  0.0656,  0.0531, -0.0312, -0.0517, -0.0680,\n","           0.0567, -0.0630, -0.0620, -0.0712,  0.0630, -0.0460,  0.0499,\n","          -0.0434, -0.0695,  0.0482, -0.0168,  0.0572,  0.0562,  0.0510,\n","          -0.0244,  0.0674,  0.0591, -0.0647, -0.0380, -0.0597, -0.0586,\n","          -0.0281,  0.0562, -0.0682,  0.0442,  0.0488, -0.0484, -0.0522,\n","           0.0614, -0.0132,  0.0601,  0.0645,  0.0605, -0.0634,  0.0565,\n","          -0.0628,  0.0655, -0.0291,  0.0626, -0.0270,  0.0236, -0.0539,\n","          -0.0361,  0.0584, -0.0667, -0.0706, -0.0517, -0.0706, -0.0237,\n","          -0.0571,  0.0036,  0.0511,  0.0605,  0.0667,  0.0491, -0.0696,\n","           0.0531,  0.0527, -0.0275, -0.0560,  0.0628,  0.0709,  0.0546,\n","          -0.0643,  0.0467, -0.0531,  0.0532,  0.0723,  0.0596,  0.0081,\n","          -0.0703,  0.0527,  0.0552, -0.0540,  0.0572,  0.0096,  0.0559,\n","          -0.0593,  0.0640, -0.0663, -0.0652,  0.0027, -0.0609,  0.0593,\n","           0.0498,  0.0713, -0.0572,  0.0618, -0.0016, -0.0539, -0.0671,\n","          -0.0667, -0.0604, -0.0606, -0.0613, -0.0221,  0.0632,  0.0471,\n","          -0.0145,  0.0579,  0.0613, -0.0791, -0.0012,  0.0691,  0.0168,\n","           0.0638,  0.0539,  0.0595, -0.0539,  0.0598, -0.0467,  0.0694,\n","           0.0589, -0.0706, -0.0613,  0.0444,  0.0587,  0.0405, -0.0557,\n","           0.0216,  0.0636, -0.0529,  0.0625,  0.0619, -0.0418, -0.0679,\n","           0.0521, -0.0579,  0.0219,  0.0526, -0.0679, -0.0617,  0.0394,\n","           0.0653,  0.0627,  0.0374, -0.0666, -0.0646,  0.0086, -0.0094,\n","          -0.0571, -0.0699, -0.0670, -0.0638,  0.0675, -0.0361, -0.0113,\n","          -0.0591,  0.0509,  0.0545, -0.0408,  0.0600,  0.0552,  0.0406,\n","          -0.0600,  0.0535,  0.0591,  0.0526,  0.0423,  0.0668,  0.0557,\n","           0.0487, -0.0649,  0.0702,  0.0234,  0.0307, -0.0356,  0.0458,\n","           0.0367, -0.0483, -0.0636,  0.0562,  0.0668,  0.0503,  0.0495,\n","          -0.0378,  0.0603,  0.0528, -0.0705,  0.0577, -0.0620,  0.0616,\n","          -0.0604, -0.0688, -0.0375, -0.0673, -0.0646,  0.0728, -0.0578,\n","          -0.0571, -0.0658, -0.0623,  0.0395,  0.0047,  0.0692,  0.0569,\n","           0.0601,  0.0662, -0.0523,  0.0484, -0.0676, -0.0389,  0.0569,\n","           0.0616,  0.0511,  0.0807, -0.0692,  0.0666, -0.0475, -0.0529,\n","           0.0284, -0.0099,  0.0717,  0.0686,  0.0514, -0.0482, -0.0593,\n","           0.0658,  0.0611,  0.0620, -0.0210,  0.0687, -0.0171,  0.0740,\n","          -0.0705, -0.0550, -0.0484,  0.0697,  0.0649, -0.0063,  0.0303,\n","          -0.0654, -0.0597,  0.0593,  0.0535, -0.0636, -0.0616,  0.0637,\n","          -0.0664, -0.0773,  0.0776,  0.0514,  0.0517, -0.0527,  0.0489,\n","          -0.0653,  0.0560, -0.0475,  0.0175,  0.0025, -0.0283,  0.0577,\n","           0.0559,  0.0664, -0.0521, -0.0766,  0.0611, -0.0442,  0.0427,\n","          -0.0472,  0.0410, -0.0363,  0.0581,  0.0641,  0.0493, -0.0533,\n","           0.0654,  0.0618, -0.0576, -0.0694,  0.0530,  0.0421, -0.0713,\n","           0.0646, -0.0571,  0.0519, -0.0645,  0.0183,  0.0055,  0.0391,\n","           0.0505, -0.0575,  0.0577, -0.0499, -0.0441, -0.0638,  0.0667,\n","          -0.0157, -0.0522, -0.0018, -0.0708,  0.0629,  0.0005, -0.0179,\n","          -0.0309, -0.0536, -0.0631,  0.0155, -0.0699,  0.0402,  0.0713,\n","          -0.0655,  0.0585, -0.0540,  0.0609, -0.0529, -0.0678, -0.0642,\n","           0.0494, -0.0758,  0.0470,  0.0607,  0.0703, -0.0604,  0.0636,\n","           0.0698, -0.0538, -0.0626,  0.0737,  0.0569,  0.0101, -0.0550,\n","          -0.0525,  0.0706, -0.0624, -0.0612,  0.0531,  0.0520, -0.0542,\n","           0.0688,  0.0430, -0.0607, -0.0610,  0.0343,  0.0563,  0.0712,\n","           0.0668,  0.0677,  0.0661, -0.0611,  0.0652,  0.0673, -0.0345,\n","           0.0654,  0.0580, -0.0281, -0.0522,  0.0395,  0.0254, -0.0574,\n","          -0.0146, -0.0444, -0.0454, -0.0193,  0.0656, -0.0460, -0.0532,\n","          -0.0526, -0.0007,  0.0683, -0.0628,  0.0746,  0.0277,  0.0480,\n","          -0.0203,  0.0521,  0.0190,  0.0231,  0.0173, -0.0592,  0.0543,\n","           0.0659, -0.0539,  0.0585,  0.0618, -0.0634, -0.0562, -0.0314,\n","           0.0481]]], device='cuda:0', grad_fn=<CudnnRnnBackward0>)\n","prediction\n","shape :  torch.Size([1, 5893])\n","tensor([[ 0.1068, -0.0894, -0.0892,  ..., -0.1003, -0.0907, -0.0877]],\n","       device='cuda:0', grad_fn=<AddmmBackward0>)\n","input\n","shape :  torch.Size([1, 1])\n","tensor([[13]], device='cuda:0')\n","embedded\n","shape :  torch.Size([1, 1, 256])\n","tensor([[[ 0.0000e+00,  0.0000e+00,  5.4879e-03, -2.1234e-02, -2.4222e-02,\n","          -4.5390e-03,  0.0000e+00, -3.4679e-03, -1.1720e-02, -4.4454e-03,\n","           3.6660e-02,  0.0000e+00,  6.2014e-03,  0.0000e+00,  0.0000e+00,\n","          -1.0535e-02,  5.2290e-02,  0.0000e+00,  0.0000e+00, -3.1652e-02,\n","           0.0000e+00,  2.8336e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n","           5.2549e-03, -1.4673e-02,  0.0000e+00, -4.7222e-03,  0.0000e+00,\n","           0.0000e+00,  1.2170e-02,  0.0000e+00, -5.0903e-03,  0.0000e+00,\n","           0.0000e+00,  0.0000e+00,  2.2732e-02,  3.2923e-05,  8.1367e-04,\n","          -1.9940e-02,  0.0000e+00,  0.0000e+00,  4.8262e-03,  0.0000e+00,\n","           0.0000e+00, -1.7330e-02,  0.0000e+00,  0.0000e+00,  1.0520e-02,\n","           0.0000e+00,  0.0000e+00,  0.0000e+00, -2.4992e-03,  2.2065e-03,\n","          -3.5746e-02,  0.0000e+00,  0.0000e+00, -1.1575e-02,  1.1412e-02,\n","           0.0000e+00,  0.0000e+00,  2.8356e-02, -7.9786e-05, -1.0733e-02,\n","           0.0000e+00,  0.0000e+00,  0.0000e+00, -1.5101e-02, -7.5612e-02,\n","           0.0000e+00,  9.9194e-03,  7.7485e-03,  0.0000e+00, -1.1675e-02,\n","           0.0000e+00,  0.0000e+00,  1.3551e-02,  1.2297e-02,  0.0000e+00,\n","           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n","          -1.0476e-02, -1.5775e-02,  1.5445e-02,  0.0000e+00,  0.0000e+00,\n","           0.0000e+00,  0.0000e+00, -1.5539e-03, -4.2163e-02,  0.0000e+00,\n","          -3.9203e-04, -2.1829e-02,  0.0000e+00,  7.9822e-03,  4.6150e-03,\n","          -3.4573e-02,  7.8929e-03,  2.2570e-03,  1.0240e-02,  8.2575e-03,\n","           0.0000e+00,  0.0000e+00,  0.0000e+00,  1.5849e-02,  8.6697e-03,\n","           1.1903e-02,  0.0000e+00, -4.8072e-03,  0.0000e+00,  0.0000e+00,\n","           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  2.0427e-02,\n","           3.9865e-02,  3.2441e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n","           0.0000e+00,  1.0308e-02, -2.4309e-02,  0.0000e+00, -3.7618e-03,\n","           0.0000e+00,  0.0000e+00, -1.2464e-02, -4.2936e-03,  0.0000e+00,\n","           0.0000e+00,  2.1679e-02,  0.0000e+00,  0.0000e+00,  1.9961e-02,\n","           0.0000e+00,  0.0000e+00,  0.0000e+00,  1.9522e-02,  0.0000e+00,\n","           2.8135e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n","           0.0000e+00, -3.6319e-02,  0.0000e+00,  0.0000e+00,  5.8445e-03,\n","           1.2600e-02,  0.0000e+00, -1.4603e-02,  0.0000e+00,  0.0000e+00,\n","          -1.9785e-02,  1.4047e-02, -1.5945e-02,  2.3300e-02,  0.0000e+00,\n","          -1.3841e-02,  0.0000e+00,  0.0000e+00, -1.2068e-02,  1.1484e-02,\n","           0.0000e+00,  0.0000e+00,  0.0000e+00, -1.7102e-02,  1.7039e-02,\n","           0.0000e+00, -3.4002e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n","           0.0000e+00,  3.3038e-02,  0.0000e+00,  1.1070e-02,  0.0000e+00,\n","           5.0960e-03, -1.1551e-02,  8.5651e-03,  0.0000e+00, -5.2044e-03,\n","          -2.5916e-02,  0.0000e+00,  1.0476e-03,  0.0000e+00,  0.0000e+00,\n","           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -5.4128e-03,\n","           0.0000e+00,  0.0000e+00,  0.0000e+00,  4.2646e-03,  0.0000e+00,\n","           0.0000e+00, -9.1771e-03, -1.5512e-03,  1.6045e-02,  2.0028e-02,\n","          -2.5514e-02, -1.2141e-02, -2.3099e-02,  0.0000e+00,  0.0000e+00,\n","           2.1235e-02,  1.7431e-02,  0.0000e+00,  3.0432e-02, -2.9841e-02,\n","          -6.6902e-03, -5.2404e-03,  0.0000e+00,  1.0896e-02,  0.0000e+00,\n","          -1.5549e-02, -1.4165e-02, -3.1535e-02, -1.5720e-04,  0.0000e+00,\n","           8.5050e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00, -2.1732e-02,\n","           0.0000e+00, -3.0598e-02,  0.0000e+00, -2.4898e-03,  1.9101e-03,\n","           3.1855e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.4361e-02,\n","          -1.1381e-02,  1.1240e-02,  1.2406e-02,  5.1198e-02, -1.7859e-02,\n","           0.0000e+00,  1.2044e-02,  9.3680e-03, -8.1465e-03,  0.0000e+00,\n","          -2.0098e-02]]], device='cuda:0', grad_fn=<FusedDropoutBackward0>)\n","hidden :  torch.Size([1, 512])\n","hidden_unsqueeze:  torch.Size([1, 17, 512])\n","encoder_outputs :  torch.Size([17, 1, 1024])\n","encoder_outputs_permute :  torch.Size([1, 17, 1024])\n","energy\n","shape :  torch.Size([1, 17, 512])\n","tensor([[[-0.0245,  0.0345, -0.0058,  ...,  0.0282, -0.0049,  0.0212],\n","         [-0.0257,  0.0382, -0.0077,  ...,  0.0309, -0.0035,  0.0233],\n","         [-0.0253,  0.0386, -0.0091,  ...,  0.0323, -0.0009,  0.0247],\n","         ...,\n","         [-0.0246,  0.0372, -0.0052,  ...,  0.0277, -0.0016,  0.0218],\n","         [-0.0242,  0.0375, -0.0062,  ...,  0.0274, -0.0010,  0.0214],\n","         [-0.0231,  0.0351, -0.0043,  ...,  0.0245, -0.0018,  0.0189]]],\n","       device='cuda:0', grad_fn=<TanhBackward0>)\n","attention\n","shape:  torch.Size([1, 17])\n","tensor([[0.0579, 0.0585, 0.0589, 0.0592, 0.0592, 0.0594, 0.0594, 0.0595, 0.0591,\n","         0.0589, 0.0586, 0.0590, 0.0587, 0.0587, 0.0583, 0.0585, 0.0581]],\n","       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n","a\n","shape :  torch.Size([1, 17])\n","tensor([[0.0579, 0.0585, 0.0589, 0.0592, 0.0592, 0.0594, 0.0594, 0.0595, 0.0591,\n","         0.0589, 0.0586, 0.0590, 0.0587, 0.0587, 0.0583, 0.0585, 0.0581]],\n","       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n","a_unsqueeze\n","shape :  torch.Size([1, 1, 17])\n","tensor([[[0.0579, 0.0585, 0.0589, 0.0592, 0.0592, 0.0594, 0.0594, 0.0595,\n","          0.0591, 0.0589, 0.0586, 0.0590, 0.0587, 0.0587, 0.0583, 0.0585,\n","          0.0581]]], device='cuda:0', grad_fn=<UnsqueezeBackward0>)\n","weighted\n","shape :  torch.Size([1, 1, 1024])\n","tensor([[[ 0.0015,  0.0127, -0.0103,  ..., -0.0120, -0.0075,  0.0108]]],\n","       device='cuda:0', grad_fn=<BmmBackward0>)\n","output\n","shape :  torch.Size([1, 1, 512])\n","tensor([[[ 6.1138e-02, -8.0812e-02,  5.3113e-02,  1.5207e-02, -5.2671e-02,\n","          -5.8849e-02,  6.1591e-02, -3.8525e-02,  6.2430e-02, -7.5894e-03,\n","          -6.7624e-02, -9.1472e-03, -4.5666e-02,  4.7232e-02,  5.1687e-02,\n","          -6.3880e-02, -2.5386e-02, -5.8809e-02,  4.4934e-02,  7.5901e-03,\n","           7.0818e-02, -6.3418e-02, -5.4012e-02, -6.0039e-02, -5.0655e-02,\n","          -3.8728e-02,  3.8875e-02, -4.8767e-02, -5.3663e-02,  6.2145e-02,\n","           5.9513e-02,  5.7120e-02,  4.9017e-02, -4.0272e-02, -6.6889e-02,\n","          -4.9852e-02,  5.0200e-02,  6.3161e-02, -6.9964e-02,  7.2658e-02,\n","          -6.2221e-02,  5.7768e-02,  5.0979e-02,  4.7315e-02, -2.6386e-02,\n","          -4.1745e-02, -6.9128e-02,  5.1389e-02, -4.4314e-02, -3.9957e-02,\n","          -5.5035e-02,  5.6005e-02,  4.6168e-02,  4.1692e-02,  6.5803e-02,\n","          -6.1550e-02, -7.0899e-02,  5.4185e-02,  3.2702e-03, -3.0068e-02,\n","          -5.1623e-02, -6.1795e-02, -6.5970e-02, -6.4569e-02,  5.9108e-02,\n","          -3.9744e-02, -4.7905e-02, -2.4844e-02,  5.8074e-02, -6.0769e-02,\n","           5.7378e-02, -4.9587e-02, -4.5312e-02,  5.0536e-02,  6.4591e-02,\n","          -2.1447e-02,  1.0460e-02, -5.0058e-02, -4.8705e-02, -4.5487e-02,\n","          -4.4873e-02, -2.9356e-02,  6.3138e-02, -2.2678e-02, -6.4001e-02,\n","          -5.8854e-02, -3.2899e-02, -5.7916e-02,  6.5588e-02, -3.5914e-02,\n","           1.9473e-02,  5.2914e-02,  1.7548e-02, -5.8576e-02, -6.4002e-02,\n","          -6.0862e-02,  4.6653e-02, -6.6583e-02, -6.2982e-02, -3.4913e-03,\n","           4.8443e-02,  7.1226e-02, -4.9808e-02, -6.7015e-02,  4.9879e-02,\n","           4.3470e-02,  3.8244e-02, -6.4903e-02,  4.9175e-02,  5.6386e-02,\n","          -5.7942e-02,  6.5128e-02,  5.7793e-02, -6.1214e-02, -1.2041e-02,\n","          -2.2867e-02, -2.0370e-02,  2.0089e-02,  5.3632e-02,  5.5117e-02,\n","          -6.1926e-02,  6.3715e-02,  6.3465e-03,  5.3064e-02,  2.0494e-02,\n","          -7.5072e-02, -5.5360e-02,  5.7531e-02,  6.9202e-02,  5.3569e-02,\n","          -3.1903e-02, -5.1265e-02, -6.7466e-02,  5.5420e-02, -6.2962e-02,\n","          -6.0572e-02, -7.0589e-02,  6.1650e-02, -4.5826e-02,  4.8922e-02,\n","          -4.4029e-02, -6.9406e-02,  4.7901e-02, -1.5235e-02,  5.7229e-02,\n","           5.6981e-02,  4.9751e-02, -2.3148e-02,  6.8495e-02,  5.9677e-02,\n","          -6.5583e-02, -3.7218e-02, -5.9242e-02, -5.8728e-02, -3.0249e-02,\n","           5.4056e-02, -6.5120e-02,  4.4592e-02,  4.7687e-02, -4.8918e-02,\n","          -5.0304e-02,  6.3491e-02, -1.5597e-02,  6.0987e-02,  6.4380e-02,\n","           6.2287e-02, -6.3782e-02,  5.7920e-02, -6.1602e-02,  6.5811e-02,\n","          -2.8638e-02,  6.1632e-02, -2.7002e-02,  2.2811e-02, -5.3841e-02,\n","          -3.3735e-02,  5.6722e-02, -6.6019e-02, -6.8930e-02, -4.9788e-02,\n","          -7.1732e-02, -2.4200e-02, -5.6368e-02,  6.1158e-03,  4.9712e-02,\n","           5.8895e-02,  6.3575e-02,  4.7756e-02, -6.9251e-02,  5.5005e-02,\n","           5.3195e-02, -2.8078e-02, -5.6401e-02,  6.3882e-02,  6.8529e-02,\n","           5.5214e-02, -6.4516e-02,  4.8201e-02, -5.2615e-02,  5.4039e-02,\n","           7.2761e-02,  5.7430e-02,  1.1096e-02, -7.1478e-02,  5.0930e-02,\n","           5.6415e-02, -5.3763e-02,  5.9652e-02,  9.6159e-03,  5.4627e-02,\n","          -6.1654e-02,  6.4116e-02, -6.5080e-02, -6.3803e-02,  4.9292e-03,\n","          -6.1671e-02,  5.9444e-02,  4.8488e-02,  7.0131e-02, -5.5965e-02,\n","           6.1574e-02, -1.7515e-03, -5.4623e-02, -6.8278e-02, -6.6406e-02,\n","          -6.1089e-02, -6.1579e-02, -5.9862e-02, -2.1410e-02,  6.2289e-02,\n","           4.5336e-02, -1.2801e-02,  5.9452e-02,  5.9698e-02, -7.7257e-02,\n","          -1.7779e-03,  6.8200e-02,  1.9489e-02,  6.5087e-02,  5.1853e-02,\n","           5.8357e-02, -5.4721e-02,  5.7839e-02, -4.8183e-02,  6.6983e-02,\n","           5.9139e-02, -6.9004e-02, -5.9448e-02,  4.1896e-02,  5.9069e-02,\n","           4.0065e-02, -5.5692e-02,  2.0893e-02,  6.3289e-02, -5.3045e-02,\n","           6.1336e-02,  6.1302e-02, -4.1553e-02, -6.7209e-02,  5.2571e-02,\n","          -5.8021e-02,  2.2362e-02,  5.0967e-02, -7.0744e-02, -6.0586e-02,\n","           3.8113e-02,  6.4523e-02,  5.9871e-02,  3.8693e-02, -6.3575e-02,\n","          -6.3373e-02,  9.1588e-03, -7.6446e-03, -5.7116e-02, -6.9998e-02,\n","          -6.4836e-02, -6.4556e-02,  6.5063e-02, -3.4534e-02, -1.0003e-02,\n","          -5.9985e-02,  4.9271e-02,  5.6056e-02, -3.8430e-02,  5.8736e-02,\n","           5.3118e-02,  4.2736e-02, -6.1329e-02,  5.1209e-02,  6.1136e-02,\n","           5.2676e-02,  4.2534e-02,  6.6478e-02,  5.5144e-02,  4.7678e-02,\n","          -6.2453e-02,  6.9159e-02,  2.6711e-02,  3.1286e-02, -3.4934e-02,\n","           4.4607e-02,  3.6747e-02, -4.4982e-02, -6.5146e-02,  5.6813e-02,\n","           6.6353e-02,  5.0855e-02,  5.1127e-02, -3.6087e-02,  6.1580e-02,\n","           5.0946e-02, -6.7481e-02,  5.7721e-02, -6.2637e-02,  6.1819e-02,\n","          -6.0817e-02, -6.7178e-02, -3.7565e-02, -6.5620e-02, -6.4283e-02,\n","           7.2717e-02, -5.9949e-02, -5.6813e-02, -6.5826e-02, -6.2264e-02,\n","           3.8238e-02,  4.2579e-03,  6.8096e-02,  5.9558e-02,  6.0094e-02,\n","           6.4741e-02, -4.9217e-02,  4.8855e-02, -6.6973e-02, -4.0333e-02,\n","           5.5701e-02,  6.2151e-02,  5.0477e-02,  8.1305e-02, -6.5448e-02,\n","           6.7367e-02, -4.6008e-02, -5.4037e-02,  2.9515e-02, -7.8871e-03,\n","           7.3764e-02,  7.0673e-02,  5.0692e-02, -4.6842e-02, -6.0280e-02,\n","           6.6220e-02,  6.2853e-02,  6.2141e-02, -2.2073e-02,  6.9005e-02,\n","          -1.5707e-02,  7.2690e-02, -7.0655e-02, -5.5678e-02, -4.8617e-02,\n","           6.6989e-02,  6.4710e-02, -8.6251e-03,  3.2813e-02, -6.5020e-02,\n","          -5.8382e-02,  5.8725e-02,  5.4349e-02, -6.3639e-02, -6.1789e-02,\n","           6.0939e-02, -6.6624e-02, -7.6238e-02,  7.6553e-02,  5.0442e-02,\n","           5.0132e-02, -5.3918e-02,  4.8507e-02, -6.6797e-02,  5.7735e-02,\n","          -4.5359e-02,  1.9322e-02,  1.9248e-03, -2.8235e-02,  5.6912e-02,\n","           5.5266e-02,  6.5678e-02, -5.2365e-02, -7.6187e-02,  5.8298e-02,\n","          -4.5155e-02,  4.0944e-02, -4.7766e-02,  3.9520e-02, -3.8202e-02,\n","           5.9508e-02,  6.5575e-02,  5.0223e-02, -5.2646e-02,  6.5225e-02,\n","           6.0616e-02, -5.7225e-02, -7.0720e-02,  5.4146e-02,  4.4600e-02,\n","          -7.1791e-02,  6.3719e-02, -5.4692e-02,  5.0825e-02, -6.4374e-02,\n","           1.9783e-02,  4.2271e-03,  3.6434e-02,  5.0675e-02, -5.9024e-02,\n","           5.9525e-02, -4.9935e-02, -4.4085e-02, -6.2678e-02,  6.5757e-02,\n","          -1.8078e-02, -5.3334e-02, -1.4313e-03, -7.2129e-02,  6.3208e-02,\n","          -3.3814e-05, -2.0235e-02, -3.3285e-02, -5.1631e-02, -6.3981e-02,\n","           1.6216e-02, -6.7399e-02,  4.0508e-02,  7.0239e-02, -6.6170e-02,\n","           5.7138e-02, -5.1913e-02,  6.0877e-02, -4.9801e-02, -6.7204e-02,\n","          -6.4788e-02,  4.9598e-02, -7.4034e-02,  4.6648e-02,  6.1389e-02,\n","           6.8481e-02, -5.9782e-02,  6.4671e-02,  7.0083e-02, -5.1349e-02,\n","          -6.3497e-02,  7.4574e-02,  5.6269e-02,  9.2651e-03, -5.6422e-02,\n","          -5.2579e-02,  6.9048e-02, -6.0976e-02, -6.1448e-02,  5.1696e-02,\n","           5.0576e-02, -5.3583e-02,  6.7496e-02,  4.4757e-02, -6.3283e-02,\n","          -6.2441e-02,  3.3340e-02,  5.5669e-02,  7.2854e-02,  6.7949e-02,\n","           6.7263e-02,  6.8168e-02, -6.2099e-02,  6.3337e-02,  6.7687e-02,\n","          -3.3015e-02,  6.3598e-02,  5.6996e-02, -2.7308e-02, -4.9942e-02,\n","           4.1471e-02,  2.3487e-02, -5.4721e-02, -1.3885e-02, -4.6404e-02,\n","          -4.4931e-02, -1.8106e-02,  6.6396e-02, -4.2820e-02, -5.3068e-02,\n","          -5.3193e-02,  1.0452e-03,  6.8466e-02, -6.2379e-02,  7.2321e-02,\n","           2.8609e-02,  5.2114e-02, -2.1516e-02,  5.2208e-02,  1.8943e-02,\n","           2.3708e-02,  1.5414e-02, -5.8427e-02,  5.2172e-02,  6.4962e-02,\n","          -5.4069e-02,  6.0671e-02,  6.1795e-02, -6.3784e-02, -5.4128e-02,\n","          -3.2953e-02,  4.9510e-02]]], device='cuda:0',\n","       grad_fn=<CudnnRnnBackward0>)\n","hidden\n","shape :  torch.Size([1, 1, 512])\n","tensor([[[ 6.1138e-02, -8.0812e-02,  5.3113e-02,  1.5207e-02, -5.2671e-02,\n","          -5.8849e-02,  6.1591e-02, -3.8525e-02,  6.2430e-02, -7.5894e-03,\n","          -6.7624e-02, -9.1472e-03, -4.5666e-02,  4.7232e-02,  5.1687e-02,\n","          -6.3880e-02, -2.5386e-02, -5.8809e-02,  4.4934e-02,  7.5901e-03,\n","           7.0818e-02, -6.3418e-02, -5.4012e-02, -6.0039e-02, -5.0655e-02,\n","          -3.8728e-02,  3.8875e-02, -4.8767e-02, -5.3663e-02,  6.2145e-02,\n","           5.9513e-02,  5.7120e-02,  4.9017e-02, -4.0272e-02, -6.6889e-02,\n","          -4.9852e-02,  5.0200e-02,  6.3161e-02, -6.9964e-02,  7.2658e-02,\n","          -6.2221e-02,  5.7768e-02,  5.0979e-02,  4.7315e-02, -2.6386e-02,\n","          -4.1745e-02, -6.9128e-02,  5.1389e-02, -4.4314e-02, -3.9957e-02,\n","          -5.5035e-02,  5.6005e-02,  4.6168e-02,  4.1692e-02,  6.5803e-02,\n","          -6.1550e-02, -7.0899e-02,  5.4185e-02,  3.2702e-03, -3.0068e-02,\n","          -5.1623e-02, -6.1795e-02, -6.5970e-02, -6.4569e-02,  5.9108e-02,\n","          -3.9744e-02, -4.7905e-02, -2.4844e-02,  5.8074e-02, -6.0769e-02,\n","           5.7378e-02, -4.9587e-02, -4.5312e-02,  5.0536e-02,  6.4591e-02,\n","          -2.1447e-02,  1.0460e-02, -5.0058e-02, -4.8705e-02, -4.5487e-02,\n","          -4.4873e-02, -2.9356e-02,  6.3138e-02, -2.2678e-02, -6.4001e-02,\n","          -5.8854e-02, -3.2899e-02, -5.7916e-02,  6.5588e-02, -3.5914e-02,\n","           1.9473e-02,  5.2914e-02,  1.7548e-02, -5.8576e-02, -6.4002e-02,\n","          -6.0862e-02,  4.6653e-02, -6.6583e-02, -6.2982e-02, -3.4913e-03,\n","           4.8443e-02,  7.1226e-02, -4.9808e-02, -6.7015e-02,  4.9879e-02,\n","           4.3470e-02,  3.8244e-02, -6.4903e-02,  4.9175e-02,  5.6386e-02,\n","          -5.7942e-02,  6.5128e-02,  5.7793e-02, -6.1214e-02, -1.2041e-02,\n","          -2.2867e-02, -2.0370e-02,  2.0089e-02,  5.3632e-02,  5.5117e-02,\n","          -6.1926e-02,  6.3715e-02,  6.3465e-03,  5.3064e-02,  2.0494e-02,\n","          -7.5072e-02, -5.5360e-02,  5.7531e-02,  6.9202e-02,  5.3569e-02,\n","          -3.1903e-02, -5.1265e-02, -6.7466e-02,  5.5420e-02, -6.2962e-02,\n","          -6.0572e-02, -7.0589e-02,  6.1650e-02, -4.5826e-02,  4.8922e-02,\n","          -4.4029e-02, -6.9406e-02,  4.7901e-02, -1.5235e-02,  5.7229e-02,\n","           5.6981e-02,  4.9751e-02, -2.3148e-02,  6.8495e-02,  5.9677e-02,\n","          -6.5583e-02, -3.7218e-02, -5.9242e-02, -5.8728e-02, -3.0249e-02,\n","           5.4056e-02, -6.5120e-02,  4.4592e-02,  4.7687e-02, -4.8918e-02,\n","          -5.0304e-02,  6.3491e-02, -1.5597e-02,  6.0987e-02,  6.4380e-02,\n","           6.2287e-02, -6.3782e-02,  5.7920e-02, -6.1602e-02,  6.5811e-02,\n","          -2.8638e-02,  6.1632e-02, -2.7002e-02,  2.2811e-02, -5.3841e-02,\n","          -3.3735e-02,  5.6722e-02, -6.6019e-02, -6.8930e-02, -4.9788e-02,\n","          -7.1732e-02, -2.4200e-02, -5.6368e-02,  6.1158e-03,  4.9712e-02,\n","           5.8895e-02,  6.3575e-02,  4.7756e-02, -6.9251e-02,  5.5005e-02,\n","           5.3195e-02, -2.8078e-02, -5.6401e-02,  6.3882e-02,  6.8529e-02,\n","           5.5214e-02, -6.4516e-02,  4.8201e-02, -5.2615e-02,  5.4039e-02,\n","           7.2761e-02,  5.7430e-02,  1.1096e-02, -7.1478e-02,  5.0930e-02,\n","           5.6415e-02, -5.3763e-02,  5.9652e-02,  9.6159e-03,  5.4627e-02,\n","          -6.1654e-02,  6.4116e-02, -6.5080e-02, -6.3803e-02,  4.9292e-03,\n","          -6.1671e-02,  5.9444e-02,  4.8488e-02,  7.0131e-02, -5.5965e-02,\n","           6.1574e-02, -1.7515e-03, -5.4623e-02, -6.8278e-02, -6.6406e-02,\n","          -6.1089e-02, -6.1579e-02, -5.9862e-02, -2.1410e-02,  6.2289e-02,\n","           4.5336e-02, -1.2801e-02,  5.9452e-02,  5.9698e-02, -7.7257e-02,\n","          -1.7779e-03,  6.8200e-02,  1.9489e-02,  6.5087e-02,  5.1853e-02,\n","           5.8357e-02, -5.4721e-02,  5.7839e-02, -4.8183e-02,  6.6983e-02,\n","           5.9139e-02, -6.9004e-02, -5.9448e-02,  4.1896e-02,  5.9069e-02,\n","           4.0065e-02, -5.5692e-02,  2.0893e-02,  6.3289e-02, -5.3045e-02,\n","           6.1336e-02,  6.1302e-02, -4.1553e-02, -6.7209e-02,  5.2571e-02,\n","          -5.8021e-02,  2.2362e-02,  5.0967e-02, -7.0744e-02, -6.0586e-02,\n","           3.8113e-02,  6.4523e-02,  5.9871e-02,  3.8693e-02, -6.3575e-02,\n","          -6.3373e-02,  9.1588e-03, -7.6446e-03, -5.7116e-02, -6.9998e-02,\n","          -6.4836e-02, -6.4556e-02,  6.5063e-02, -3.4534e-02, -1.0003e-02,\n","          -5.9985e-02,  4.9271e-02,  5.6056e-02, -3.8430e-02,  5.8736e-02,\n","           5.3118e-02,  4.2736e-02, -6.1329e-02,  5.1209e-02,  6.1136e-02,\n","           5.2676e-02,  4.2534e-02,  6.6478e-02,  5.5144e-02,  4.7678e-02,\n","          -6.2453e-02,  6.9159e-02,  2.6711e-02,  3.1286e-02, -3.4934e-02,\n","           4.4607e-02,  3.6747e-02, -4.4982e-02, -6.5146e-02,  5.6813e-02,\n","           6.6353e-02,  5.0855e-02,  5.1127e-02, -3.6087e-02,  6.1580e-02,\n","           5.0946e-02, -6.7481e-02,  5.7721e-02, -6.2637e-02,  6.1819e-02,\n","          -6.0817e-02, -6.7178e-02, -3.7565e-02, -6.5620e-02, -6.4283e-02,\n","           7.2717e-02, -5.9949e-02, -5.6813e-02, -6.5826e-02, -6.2264e-02,\n","           3.8238e-02,  4.2579e-03,  6.8096e-02,  5.9558e-02,  6.0094e-02,\n","           6.4741e-02, -4.9217e-02,  4.8855e-02, -6.6973e-02, -4.0333e-02,\n","           5.5701e-02,  6.2151e-02,  5.0477e-02,  8.1305e-02, -6.5448e-02,\n","           6.7367e-02, -4.6008e-02, -5.4037e-02,  2.9515e-02, -7.8871e-03,\n","           7.3764e-02,  7.0673e-02,  5.0692e-02, -4.6842e-02, -6.0280e-02,\n","           6.6220e-02,  6.2853e-02,  6.2141e-02, -2.2073e-02,  6.9005e-02,\n","          -1.5707e-02,  7.2690e-02, -7.0655e-02, -5.5678e-02, -4.8617e-02,\n","           6.6989e-02,  6.4710e-02, -8.6251e-03,  3.2813e-02, -6.5020e-02,\n","          -5.8382e-02,  5.8725e-02,  5.4349e-02, -6.3639e-02, -6.1789e-02,\n","           6.0939e-02, -6.6624e-02, -7.6238e-02,  7.6553e-02,  5.0442e-02,\n","           5.0132e-02, -5.3918e-02,  4.8507e-02, -6.6797e-02,  5.7735e-02,\n","          -4.5359e-02,  1.9322e-02,  1.9248e-03, -2.8235e-02,  5.6912e-02,\n","           5.5266e-02,  6.5678e-02, -5.2365e-02, -7.6187e-02,  5.8298e-02,\n","          -4.5155e-02,  4.0944e-02, -4.7766e-02,  3.9520e-02, -3.8202e-02,\n","           5.9508e-02,  6.5575e-02,  5.0223e-02, -5.2646e-02,  6.5225e-02,\n","           6.0616e-02, -5.7225e-02, -7.0720e-02,  5.4146e-02,  4.4600e-02,\n","          -7.1791e-02,  6.3719e-02, -5.4692e-02,  5.0825e-02, -6.4374e-02,\n","           1.9783e-02,  4.2271e-03,  3.6434e-02,  5.0675e-02, -5.9024e-02,\n","           5.9525e-02, -4.9935e-02, -4.4085e-02, -6.2678e-02,  6.5757e-02,\n","          -1.8078e-02, -5.3334e-02, -1.4313e-03, -7.2129e-02,  6.3208e-02,\n","          -3.3814e-05, -2.0235e-02, -3.3285e-02, -5.1631e-02, -6.3981e-02,\n","           1.6216e-02, -6.7399e-02,  4.0508e-02,  7.0239e-02, -6.6170e-02,\n","           5.7138e-02, -5.1913e-02,  6.0877e-02, -4.9801e-02, -6.7204e-02,\n","          -6.4788e-02,  4.9598e-02, -7.4034e-02,  4.6648e-02,  6.1389e-02,\n","           6.8481e-02, -5.9782e-02,  6.4671e-02,  7.0083e-02, -5.1349e-02,\n","          -6.3497e-02,  7.4574e-02,  5.6269e-02,  9.2651e-03, -5.6422e-02,\n","          -5.2579e-02,  6.9048e-02, -6.0976e-02, -6.1448e-02,  5.1696e-02,\n","           5.0576e-02, -5.3583e-02,  6.7496e-02,  4.4757e-02, -6.3283e-02,\n","          -6.2441e-02,  3.3340e-02,  5.5669e-02,  7.2854e-02,  6.7949e-02,\n","           6.7263e-02,  6.8168e-02, -6.2099e-02,  6.3337e-02,  6.7687e-02,\n","          -3.3015e-02,  6.3598e-02,  5.6996e-02, -2.7308e-02, -4.9942e-02,\n","           4.1471e-02,  2.3487e-02, -5.4721e-02, -1.3885e-02, -4.6404e-02,\n","          -4.4931e-02, -1.8106e-02,  6.6396e-02, -4.2820e-02, -5.3068e-02,\n","          -5.3193e-02,  1.0452e-03,  6.8466e-02, -6.2379e-02,  7.2321e-02,\n","           2.8609e-02,  5.2114e-02, -2.1516e-02,  5.2208e-02,  1.8943e-02,\n","           2.3708e-02,  1.5414e-02, -5.8427e-02,  5.2172e-02,  6.4962e-02,\n","          -5.4069e-02,  6.0671e-02,  6.1795e-02, -6.3784e-02, -5.4128e-02,\n","          -3.2953e-02,  4.9510e-02]]], device='cuda:0',\n","       grad_fn=<CudnnRnnBackward0>)\n","prediction\n","shape :  torch.Size([1, 5893])\n","tensor([[ 0.1094, -0.0924, -0.0928,  ..., -0.1036, -0.0946, -0.0913]],\n","       device='cuda:0', grad_fn=<AddmmBackward0>)\n","input\n","shape :  torch.Size([1, 1])\n","tensor([[4]], device='cuda:0')\n","embedded\n","shape :  torch.Size([1, 1, 256])\n","tensor([[[ 0.0000,  0.0346,  0.0333,  0.0301,  0.0244,  0.0214,  0.0058,\n","           0.0000,  0.0000, -0.0012,  0.0092,  0.0067,  0.0044,  0.0000,\n","           0.0000, -0.0094,  0.0127,  0.0000,  0.0236, -0.0102,  0.0028,\n","           0.0009, -0.0072, -0.0123, -0.0012,  0.0000,  0.0000, -0.0175,\n","           0.0000,  0.0000,  0.0032,  0.0000,  0.0185,  0.0355,  0.0000,\n","           0.0000, -0.0344,  0.0000,  0.0185, -0.0345,  0.0038,  0.0000,\n","          -0.0534,  0.0000,  0.0000,  0.0016,  0.0034,  0.0413,  0.0022,\n","          -0.0134,  0.0499,  0.0016, -0.0173,  0.0000,  0.0263,  0.0096,\n","           0.0246, -0.0009,  0.0000, -0.0178, -0.0282, -0.0220,  0.0000,\n","           0.0000, -0.0262, -0.0278,  0.0000, -0.0172, -0.0042,  0.0000,\n","           0.0000,  0.0262,  0.0290,  0.0051, -0.0160, -0.0192,  0.0212,\n","           0.0000, -0.0126,  0.0000, -0.0090,  0.0000,  0.0000,  0.0000,\n","           0.0000,  0.0355, -0.0087,  0.0150,  0.0000, -0.0093,  0.0000,\n","           0.0137, -0.0240,  0.0000, -0.0084,  0.0000,  0.0000,  0.0000,\n","          -0.0054,  0.0000,  0.0000,  0.0022,  0.0000, -0.0010,  0.0000,\n","           0.0000,  0.0000,  0.0421,  0.0000,  0.0000,  0.0077,  0.0000,\n","          -0.0370,  0.0181,  0.0000,  0.0278, -0.0076,  0.0002,  0.0096,\n","           0.0256,  0.0000,  0.0000,  0.0000, -0.0187,  0.0000,  0.0000,\n","           0.0000, -0.0283,  0.0000,  0.0000,  0.0313,  0.0000,  0.0000,\n","           0.0356,  0.0168,  0.0000, -0.0146,  0.0000,  0.0039,  0.0000,\n","          -0.0231,  0.0000,  0.0328,  0.0117,  0.0000,  0.0166,  0.0000,\n","          -0.0094,  0.0000,  0.0070, -0.0040,  0.0000,  0.0176,  0.0476,\n","           0.0118, -0.0056,  0.0000, -0.0086,  0.0000,  0.0000,  0.0263,\n","           0.0000,  0.0000,  0.0000, -0.0339, -0.0243,  0.0000,  0.0000,\n","           0.0000,  0.0000,  0.0000, -0.0133, -0.0011,  0.0000, -0.0154,\n","           0.0153,  0.0000,  0.0426,  0.0000, -0.0393,  0.0106,  0.0275,\n","           0.0000,  0.0160,  0.0020,  0.0282,  0.0144,  0.0000, -0.0244,\n","           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0059,  0.0000,\n","           0.0000,  0.0000,  0.0000,  0.0000,  0.0224,  0.0000,  0.0000,\n","           0.0135,  0.0000,  0.0000,  0.0016,  0.0000,  0.0000,  0.0000,\n","           0.0000,  0.0000,  0.0130,  0.0000,  0.0000,  0.0000,  0.0005,\n","           0.0000,  0.0238,  0.0000, -0.0149,  0.0000, -0.0110, -0.0139,\n","           0.0274,  0.0000, -0.0066,  0.0000,  0.0182,  0.0000,  0.0000,\n","          -0.0094,  0.0133, -0.0034,  0.0000,  0.0167,  0.0000,  0.0000,\n","           0.0000,  0.0195,  0.0000,  0.0000,  0.0000, -0.0113,  0.0000,\n","           0.0353, -0.0195, -0.0002,  0.0000,  0.0000,  0.0000,  0.0000,\n","           0.0000,  0.0000,  0.0000,  0.0128]]], device='cuda:0',\n","       grad_fn=<FusedDropoutBackward0>)\n","hidden :  torch.Size([1, 512])\n","hidden_unsqueeze:  torch.Size([1, 17, 512])\n","encoder_outputs :  torch.Size([17, 1, 1024])\n","encoder_outputs_permute :  torch.Size([1, 17, 1024])\n","energy\n","shape :  torch.Size([1, 17, 512])\n","tensor([[[-0.0242,  0.0348, -0.0056,  ...,  0.0278, -0.0046,  0.0217],\n","         [-0.0254,  0.0385, -0.0074,  ...,  0.0306, -0.0032,  0.0237],\n","         [-0.0250,  0.0389, -0.0089,  ...,  0.0319, -0.0007,  0.0251],\n","         ...,\n","         [-0.0243,  0.0375, -0.0050,  ...,  0.0273, -0.0014,  0.0222],\n","         [-0.0239,  0.0378, -0.0059,  ...,  0.0270, -0.0007,  0.0218],\n","         [-0.0228,  0.0353, -0.0040,  ...,  0.0241, -0.0015,  0.0194]]],\n","       device='cuda:0', grad_fn=<TanhBackward0>)\n","attention\n","shape:  torch.Size([1, 17])\n","tensor([[0.0579, 0.0585, 0.0589, 0.0592, 0.0592, 0.0594, 0.0594, 0.0595, 0.0591,\n","         0.0589, 0.0586, 0.0590, 0.0587, 0.0587, 0.0583, 0.0585, 0.0581]],\n","       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n","a\n","shape :  torch.Size([1, 17])\n","tensor([[0.0579, 0.0585, 0.0589, 0.0592, 0.0592, 0.0594, 0.0594, 0.0595, 0.0591,\n","         0.0589, 0.0586, 0.0590, 0.0587, 0.0587, 0.0583, 0.0585, 0.0581]],\n","       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n","a_unsqueeze\n","shape :  torch.Size([1, 1, 17])\n","tensor([[[0.0579, 0.0585, 0.0589, 0.0592, 0.0592, 0.0594, 0.0594, 0.0595,\n","          0.0591, 0.0589, 0.0586, 0.0590, 0.0587, 0.0587, 0.0583, 0.0585,\n","          0.0581]]], device='cuda:0', grad_fn=<UnsqueezeBackward0>)\n","weighted\n","shape :  torch.Size([1, 1, 1024])\n","tensor([[[ 0.0015,  0.0127, -0.0103,  ..., -0.0120, -0.0075,  0.0108]]],\n","       device='cuda:0', grad_fn=<BmmBackward0>)\n","output\n","shape :  torch.Size([1, 1, 512])\n","tensor([[[ 0.0641, -0.0841,  0.0566,  0.0169, -0.0524, -0.0612,  0.0658,\n","          -0.0356,  0.0646, -0.0072, -0.0693, -0.0085, -0.0512,  0.0486,\n","           0.0532, -0.0679, -0.0299, -0.0610,  0.0467,  0.0093,  0.0723,\n","          -0.0670, -0.0540, -0.0625, -0.0543, -0.0399,  0.0396, -0.0503,\n","          -0.0557,  0.0626,  0.0610,  0.0607,  0.0522, -0.0407, -0.0701,\n","          -0.0531,  0.0547,  0.0659, -0.0680,  0.0765, -0.0662,  0.0607,\n","           0.0538,  0.0483, -0.0278, -0.0418, -0.0714,  0.0532, -0.0465,\n","          -0.0421, -0.0557,  0.0558,  0.0505,  0.0429,  0.0670, -0.0628,\n","          -0.0739,  0.0566,  0.0043, -0.0307, -0.0535, -0.0641, -0.0677,\n","          -0.0663,  0.0587, -0.0413, -0.0514, -0.0254,  0.0619, -0.0621,\n","           0.0587, -0.0525, -0.0470,  0.0556,  0.0676, -0.0236,  0.0138,\n","          -0.0523, -0.0516, -0.0479, -0.0474, -0.0274,  0.0645, -0.0234,\n","          -0.0661, -0.0604, -0.0313, -0.0616,  0.0672, -0.0369,  0.0223,\n","           0.0565,  0.0177, -0.0609, -0.0675, -0.0643,  0.0470, -0.0670,\n","          -0.0667, -0.0028,  0.0494,  0.0735, -0.0520, -0.0677,  0.0526,\n","           0.0450,  0.0395, -0.0652,  0.0524,  0.0570, -0.0575,  0.0663,\n","           0.0616, -0.0629, -0.0122, -0.0248, -0.0221,  0.0207,  0.0535,\n","           0.0575, -0.0638,  0.0648,  0.0103,  0.0525,  0.0216, -0.0777,\n","          -0.0578,  0.0581,  0.0719,  0.0543, -0.0347, -0.0555, -0.0731,\n","           0.0577, -0.0638, -0.0647, -0.0726,  0.0660, -0.0471,  0.0513,\n","          -0.0465, -0.0709,  0.0487, -0.0161,  0.0590,  0.0569,  0.0543,\n","          -0.0263,  0.0700,  0.0638, -0.0661, -0.0381, -0.0612, -0.0596,\n","          -0.0297,  0.0596, -0.0687,  0.0465,  0.0483, -0.0514, -0.0522,\n","           0.0649, -0.0143,  0.0622,  0.0656,  0.0645, -0.0659,  0.0598,\n","          -0.0647,  0.0679, -0.0293,  0.0641, -0.0261,  0.0218, -0.0536,\n","          -0.0375,  0.0576, -0.0670, -0.0716, -0.0533, -0.0752, -0.0240,\n","          -0.0594,  0.0053,  0.0558,  0.0607,  0.0664,  0.0481, -0.0723,\n","           0.0560,  0.0552, -0.0265, -0.0570,  0.0655,  0.0717,  0.0587,\n","          -0.0664,  0.0492, -0.0542,  0.0538,  0.0746,  0.0597,  0.0109,\n","          -0.0709,  0.0542,  0.0556, -0.0567,  0.0613,  0.0117,  0.0563,\n","          -0.0634,  0.0648, -0.0700, -0.0668,  0.0010, -0.0623,  0.0593,\n","           0.0537,  0.0726, -0.0586,  0.0653, -0.0027, -0.0570, -0.0679,\n","          -0.0675, -0.0628, -0.0616, -0.0627, -0.0217,  0.0660,  0.0486,\n","          -0.0139,  0.0615,  0.0609, -0.0813, -0.0053,  0.0706,  0.0195,\n","           0.0676,  0.0562,  0.0615, -0.0573,  0.0601, -0.0496,  0.0686,\n","           0.0618, -0.0723, -0.0611,  0.0449,  0.0650,  0.0422, -0.0587,\n","           0.0231,  0.0650, -0.0525,  0.0630,  0.0652, -0.0435, -0.0698,\n","           0.0514, -0.0608,  0.0211,  0.0524, -0.0720, -0.0611,  0.0390,\n","           0.0653,  0.0634,  0.0378, -0.0647, -0.0669,  0.0093, -0.0081,\n","          -0.0613, -0.0727, -0.0666, -0.0682,  0.0669, -0.0351, -0.0108,\n","          -0.0612,  0.0503,  0.0554, -0.0427,  0.0625,  0.0566,  0.0427,\n","          -0.0627,  0.0518,  0.0643,  0.0534,  0.0431,  0.0683,  0.0571,\n","           0.0489, -0.0655,  0.0720,  0.0281,  0.0324, -0.0364,  0.0479,\n","           0.0359, -0.0456, -0.0666,  0.0589,  0.0699,  0.0531,  0.0525,\n","          -0.0417,  0.0616,  0.0533, -0.0697,  0.0605, -0.0634,  0.0634,\n","          -0.0625, -0.0694, -0.0396, -0.0675, -0.0685,  0.0760, -0.0612,\n","          -0.0584, -0.0698, -0.0630,  0.0396,  0.0047,  0.0694,  0.0589,\n","           0.0598,  0.0662, -0.0501,  0.0510, -0.0676, -0.0397,  0.0594,\n","           0.0613,  0.0528,  0.0832, -0.0691,  0.0686, -0.0493, -0.0564,\n","           0.0308, -0.0103,  0.0737,  0.0688,  0.0535, -0.0483, -0.0613,\n","           0.0666,  0.0628,  0.0643, -0.0225,  0.0714, -0.0191,  0.0751,\n","          -0.0724, -0.0565, -0.0498,  0.0712,  0.0674, -0.0081,  0.0335,\n","          -0.0689, -0.0615,  0.0608,  0.0530, -0.0656, -0.0649,  0.0646,\n","          -0.0690, -0.0772,  0.0798,  0.0519,  0.0515, -0.0555,  0.0527,\n","          -0.0683,  0.0596, -0.0481,  0.0195,  0.0031, -0.0276,  0.0608,\n","           0.0603,  0.0655, -0.0509, -0.0791,  0.0613, -0.0446,  0.0407,\n","          -0.0469,  0.0404, -0.0408,  0.0615,  0.0654,  0.0529, -0.0561,\n","           0.0675,  0.0599, -0.0605, -0.0736,  0.0556,  0.0487, -0.0738,\n","           0.0659, -0.0567,  0.0517, -0.0657,  0.0208,  0.0035,  0.0390,\n","           0.0521, -0.0606,  0.0601, -0.0531, -0.0453, -0.0645,  0.0693,\n","          -0.0170, -0.0530, -0.0025, -0.0740,  0.0635,  0.0024, -0.0180,\n","          -0.0326, -0.0549, -0.0670,  0.0160, -0.0680,  0.0420,  0.0729,\n","          -0.0672,  0.0592, -0.0550,  0.0620, -0.0533, -0.0693, -0.0683,\n","           0.0509, -0.0742,  0.0485,  0.0626,  0.0726, -0.0613,  0.0682,\n","           0.0705, -0.0555, -0.0642,  0.0765,  0.0594,  0.0107, -0.0571,\n","          -0.0529,  0.0711, -0.0633, -0.0625,  0.0549,  0.0513, -0.0559,\n","           0.0694,  0.0440, -0.0648, -0.0643,  0.0338,  0.0567,  0.0747,\n","           0.0709,  0.0687,  0.0697, -0.0653,  0.0656,  0.0708, -0.0338,\n","           0.0654,  0.0596, -0.0290, -0.0515,  0.0403,  0.0256, -0.0591,\n","          -0.0145, -0.0490, -0.0465, -0.0214,  0.0676, -0.0470, -0.0568,\n","          -0.0539,  0.0009,  0.0706, -0.0636,  0.0754,  0.0280,  0.0516,\n","          -0.0202,  0.0547,  0.0195,  0.0250,  0.0176, -0.0604,  0.0537,\n","           0.0679, -0.0529,  0.0621,  0.0645, -0.0654, -0.0585, -0.0321,\n","           0.0511]]], device='cuda:0', grad_fn=<CudnnRnnBackward0>)\n","hidden\n","shape :  torch.Size([1, 1, 512])\n","tensor([[[ 0.0641, -0.0841,  0.0566,  0.0169, -0.0524, -0.0612,  0.0658,\n","          -0.0356,  0.0646, -0.0072, -0.0693, -0.0085, -0.0512,  0.0486,\n","           0.0532, -0.0679, -0.0299, -0.0610,  0.0467,  0.0093,  0.0723,\n","          -0.0670, -0.0540, -0.0625, -0.0543, -0.0399,  0.0396, -0.0503,\n","          -0.0557,  0.0626,  0.0610,  0.0607,  0.0522, -0.0407, -0.0701,\n","          -0.0531,  0.0547,  0.0659, -0.0680,  0.0765, -0.0662,  0.0607,\n","           0.0538,  0.0483, -0.0278, -0.0418, -0.0714,  0.0532, -0.0465,\n","          -0.0421, -0.0557,  0.0558,  0.0505,  0.0429,  0.0670, -0.0628,\n","          -0.0739,  0.0566,  0.0043, -0.0307, -0.0535, -0.0641, -0.0677,\n","          -0.0663,  0.0587, -0.0413, -0.0514, -0.0254,  0.0619, -0.0621,\n","           0.0587, -0.0525, -0.0470,  0.0556,  0.0676, -0.0236,  0.0138,\n","          -0.0523, -0.0516, -0.0479, -0.0474, -0.0274,  0.0645, -0.0234,\n","          -0.0661, -0.0604, -0.0313, -0.0616,  0.0672, -0.0369,  0.0223,\n","           0.0565,  0.0177, -0.0609, -0.0675, -0.0643,  0.0470, -0.0670,\n","          -0.0667, -0.0028,  0.0494,  0.0735, -0.0520, -0.0677,  0.0526,\n","           0.0450,  0.0395, -0.0652,  0.0524,  0.0570, -0.0575,  0.0663,\n","           0.0616, -0.0629, -0.0122, -0.0248, -0.0221,  0.0207,  0.0535,\n","           0.0575, -0.0638,  0.0648,  0.0103,  0.0525,  0.0216, -0.0777,\n","          -0.0578,  0.0581,  0.0719,  0.0543, -0.0347, -0.0555, -0.0731,\n","           0.0577, -0.0638, -0.0647, -0.0726,  0.0660, -0.0471,  0.0513,\n","          -0.0465, -0.0709,  0.0487, -0.0161,  0.0590,  0.0569,  0.0543,\n","          -0.0263,  0.0700,  0.0638, -0.0661, -0.0381, -0.0612, -0.0596,\n","          -0.0297,  0.0596, -0.0687,  0.0465,  0.0483, -0.0514, -0.0522,\n","           0.0649, -0.0143,  0.0622,  0.0656,  0.0645, -0.0659,  0.0598,\n","          -0.0647,  0.0679, -0.0293,  0.0641, -0.0261,  0.0218, -0.0536,\n","          -0.0375,  0.0576, -0.0670, -0.0716, -0.0533, -0.0752, -0.0240,\n","          -0.0594,  0.0053,  0.0558,  0.0607,  0.0664,  0.0481, -0.0723,\n","           0.0560,  0.0552, -0.0265, -0.0570,  0.0655,  0.0717,  0.0587,\n","          -0.0664,  0.0492, -0.0542,  0.0538,  0.0746,  0.0597,  0.0109,\n","          -0.0709,  0.0542,  0.0556, -0.0567,  0.0613,  0.0117,  0.0563,\n","          -0.0634,  0.0648, -0.0700, -0.0668,  0.0010, -0.0623,  0.0593,\n","           0.0537,  0.0726, -0.0586,  0.0653, -0.0027, -0.0570, -0.0679,\n","          -0.0675, -0.0628, -0.0616, -0.0627, -0.0217,  0.0660,  0.0486,\n","          -0.0139,  0.0615,  0.0609, -0.0813, -0.0053,  0.0706,  0.0195,\n","           0.0676,  0.0562,  0.0615, -0.0573,  0.0601, -0.0496,  0.0686,\n","           0.0618, -0.0723, -0.0611,  0.0449,  0.0650,  0.0422, -0.0587,\n","           0.0231,  0.0650, -0.0525,  0.0630,  0.0652, -0.0435, -0.0698,\n","           0.0514, -0.0608,  0.0211,  0.0524, -0.0720, -0.0611,  0.0390,\n","           0.0653,  0.0634,  0.0378, -0.0647, -0.0669,  0.0093, -0.0081,\n","          -0.0613, -0.0727, -0.0666, -0.0682,  0.0669, -0.0351, -0.0108,\n","          -0.0612,  0.0503,  0.0554, -0.0427,  0.0625,  0.0566,  0.0427,\n","          -0.0627,  0.0518,  0.0643,  0.0534,  0.0431,  0.0683,  0.0571,\n","           0.0489, -0.0655,  0.0720,  0.0281,  0.0324, -0.0364,  0.0479,\n","           0.0359, -0.0456, -0.0666,  0.0589,  0.0699,  0.0531,  0.0525,\n","          -0.0417,  0.0616,  0.0533, -0.0697,  0.0605, -0.0634,  0.0634,\n","          -0.0625, -0.0694, -0.0396, -0.0675, -0.0685,  0.0760, -0.0612,\n","          -0.0584, -0.0698, -0.0630,  0.0396,  0.0047,  0.0694,  0.0589,\n","           0.0598,  0.0662, -0.0501,  0.0510, -0.0676, -0.0397,  0.0594,\n","           0.0613,  0.0528,  0.0832, -0.0691,  0.0686, -0.0493, -0.0564,\n","           0.0308, -0.0103,  0.0737,  0.0688,  0.0535, -0.0483, -0.0613,\n","           0.0666,  0.0628,  0.0643, -0.0225,  0.0714, -0.0191,  0.0751,\n","          -0.0724, -0.0565, -0.0498,  0.0712,  0.0674, -0.0081,  0.0335,\n","          -0.0689, -0.0615,  0.0608,  0.0530, -0.0656, -0.0649,  0.0646,\n","          -0.0690, -0.0772,  0.0798,  0.0519,  0.0515, -0.0555,  0.0527,\n","          -0.0683,  0.0596, -0.0481,  0.0195,  0.0031, -0.0276,  0.0608,\n","           0.0603,  0.0655, -0.0509, -0.0791,  0.0613, -0.0446,  0.0407,\n","          -0.0469,  0.0404, -0.0408,  0.0615,  0.0654,  0.0529, -0.0561,\n","           0.0675,  0.0599, -0.0605, -0.0736,  0.0556,  0.0487, -0.0738,\n","           0.0659, -0.0567,  0.0517, -0.0657,  0.0208,  0.0035,  0.0390,\n","           0.0521, -0.0606,  0.0601, -0.0531, -0.0453, -0.0645,  0.0693,\n","          -0.0170, -0.0530, -0.0025, -0.0740,  0.0635,  0.0024, -0.0180,\n","          -0.0326, -0.0549, -0.0670,  0.0160, -0.0680,  0.0420,  0.0729,\n","          -0.0672,  0.0592, -0.0550,  0.0620, -0.0533, -0.0693, -0.0683,\n","           0.0509, -0.0742,  0.0485,  0.0626,  0.0726, -0.0613,  0.0682,\n","           0.0705, -0.0555, -0.0642,  0.0765,  0.0594,  0.0107, -0.0571,\n","          -0.0529,  0.0711, -0.0633, -0.0625,  0.0549,  0.0513, -0.0559,\n","           0.0694,  0.0440, -0.0648, -0.0643,  0.0338,  0.0567,  0.0747,\n","           0.0709,  0.0687,  0.0697, -0.0653,  0.0656,  0.0708, -0.0338,\n","           0.0654,  0.0596, -0.0290, -0.0515,  0.0403,  0.0256, -0.0591,\n","          -0.0145, -0.0490, -0.0465, -0.0214,  0.0676, -0.0470, -0.0568,\n","          -0.0539,  0.0009,  0.0706, -0.0636,  0.0754,  0.0280,  0.0516,\n","          -0.0202,  0.0547,  0.0195,  0.0250,  0.0176, -0.0604,  0.0537,\n","           0.0679, -0.0529,  0.0621,  0.0645, -0.0654, -0.0585, -0.0321,\n","           0.0511]]], device='cuda:0', grad_fn=<CudnnRnnBackward0>)\n","prediction\n","shape :  torch.Size([1, 5893])\n","tensor([[ 0.1149, -0.0992, -0.0970,  ..., -0.1058, -0.1010, -0.1018]],\n","       device='cuda:0', grad_fn=<AddmmBackward0>)\n","input\n","shape :  torch.Size([1, 1])\n","tensor([[4]], device='cuda:0')\n","embedded\n","shape :  torch.Size([1, 1, 256])\n","tensor([[[ 0.0000e+00,  3.4645e-02,  0.0000e+00,  0.0000e+00,  2.4405e-02,\n","           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.1846e-03,\n","           9.1595e-03,  6.7084e-03,  4.3580e-03,  2.6623e-02,  3.5461e-02,\n","          -9.3843e-03,  0.0000e+00,  2.2573e-02,  0.0000e+00, -1.0166e-02,\n","           2.7931e-03,  0.0000e+00, -7.1932e-03,  0.0000e+00, -1.1886e-03,\n","          -3.9367e-02,  1.2866e-03, -1.7484e-02,  0.0000e+00, -4.5220e-03,\n","           0.0000e+00, -7.6678e-03,  0.0000e+00,  3.5479e-02,  0.0000e+00,\n","           0.0000e+00,  0.0000e+00,  8.7471e-03,  0.0000e+00, -3.4529e-02,\n","           3.8271e-03,  0.0000e+00, -5.3435e-02,  0.0000e+00,  0.0000e+00,\n","           1.6476e-03,  0.0000e+00,  0.0000e+00,  2.1839e-03, -1.3377e-02,\n","           4.9885e-02,  1.5593e-03,  0.0000e+00,  1.5164e-02,  0.0000e+00,\n","           9.6168e-03,  2.4577e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n","          -2.8231e-02,  0.0000e+00,  0.0000e+00,  1.8052e-03,  0.0000e+00,\n","          -2.7826e-02,  0.0000e+00,  0.0000e+00, -4.2019e-03,  1.0590e-02,\n","          -1.6763e-02,  0.0000e+00,  2.9043e-02,  0.0000e+00,  0.0000e+00,\n","           0.0000e+00,  2.1168e-02,  0.0000e+00,  0.0000e+00, -1.1366e-02,\n","           0.0000e+00,  0.0000e+00,  2.2715e-02,  0.0000e+00,  0.0000e+00,\n","           0.0000e+00, -8.7161e-03,  0.0000e+00,  1.7465e-02,  0.0000e+00,\n","           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n","           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  4.2007e-02,\n","           1.5936e-02,  2.1854e-03, -1.3909e-02,  0.0000e+00,  1.6670e-02,\n","           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  5.6076e-03,\n","           7.6755e-03,  0.0000e+00,  0.0000e+00,  1.8093e-02,  0.0000e+00,\n","           2.7840e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  2.5596e-02,\n","          -8.8254e-03, -5.5914e-03, -2.0376e-02, -1.8731e-02,  0.0000e+00,\n","           3.0832e-02,  2.2462e-02,  0.0000e+00,  0.0000e+00,  2.8394e-02,\n","           0.0000e+00,  1.8211e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n","           0.0000e+00, -1.4558e-02, -2.6115e-02,  3.8957e-03,  0.0000e+00,\n","          -2.3086e-02, -2.5836e-02,  0.0000e+00,  1.1692e-02, -1.7256e-02,\n","           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n","          -4.0073e-03,  0.0000e+00,  0.0000e+00,  4.7561e-02,  1.1780e-02,\n","           0.0000e+00, -3.4060e-03,  0.0000e+00,  0.0000e+00, -2.6328e-02,\n","           2.6258e-02, -5.4319e-03,  0.0000e+00,  0.0000e+00, -3.3899e-02,\n","           0.0000e+00,  0.0000e+00, -7.3450e-03,  0.0000e+00,  0.0000e+00,\n","           0.0000e+00, -1.3256e-02, -1.1072e-03,  0.0000e+00, -1.5395e-02,\n","           0.0000e+00, -1.6279e-02,  0.0000e+00, -3.3425e-05,  0.0000e+00,\n","           1.0634e-02,  0.0000e+00,  1.4568e-02,  1.5970e-02,  2.0003e-03,\n","           2.8194e-02,  0.0000e+00,  7.3313e-03, -2.4396e-02,  0.0000e+00,\n","           0.0000e+00,  1.0814e-02,  1.8444e-02,  7.9835e-03,  0.0000e+00,\n","           6.8639e-03,  0.0000e+00,  4.3529e-02, -1.5369e-03, -1.5033e-02,\n","           2.2354e-02,  0.0000e+00,  0.0000e+00,  1.3459e-02,  0.0000e+00,\n","           0.0000e+00,  1.5733e-03,  8.0419e-05,  0.0000e+00,  0.0000e+00,\n","          -1.1032e-02,  0.0000e+00,  1.2975e-02,  0.0000e+00,  4.9258e-03,\n","          -1.1707e-02,  0.0000e+00, -1.3479e-02,  2.3792e-02,  0.0000e+00,\n","          -1.4910e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n","           0.0000e+00, -6.5817e-03,  9.2606e-03,  1.8188e-02,  0.0000e+00,\n","           0.0000e+00, -9.3985e-03,  0.0000e+00, -3.4385e-03,  0.0000e+00,\n","           0.0000e+00, -1.7875e-02,  4.5340e-03, -2.0020e-02,  0.0000e+00,\n","           0.0000e+00,  2.4676e-02,  0.0000e+00, -1.1276e-02, -2.8979e-03,\n","           3.5272e-02, -1.9455e-02,  0.0000e+00,  1.3218e-02,  0.0000e+00,\n","          -9.4923e-03,  0.0000e+00,  0.0000e+00, -2.5812e-02,  0.0000e+00,\n","           0.0000e+00]]], device='cuda:0', grad_fn=<FusedDropoutBackward0>)\n","hidden :  torch.Size([1, 512])\n","hidden_unsqueeze:  torch.Size([1, 17, 512])\n","encoder_outputs :  torch.Size([17, 1, 1024])\n","encoder_outputs_permute :  torch.Size([1, 17, 1024])\n","energy\n","shape :  torch.Size([1, 17, 512])\n","tensor([[[-0.0247,  0.0353, -0.0058,  ...,  0.0284, -0.0049,  0.0220],\n","         [-0.0259,  0.0390, -0.0077,  ...,  0.0312, -0.0034,  0.0241],\n","         [-0.0255,  0.0394, -0.0091,  ...,  0.0326, -0.0009,  0.0255],\n","         ...,\n","         [-0.0248,  0.0381, -0.0052,  ...,  0.0279, -0.0016,  0.0226],\n","         [-0.0245,  0.0383, -0.0062,  ...,  0.0277, -0.0010,  0.0222],\n","         [-0.0233,  0.0359, -0.0043,  ...,  0.0248, -0.0018,  0.0198]]],\n","       device='cuda:0', grad_fn=<TanhBackward0>)\n","attention\n","shape:  torch.Size([1, 17])\n","tensor([[0.0579, 0.0585, 0.0589, 0.0592, 0.0592, 0.0594, 0.0594, 0.0595, 0.0591,\n","         0.0589, 0.0586, 0.0590, 0.0587, 0.0587, 0.0583, 0.0585, 0.0581]],\n","       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n","a\n","shape :  torch.Size([1, 17])\n","tensor([[0.0579, 0.0585, 0.0589, 0.0592, 0.0592, 0.0594, 0.0594, 0.0595, 0.0591,\n","         0.0589, 0.0586, 0.0590, 0.0587, 0.0587, 0.0583, 0.0585, 0.0581]],\n","       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n","a_unsqueeze\n","shape :  torch.Size([1, 1, 17])\n","tensor([[[0.0579, 0.0585, 0.0589, 0.0592, 0.0592, 0.0594, 0.0594, 0.0595,\n","          0.0591, 0.0589, 0.0586, 0.0590, 0.0587, 0.0587, 0.0583, 0.0585,\n","          0.0581]]], device='cuda:0', grad_fn=<UnsqueezeBackward0>)\n","weighted\n","shape :  torch.Size([1, 1, 1024])\n","tensor([[[ 0.0015,  0.0127, -0.0103,  ..., -0.0120, -0.0075,  0.0108]]],\n","       device='cuda:0', grad_fn=<BmmBackward0>)\n","output\n","shape :  torch.Size([1, 1, 512])\n","tensor([[[ 0.0647, -0.0842,  0.0578,  0.0181, -0.0548, -0.0622,  0.0688,\n","          -0.0370,  0.0658, -0.0080, -0.0696, -0.0086, -0.0533,  0.0500,\n","           0.0523, -0.0685, -0.0311, -0.0639,  0.0469,  0.0100,  0.0733,\n","          -0.0677, -0.0538, -0.0661, -0.0558, -0.0414,  0.0398, -0.0514,\n","          -0.0574,  0.0622,  0.0623,  0.0628,  0.0529, -0.0412, -0.0694,\n","          -0.0514,  0.0561,  0.0664, -0.0689,  0.0797, -0.0671,  0.0610,\n","           0.0557,  0.0490, -0.0274, -0.0425, -0.0735,  0.0538, -0.0477,\n","          -0.0419, -0.0561,  0.0567,  0.0519,  0.0453,  0.0686, -0.0642,\n","          -0.0744,  0.0588,  0.0051, -0.0316, -0.0538, -0.0661, -0.0686,\n","          -0.0658,  0.0608, -0.0415, -0.0535, -0.0250,  0.0642, -0.0616,\n","           0.0603, -0.0541, -0.0489,  0.0572,  0.0688, -0.0237,  0.0126,\n","          -0.0562, -0.0522, -0.0492, -0.0482, -0.0290,  0.0653, -0.0245,\n","          -0.0677, -0.0620, -0.0303, -0.0635,  0.0702, -0.0379,  0.0216,\n","           0.0567,  0.0184, -0.0609, -0.0688, -0.0654,  0.0473, -0.0690,\n","          -0.0677, -0.0041,  0.0513,  0.0742, -0.0534, -0.0697,  0.0534,\n","           0.0470,  0.0416, -0.0660,  0.0537,  0.0582, -0.0592,  0.0676,\n","           0.0627, -0.0623, -0.0127, -0.0247, -0.0240,  0.0214,  0.0544,\n","           0.0612, -0.0657,  0.0648,  0.0120,  0.0531,  0.0233, -0.0806,\n","          -0.0590,  0.0606,  0.0734,  0.0564, -0.0346, -0.0577, -0.0735,\n","           0.0612, -0.0659, -0.0667, -0.0750,  0.0678, -0.0474,  0.0544,\n","          -0.0477, -0.0726,  0.0502, -0.0149,  0.0606,  0.0576,  0.0553,\n","          -0.0267,  0.0712,  0.0643, -0.0674, -0.0388, -0.0633, -0.0616,\n","          -0.0307,  0.0613, -0.0700,  0.0475,  0.0497, -0.0527, -0.0523,\n","           0.0659, -0.0136,  0.0643,  0.0675,  0.0662, -0.0666,  0.0611,\n","          -0.0669,  0.0687, -0.0302,  0.0640, -0.0276,  0.0209, -0.0549,\n","          -0.0390,  0.0596, -0.0707, -0.0728, -0.0542, -0.0765, -0.0253,\n","          -0.0600,  0.0044,  0.0583,  0.0615,  0.0681,  0.0515, -0.0740,\n","           0.0566,  0.0568, -0.0285, -0.0568,  0.0677,  0.0719,  0.0588,\n","          -0.0671,  0.0484, -0.0543,  0.0536,  0.0772,  0.0617,  0.0109,\n","          -0.0711,  0.0546,  0.0577, -0.0565,  0.0624,  0.0108,  0.0567,\n","          -0.0633,  0.0667, -0.0718, -0.0679,  0.0009, -0.0645,  0.0587,\n","           0.0539,  0.0744, -0.0600,  0.0660, -0.0009, -0.0582, -0.0678,\n","          -0.0690, -0.0644, -0.0624, -0.0632, -0.0225,  0.0682,  0.0492,\n","          -0.0142,  0.0635,  0.0640, -0.0809, -0.0062,  0.0710,  0.0183,\n","           0.0689,  0.0568,  0.0636, -0.0592,  0.0622, -0.0500,  0.0714,\n","           0.0618, -0.0734, -0.0636,  0.0451,  0.0653,  0.0419, -0.0598,\n","           0.0254,  0.0664, -0.0530,  0.0646,  0.0664, -0.0444, -0.0709,\n","           0.0509, -0.0625,  0.0221,  0.0546, -0.0739, -0.0626,  0.0394,\n","           0.0672,  0.0652,  0.0394, -0.0664, -0.0685,  0.0090, -0.0085,\n","          -0.0659, -0.0731, -0.0695, -0.0698,  0.0682, -0.0366, -0.0101,\n","          -0.0621,  0.0514,  0.0570, -0.0432,  0.0647,  0.0574,  0.0430,\n","          -0.0634,  0.0542,  0.0639,  0.0554,  0.0440,  0.0698,  0.0582,\n","           0.0488, -0.0683,  0.0725,  0.0278,  0.0336, -0.0375,  0.0499,\n","           0.0367, -0.0472, -0.0673,  0.0594,  0.0701,  0.0544,  0.0528,\n","          -0.0420,  0.0638,  0.0542, -0.0692,  0.0618, -0.0640,  0.0648,\n","          -0.0637, -0.0695, -0.0407, -0.0676, -0.0701,  0.0771, -0.0624,\n","          -0.0589, -0.0719, -0.0630,  0.0422,  0.0051,  0.0722,  0.0603,\n","           0.0606,  0.0666, -0.0527,  0.0506, -0.0681, -0.0413,  0.0599,\n","           0.0616,  0.0549,  0.0838, -0.0694,  0.0706, -0.0502, -0.0583,\n","           0.0305, -0.0106,  0.0753,  0.0708,  0.0556, -0.0513, -0.0634,\n","           0.0664,  0.0625,  0.0664, -0.0212,  0.0706, -0.0210,  0.0752,\n","          -0.0746, -0.0566, -0.0528,  0.0730,  0.0685, -0.0086,  0.0348,\n","          -0.0695, -0.0640,  0.0618,  0.0533, -0.0682, -0.0674,  0.0663,\n","          -0.0689, -0.0794,  0.0827,  0.0536,  0.0531, -0.0560,  0.0541,\n","          -0.0692,  0.0603, -0.0497,  0.0189,  0.0021, -0.0274,  0.0621,\n","           0.0626,  0.0665, -0.0531, -0.0807,  0.0636, -0.0458,  0.0433,\n","          -0.0485,  0.0404, -0.0408,  0.0613,  0.0673,  0.0549, -0.0551,\n","           0.0688,  0.0605, -0.0616, -0.0744,  0.0555,  0.0506, -0.0753,\n","           0.0676, -0.0577,  0.0528, -0.0659,  0.0213,  0.0038,  0.0396,\n","           0.0515, -0.0599,  0.0605, -0.0530, -0.0466, -0.0651,  0.0720,\n","          -0.0181, -0.0524, -0.0036, -0.0761,  0.0654,  0.0018, -0.0190,\n","          -0.0359, -0.0568, -0.0677,  0.0161, -0.0703,  0.0434,  0.0756,\n","          -0.0694,  0.0605, -0.0575,  0.0625, -0.0550, -0.0717, -0.0712,\n","           0.0516, -0.0766,  0.0501,  0.0635,  0.0747, -0.0623,  0.0703,\n","           0.0704, -0.0570, -0.0652,  0.0765,  0.0618,  0.0112, -0.0585,\n","          -0.0531,  0.0730, -0.0634, -0.0657,  0.0560,  0.0540, -0.0573,\n","           0.0699,  0.0447, -0.0653, -0.0656,  0.0340,  0.0572,  0.0747,\n","           0.0710,  0.0706,  0.0708, -0.0665,  0.0672,  0.0709, -0.0340,\n","           0.0652,  0.0609, -0.0293, -0.0537,  0.0423,  0.0281, -0.0587,\n","          -0.0146, -0.0481, -0.0481, -0.0233,  0.0684, -0.0475, -0.0576,\n","          -0.0544, -0.0007,  0.0723, -0.0650,  0.0770,  0.0290,  0.0516,\n","          -0.0191,  0.0537,  0.0202,  0.0255,  0.0175, -0.0622,  0.0551,\n","           0.0693, -0.0540,  0.0631,  0.0656, -0.0659, -0.0594, -0.0319,\n","           0.0522]]], device='cuda:0', grad_fn=<CudnnRnnBackward0>)\n","hidden\n","shape :  torch.Size([1, 1, 512])\n","tensor([[[ 0.0647, -0.0842,  0.0578,  0.0181, -0.0548, -0.0622,  0.0688,\n","          -0.0370,  0.0658, -0.0080, -0.0696, -0.0086, -0.0533,  0.0500,\n","           0.0523, -0.0685, -0.0311, -0.0639,  0.0469,  0.0100,  0.0733,\n","          -0.0677, -0.0538, -0.0661, -0.0558, -0.0414,  0.0398, -0.0514,\n","          -0.0574,  0.0622,  0.0623,  0.0628,  0.0529, -0.0412, -0.0694,\n","          -0.0514,  0.0561,  0.0664, -0.0689,  0.0797, -0.0671,  0.0610,\n","           0.0557,  0.0490, -0.0274, -0.0425, -0.0735,  0.0538, -0.0477,\n","          -0.0419, -0.0561,  0.0567,  0.0519,  0.0453,  0.0686, -0.0642,\n","          -0.0744,  0.0588,  0.0051, -0.0316, -0.0538, -0.0661, -0.0686,\n","          -0.0658,  0.0608, -0.0415, -0.0535, -0.0250,  0.0642, -0.0616,\n","           0.0603, -0.0541, -0.0489,  0.0572,  0.0688, -0.0237,  0.0126,\n","          -0.0562, -0.0522, -0.0492, -0.0482, -0.0290,  0.0653, -0.0245,\n","          -0.0677, -0.0620, -0.0303, -0.0635,  0.0702, -0.0379,  0.0216,\n","           0.0567,  0.0184, -0.0609, -0.0688, -0.0654,  0.0473, -0.0690,\n","          -0.0677, -0.0041,  0.0513,  0.0742, -0.0534, -0.0697,  0.0534,\n","           0.0470,  0.0416, -0.0660,  0.0537,  0.0582, -0.0592,  0.0676,\n","           0.0627, -0.0623, -0.0127, -0.0247, -0.0240,  0.0214,  0.0544,\n","           0.0612, -0.0657,  0.0648,  0.0120,  0.0531,  0.0233, -0.0806,\n","          -0.0590,  0.0606,  0.0734,  0.0564, -0.0346, -0.0577, -0.0735,\n","           0.0612, -0.0659, -0.0667, -0.0750,  0.0678, -0.0474,  0.0544,\n","          -0.0477, -0.0726,  0.0502, -0.0149,  0.0606,  0.0576,  0.0553,\n","          -0.0267,  0.0712,  0.0643, -0.0674, -0.0388, -0.0633, -0.0616,\n","          -0.0307,  0.0613, -0.0700,  0.0475,  0.0497, -0.0527, -0.0523,\n","           0.0659, -0.0136,  0.0643,  0.0675,  0.0662, -0.0666,  0.0611,\n","          -0.0669,  0.0687, -0.0302,  0.0640, -0.0276,  0.0209, -0.0549,\n","          -0.0390,  0.0596, -0.0707, -0.0728, -0.0542, -0.0765, -0.0253,\n","          -0.0600,  0.0044,  0.0583,  0.0615,  0.0681,  0.0515, -0.0740,\n","           0.0566,  0.0568, -0.0285, -0.0568,  0.0677,  0.0719,  0.0588,\n","          -0.0671,  0.0484, -0.0543,  0.0536,  0.0772,  0.0617,  0.0109,\n","          -0.0711,  0.0546,  0.0577, -0.0565,  0.0624,  0.0108,  0.0567,\n","          -0.0633,  0.0667, -0.0718, -0.0679,  0.0009, -0.0645,  0.0587,\n","           0.0539,  0.0744, -0.0600,  0.0660, -0.0009, -0.0582, -0.0678,\n","          -0.0690, -0.0644, -0.0624, -0.0632, -0.0225,  0.0682,  0.0492,\n","          -0.0142,  0.0635,  0.0640, -0.0809, -0.0062,  0.0710,  0.0183,\n","           0.0689,  0.0568,  0.0636, -0.0592,  0.0622, -0.0500,  0.0714,\n","           0.0618, -0.0734, -0.0636,  0.0451,  0.0653,  0.0419, -0.0598,\n","           0.0254,  0.0664, -0.0530,  0.0646,  0.0664, -0.0444, -0.0709,\n","           0.0509, -0.0625,  0.0221,  0.0546, -0.0739, -0.0626,  0.0394,\n","           0.0672,  0.0652,  0.0394, -0.0664, -0.0685,  0.0090, -0.0085,\n","          -0.0659, -0.0731, -0.0695, -0.0698,  0.0682, -0.0366, -0.0101,\n","          -0.0621,  0.0514,  0.0570, -0.0432,  0.0647,  0.0574,  0.0430,\n","          -0.0634,  0.0542,  0.0639,  0.0554,  0.0440,  0.0698,  0.0582,\n","           0.0488, -0.0683,  0.0725,  0.0278,  0.0336, -0.0375,  0.0499,\n","           0.0367, -0.0472, -0.0673,  0.0594,  0.0701,  0.0544,  0.0528,\n","          -0.0420,  0.0638,  0.0542, -0.0692,  0.0618, -0.0640,  0.0648,\n","          -0.0637, -0.0695, -0.0407, -0.0676, -0.0701,  0.0771, -0.0624,\n","          -0.0589, -0.0719, -0.0630,  0.0422,  0.0051,  0.0722,  0.0603,\n","           0.0606,  0.0666, -0.0527,  0.0506, -0.0681, -0.0413,  0.0599,\n","           0.0616,  0.0549,  0.0838, -0.0694,  0.0706, -0.0502, -0.0583,\n","           0.0305, -0.0106,  0.0753,  0.0708,  0.0556, -0.0513, -0.0634,\n","           0.0664,  0.0625,  0.0664, -0.0212,  0.0706, -0.0210,  0.0752,\n","          -0.0746, -0.0566, -0.0528,  0.0730,  0.0685, -0.0086,  0.0348,\n","          -0.0695, -0.0640,  0.0618,  0.0533, -0.0682, -0.0674,  0.0663,\n","          -0.0689, -0.0794,  0.0827,  0.0536,  0.0531, -0.0560,  0.0541,\n","          -0.0692,  0.0603, -0.0497,  0.0189,  0.0021, -0.0274,  0.0621,\n","           0.0626,  0.0665, -0.0531, -0.0807,  0.0636, -0.0458,  0.0433,\n","          -0.0485,  0.0404, -0.0408,  0.0613,  0.0673,  0.0549, -0.0551,\n","           0.0688,  0.0605, -0.0616, -0.0744,  0.0555,  0.0506, -0.0753,\n","           0.0676, -0.0577,  0.0528, -0.0659,  0.0213,  0.0038,  0.0396,\n","           0.0515, -0.0599,  0.0605, -0.0530, -0.0466, -0.0651,  0.0720,\n","          -0.0181, -0.0524, -0.0036, -0.0761,  0.0654,  0.0018, -0.0190,\n","          -0.0359, -0.0568, -0.0677,  0.0161, -0.0703,  0.0434,  0.0756,\n","          -0.0694,  0.0605, -0.0575,  0.0625, -0.0550, -0.0717, -0.0712,\n","           0.0516, -0.0766,  0.0501,  0.0635,  0.0747, -0.0623,  0.0703,\n","           0.0704, -0.0570, -0.0652,  0.0765,  0.0618,  0.0112, -0.0585,\n","          -0.0531,  0.0730, -0.0634, -0.0657,  0.0560,  0.0540, -0.0573,\n","           0.0699,  0.0447, -0.0653, -0.0656,  0.0340,  0.0572,  0.0747,\n","           0.0710,  0.0706,  0.0708, -0.0665,  0.0672,  0.0709, -0.0340,\n","           0.0652,  0.0609, -0.0293, -0.0537,  0.0423,  0.0281, -0.0587,\n","          -0.0146, -0.0481, -0.0481, -0.0233,  0.0684, -0.0475, -0.0576,\n","          -0.0544, -0.0007,  0.0723, -0.0650,  0.0770,  0.0290,  0.0516,\n","          -0.0191,  0.0537,  0.0202,  0.0255,  0.0175, -0.0622,  0.0551,\n","           0.0693, -0.0540,  0.0631,  0.0656, -0.0659, -0.0594, -0.0319,\n","           0.0522]]], device='cuda:0', grad_fn=<CudnnRnnBackward0>)\n","prediction\n","shape :  torch.Size([1, 5893])\n","tensor([[ 0.1188, -0.0998, -0.1006,  ..., -0.1109, -0.1009, -0.1035]],\n","       device='cuda:0', grad_fn=<AddmmBackward0>)\n"]},{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-28-05fe4beb95ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mepoch_mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_secs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mvalid_loss\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbest_valid_loss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mbest_valid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tut3-model.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'valid_loss' is not defined"]}],"source":["N_EPOCHS = 10\n","CLIP = 1\n","\n","best_valid_loss = float('inf')\n","\n","for epoch in range(N_EPOCHS):\n","    \n","    start_time = time.time()\n","    \n","    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n","    valid_loss = evaluate(model, valid_iterator, criterion)\n","    \n","    end_time = time.time()\n","    \n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","    \n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), 'tut3-model.pt')\n","    \n","    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n","    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"]},{"cell_type":"markdown","metadata":{"id":"PbaOAsu2MQ9t"},"source":["Finally, we test the model on the test set using these \"best\" parameters."]},{"cell_type":"code","source":["N_EPOCHS = 10\n","CLIP = 1\n","\n","best_valid_loss = float('inf')\n","\n","for epoch in range(N_EPOCHS):\n","    \n","    start_time = time.time()\n","    \n","    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n","    valid_loss = evaluate(model, valid_iterator, criterion)\n","    \n","    end_time = time.time()\n","    \n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","    \n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), 'tut3-model.pt')\n","    \n","    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n","    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W0qSMRjMNVLt","executionInfo":{"status":"ok","timestamp":1665546037943,"user_tz":-540,"elapsed":833080,"user":{"displayName":"ᄀᄒᄌ","userId":"07458837797284130526"}},"outputId":"7aad088f-084a-451b-b6eb-82b2ca0dc56d"},"execution_count":52,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 01 | Time: 1m 18s\n","\tTrain Loss: 5.037 | Train PPL: 154.020\n","\t Val. Loss: 4.857 |  Val. PPL: 128.699\n","Epoch: 02 | Time: 1m 20s\n","\tTrain Loss: 4.127 | Train PPL:  61.970\n","\t Val. Loss: 4.393 |  Val. PPL:  80.880\n","Epoch: 03 | Time: 1m 23s\n","\tTrain Loss: 3.469 | Train PPL:  32.100\n","\t Val. Loss: 3.741 |  Val. PPL:  42.132\n","Epoch: 04 | Time: 1m 23s\n","\tTrain Loss: 2.925 | Train PPL:  18.630\n","\t Val. Loss: 3.458 |  Val. PPL:  31.748\n","Epoch: 05 | Time: 1m 23s\n","\tTrain Loss: 2.527 | Train PPL:  12.519\n","\t Val. Loss: 3.239 |  Val. PPL:  25.518\n","Epoch: 06 | Time: 1m 23s\n","\tTrain Loss: 2.230 | Train PPL:   9.298\n","\t Val. Loss: 3.194 |  Val. PPL:  24.396\n","Epoch: 07 | Time: 1m 24s\n","\tTrain Loss: 1.980 | Train PPL:   7.243\n","\t Val. Loss: 3.233 |  Val. PPL:  25.345\n","Epoch: 08 | Time: 1m 24s\n","\tTrain Loss: 1.768 | Train PPL:   5.862\n","\t Val. Loss: 3.278 |  Val. PPL:  26.522\n","Epoch: 09 | Time: 1m 24s\n","\tTrain Loss: 1.614 | Train PPL:   5.025\n","\t Val. Loss: 3.244 |  Val. PPL:  25.641\n","Epoch: 10 | Time: 1m 24s\n","\tTrain Loss: 1.483 | Train PPL:   4.408\n","\t Val. Loss: 3.245 |  Val. PPL:  25.674\n"]}]},{"cell_type":"code","execution_count":53,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HZLMyxEuMQ9u","outputId":"34d2aea3-2b6c-48e0-bcf8-c71a8c7e1c0e","executionInfo":{"status":"ok","timestamp":1665546582531,"user_tz":-540,"elapsed":1607,"user":{"displayName":"ᄀᄒᄌ","userId":"07458837797284130526"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["| Test Loss: 3.183 | Test PPL:  24.121 |\n"]}],"source":["model.load_state_dict(torch.load('tut3-model.pt'))\n","\n","test_loss = evaluate(model, test_iterator, criterion)\n","\n","print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"]},{"cell_type":"markdown","metadata":{"id":"hGK-F3IKMQ9u"},"source":["We've improved on the previous model, but this came at the cost of doubling the training time.\n","\n","In the next notebook, we'll be using the same architecture but using a few tricks that are applicable to all RNN architectures - packed padded sequences and masking. We'll also implement code which will allow us to look at what words in the input the RNN is paying attention to when decoding the output."]},{"cell_type":"markdown","source":["![nlp과제_1.png](data:image/png;base64,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)"],"metadata":{"id":"v282eIc-ana3"}}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"nbformat":4,"nbformat_minor":0}